\section{Simple Rings}


\begin{definition}
  A ring $R$ is called \emph{simple} if it’s only two-sided ideals are $R$ and $0$.
\end{definition}


\begin{warning}
  This definition of a simple ring is no equivalent to the last one:
  Earlier we defined a ring $R$ to be simple if it is semisimple and has precisely one simple module up to isomorphism.
  We will refer to these rings as \emph{simple according to definition 1}.
  Rings which are simple according to the new definition above will be referred to as just \emph{simple}.
\end{warning}


\begin{example}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      Let $D$ be a division ring and $n \geq 1$.
      We have already seen that $\Mat_n(D)$ is a simple according to definition 1.
      It is also simple:
      Let $I \subseteq \Mat_n(D)$ be a two-sided ideal with $I \neq 0$.
      Let $A = (a_{ij})_{1 \leq i,j \leq n} \in I$ with $A \neq 0$.
      Then $a_{ij} \neq 0$ for some $1 \leq i,j \leq n$.
      Therefore
      \[
          \diag\left( a_{ij}^{-1}, \dotsc, a_{ij}^{-1} \right) E_{ii} A E_{jj}
        = E_{ij} \in I
      \]
      and thus for every $1 \leq k,l \leq n$
      \[
            E_{kl}
        =   E_{ki} E_{ij} E_{jl}
        \in I \,.
      \]
      Since $I$ is a $D$-submodule of $\Mat_n(D)$ we find that $I = \Mat_n(D)$.
    \item
      The Weyl-algebra
      \[
          \mc{A}_2
        = k \gen{X,\partial} / (X \partial - \partial X - 1)
      \]
      is simple, but not simple according to definition 1.
  \end{enumerate}
\end{example}


\begin{warning}
  A simple ring $R$ is not necessarily simple as an $R$-module.
  A counterexample is $\Mat_n(D)$ for a skew field $D$ and $n \geq 2$.
\end{warning}


\begin{lemma}
  Let $R$ be a ring (with $1$).
  If $R$ is simple according to definition 1 it is also simple.
\end{lemma}
\begin{proof}
  Since $R$ is semisimple we have
  \[
    R \cong \Mat_{n_1}(D_1) \times \dotsb \times \Mat_{n_r}(D_r)
  \]
  for $r \geq 1$, $n_1, \dotsc, n_r \geq 1$ and skew fields $D_1, \dotsc, D_r$ by Artin--Wedderburn.
  Since $r = |\Irr(R)| = 1$ we have
  \[
    R \cong \Mat_n(D)
  \]
  for $n \geq 1$ and a skew field $D$.
\end{proof}


We can also ask ourselves under what conditions a simple ring $R$ is semisimple (and thus semisimple as an $R$-module). The following theorem by Wedderburn answers that question:


\begin{theorem}[Wedderburn]
  Let $R$ be a simple ring (with $1$). Then the following are equivalent:
  \begin{enumerate}[label=\emph{\roman*)},leftmargin=*]
    \item \label{enum: semisimple}
      $R$ is semisimple.
    \item \label{enum: left artian}
      $R$ is (left) artian.
    \item \label{enum: minimal left ideal}
      $R$ has a minimal left ideal $I \neq 0$.
    \item \label{enum: matrix ring over skew field}
      $R \cong \Mat_n(D)$ for some $n \in \Natural$ and skew field $D$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  The equivalence of \ref{enum: semisimple} and \ref{enum: matrix ring over skew field} follows directly from Artin--Wedderburn.
  
  To show that \ref{enum: semisimple} implies \ref{enum: left artian} suppose that \ref{enum: semisimple} holds.
  Then $R = \bigoplus_{i=1}^s V_i$ where $V_i \subseteq R$ is a simple $R$-module for every $1 \leq i \leq s$.
  Then
  \[
              0
    \subseteq V_1
    \subseteq V_1 \oplus V_2
    \subseteq \dotsb
    \subseteq V_1 \oplus \dotsb \oplus V_s
    =         R
  \]
  is a composition series of $R$, so by the Jordan-Hölder theorem (which we will not prove in this lecture) every strictly decreasing chain of left ideals in $R$ stabilizes (after at most $s$ ideal).
  
  To see that \ref{enum: left artian} implies \ref{enum: minimal left ideal} notice that if \ref{enum: minimal left ideal} does not hold we have an infinite chain
  \[
                A
    \supsetneq  I_1
    \supsetneq  I_2
    \supsetneq  I_3
    \supsetneq  \dotso
  \]
  of strictly decreasing nonzero left ideals, which contradicts \ref{enum: left artian}.
  
  Last we show that \ref{enum: minimal left ideal} implies \ref{enum: semisimple}.
  Suppose that $I \neq 0$ is a minimal left ideal.
  Then for every $r \in R$ the left ideal $Ir$ is either zero or minimal (i.e.\ simple as an $R$-submodule), since the map
  \[
            \varphi
    \colon  I
    \to     Ir,
    \quad   x
    \mapsto xr
  \]
  is an epimorphism of $R$-modules and thus either zero or an isomorphism (since $I$ is a simple $R$ module).
  Now
  \[
              J
    \defined  \sum_{r \in R} Ir
    =         IR
  \]
  is a two-sided ideal in $R$ which is nonzero (because $0 \subsetneq I = I1 \subseteq J$), so $J = R$.
  This show that $R$ is the sum of simple submodules.
\end{proof}


\begin{corollary}
  Let $A$ be finite-dimensional simple $k$-algebra.
  Then $A$ is semisimple and $A \cong \Mat_n(D)$ for some skew field $D$ and $n \in \Natural$.
\end{corollary}
\begin{proof}
  Because $A$ is finite-dimenisonal it contains a minimal ideal $I \neq 0$.
  The rest follows from Wedderburn’s theorem.
\end{proof}


\begin{lemma}
  Let $D$ be a skew field and $n \geq 1$.
  Then $Z(D)$ is a field and
  \[
    Z(\Mat_n(D)) \cong Z(D)
  \]
  as rings.
\end{lemma}
\begin{proof}
  We start by showing that $Z(D)$ is a field.
  We know that $Z(D) \subseteq D$ is a commutative subring (with $1$).
  Since $0 \neq 1$ in $D$ we also have $0 \neq 1$ in $Z(D)$.
  $Z(D)$ is also an integral domain, since $D$ is.
  All that we need to show is that for every $x \in Z(D)$ we also have $x^{-1} \in Z(D)$.
  This is clear, because for every $y \in D$
  \[
      x^{-1} y
    = x^{-1} y x x^{-1}
    = x^{-1} x y x^{-1}
    = y x^{-1} \,.
  \]
  
  Next we show that $Z(\Mat_n(D)) \cong Z(D)$.
  For this let $A \in Z(\Mat_n(D))$.
  We first show that $A$ is a diagonal matrix.
  To see this let $\pi_{ij} \colon \Mat_n(D) \to D$ be the canonical projection of the $(i,j)$-th coordinate for all $1 \leq i,j \leq n$.
  For all $1 \leq i,j \leq n$ we have
  \[
      a_{ij}
    = \pi_{ij}(E_{ii} A_{ij} E_{jj})
    = \pi_{ij}(E_{ii} E_{jj} A)
    = \delta_{ij} a_{ij} \,,
  \]
  so $a_{ij} = 0$ for $i \neq j$.
  Let $d_1, \dotsc, d_n \in D$ with $A = \diag(d_1, \dotsc, d_n)$.
  For every $1 \leq i,j \leq n$ we have
  \begin{align*}
        d_i
    &=  \pi_{ii}(A E_{ii})
     =  \pi_{ii}(A E_{ij} E_{jj} E_{ji})
     =  \pi_{ii}(E_{ij} A E_{jj} E_{ji}) \\
    &=  \pi_{ii}(E_{ij} d_j E_{jj} E_{ji})
     =  \pi_{ii}(d_j E_{ij} E_{jj} E_{ji})
     =  \pi_{ii}(d_j E_{ii})
     =  d_j \,,
  \end{align*}
  so $A = \diag(d, \dotsc, d)$ for $d \defined d_1 = \dotsb = d_n$.
  Since $A$ commutes with all diagonal matrices we have $d \in Z(D)$.
\end{proof}


\begin{definition}
  Let $k$ be a field.
  A $k$-algebra $A$ is called a \emph{central simple algebra (over $k$)} if $A$ is finite-dimensional, simple and $Z(A) = k$.
\end{definition}


\begin{lemma}\label{lemma: Z(A o B) = Z(A) o Z(B)}
  Let $A$ and $B$ be $k$-algebras.
  Then
  \[
      Z(A \tensor_k B)
    = Z(A) \tensor_k Z(B) \,.
  \]
\end{lemma}


\begin{recall}
  Let $k$ be a field $V$ and $W$ be $k$-vector spaces.
  We know from linear algebra that every element $x \in V \tensor_k W$ can be written as a finite sum of simple tensors $x = \sum_{i=1}^n v_i \tensor w_i$.
  Furthermore $v_1, \dotsc, v_n$ are unique if $w_1, \dotsc, w_n \in W$ are linearly independent.
  \begin{proof}
    We can assume w.l.o.g.\ that $W = \vspan_k \{w_1, \dotsc, w_n\}$.
    We have for every $1 \leq i \leq n$ a $k$-bilinear map
    \[
              s_i
      \colon  V \times W \to V,
      \quad   \left(v, \sum_{i=1}^n \lambda_i w_i\right)
      \mapsto \lambda_i v \,.
    \]
    and thus a $k$-linear map
    \[
              f_i
      \colon  V \tensor_k W
      \to     V,
      \quad   v \tensor w_j
      \mapsto \delta_{ij} v \,.
    \]
    For $x \in V \tensor_k W$ with $x = \sum_{j=1}^n v_j \tensor w_j = \sum_{j=1}^n v'_j \tensor w_j$ we have
    \[
        0
      =   \left( \sum_{j=1}^n v_j \tensor w_j \right)
        - \left( \sum_{j=1}^n v'_j \tensor w_j \right)
      = \sum_{j=1}^n (v_j - v'_j) \tensor w_j
    \]
    and therefore for every $1 \leq i \leq n$
    \[
        v_i - v'_i
      = f_i\left( \sum_{j=1}^n (v_j - v'_j) \tensor w_j\right)
      = f_i(0)
      = 0 \,.
      \qedhere
    \]
  \end{proof}
\end{recall}


\begin{proof}[Proof of the Lemma]
  It is clear that $Z(A) \tensor_k Z(B) \subseteq Z(A \tensor_k B)$.
  To show the other inclusion let $x \in Z(A \tensor_k B)$.
  We can write $x = \sum_{i=1}^n a_i \tensor b_i$.
  We can assume w.l.o.g.\ that both $a_1, \dotsc, a_n$ and $b_1, \dotsc, b_n$ are linearly independent.
  For every $a \in A$ we have
  \[
      \sum_{i=1}^n (a a_i) \tensor b_i
    = (a \tensor 1) x
    = x (a \tensor 1)
    = \sum_{i=1}^n (a_i a) \tensor b_i
  \]
  and thus $a_i a = a a_i$ (because $b_1, \dotsc, b_n$ are linearly independent).
  So $a_i \in Z(A)$ for every $1 \leq i \leq n$.
  In the same way we find that $b_1, \dotsc, b_n \in Z(B)$.
  This shows that $x \in Z(A) \tensor_k Z(B)$.
\end{proof}


\begin{proposition}
  Let $A$ and $B$ be central simple algebras over the same field $k$.
  Then $A \tensor_k B$ is a central simple algebra.
\end{proposition}
\begin{proof}
  Since both $A$ and $B$ are finite-dimensional the same holds for $A \tensor_k B$.
  By Lemma \ref{lemma: Z(A o B) = Z(A) o Z(B)} we have
  \[
      Z(A \tensor_k B)
    = Z(A) \tensor_k Z(B
    = k \tensor_k k
    = k \,.
  \]
  So we only need to show that $A \tensor_k B$ only contains $0$ and $A \tensor_k B$ as two-sided ideals.
  To show this let $I \subseteq A \tensor_k B$ be a two-sided ideal with $I \neq 0$.
  We can write every $u \in I$ as $u = \sum_{i=1}^n a_i \tensor b_i$ where $b_1, \dotsc, b_n$ are linearly independent.
  Let $u \in I$ with $u \neq 0$ such that $u$ can be written as above so that the number of summands is minimal with respect to all nonzero elements in $I$.
  Let
  \begin{equation}\label{eqn: u as a sum}
    u = \sum_{i=1}^n a_i \tensor b_i
  \end{equation}
  be such a sum.
  Since $n$ is minimal we have $a_1 \neq 0$.
  Therefore the two-sided ideal $A a_1 A \subseteq A$ is non-zero, so $A a_1 A = A$ because $A$ is simple.
  In particular there exists $c, c' \in A$ with $1 = c a_1 c'$.
  By multiplying \eqref{eqn: u as a sum} from the left with $(c \tensor 1)$ and from the right with $(c' \tensor 1)$ we see that the element
  \[
              x
    \defined  (c \tensor 1) u (c' \tensor 1)
    \in       I
  \]
  can be written as
  \begin{equation}\label{eqn: x as a sum}
        x
    =   1 \tensor b_1
      + a'_2 \tensor b_2
      + \dotsb
      + a'_n \tensor b_n
  \end{equation}
  where $b_1, \dotsc, b_n$ are linearly independent.
  In particular $x \neq 0$.
  For every $a \in A$ we have
  \[
        (a \tensor 1) x - x (a \tensor 1)
    =   (a a'_2 - a'_2 a) \tensor b_2
      + \dotsb
      + (a a'_n - a'_n a) \tensor b_2 \in I \,.
  \]
  By the minimality of $u$ we find that
  \[
      (a \tensor 1) x - x (a \tensor 1)
    = 0
  \]
  for every $a \in A$.
  Because $b_2, \dotsc, b_n$ are linearly independent it follows that $a a'_i - a'_i a = 0$ for all $a \in A$ and $2 \leq i \leq n$.
  So $a'_2, \dotsc, a'_n \in Z(A) = k$.
  Using \eqref{eqn: x as a sum} we find that $x = 1 \tensor b$ for some $b \in B$.
  Since $x \neq 0$ we also have $b \neq 0$.
  Because $B$ is simple we find that $BbB = B$ and therefore
  \[
              I
    \supseteq (1 \tensor B) x (1 \tensor B)
    =         1 \tensor (BbB)
    =         1 \tensor B \,.
  \]
  Using this we find that
  \[
              I
    \supseteq (A \tensor 1) (1 \tensor B)
    =         A \tensor_k B \,.
  \]
  So $0$ and $A \tensor_k B$ are the only two-sided ideals in $A \tensor_k B$.
\end{proof}
