\section{Symmetric Polynomials}

%%%%%%  New text

\begin{fluff}
  The symmetric group $S_n$ acts by $k$-algebra automorphisms on the polynomial ring $k[X_1, \dotsc, X_n]$ by
  \[
      \sigma.f(X_1, \dotsc, X_n)
    = f(X_{\sigma(1)}, \dotsc, X_{\sigma(n)}) \,.
  \]
  In this section we will be concerned by the $k$-algebra of invariants $k[X_1, \dotsc, X_n]^{S_n}$.
\end{fluff}


\begin{definition}
  Let $k$ be a field.
  The polynomial $f \in k[X_1, \dotsc, X_n]^{S_n}$ are \emph{symmetric}, and $k[X_1, \dotsc, X_n]^{S_n}$ is the \emph{ring of symmetric polynomials \textup(in $n$ variables\textup) \textup(over $k$\textup)}.
\end{definition}


\begin{example}
  \label{example: symmetric polynomials}
  In $k[X_1, X_2, X_3]$ we have the symmetric polynomials
  \begin{align*}
                p_2
    &\coloneqq  X_1^2 + X_2^2 + X_3^2 \,,
    \\
                h_2
    &\coloneqq  X_1^2 + X_1 X_2 + X_1 X_3 + X_2^2 + X_2 X_3 + X_3^2 \,,
    \\
                e_2
    &\coloneqq  X_1 X_2 + X_1 X_3 + X_2 X_3 \,,
    \\
                m_{(4,4,2)}
    &\coloneqq  X_1^4 X_2^2 X_3^2 + X_1^2 X_2^4 X_3^2 + X_1^2 X_2^2 X_3^4 \,.
  \end{align*}
  In the next subsections we will generalize these examples.
\end{example}


\begin{lemma}
  \label{lemma: symmetric iff all homogeneous parts are symmetric}
  With respect to the usual grading $k[X_1, \dotsc, X_n] = \bigoplus_{d \in \Natural} k[X_1, \dotsc, X_n]_d$ a polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if all of its homogeneous parts are symmetric.
\end{lemma}


\begin{proof}
  The decomposition $k[X_1, \dotsc, X_n] = \bigoplus_{d \geq 0} k[X_1, \dotsc, X_n]_d$ is a decomposition into subrepresentations of $S_n$, thus the claim follows from Lemma~\ref{lemma: direct sum and invariants commute}.
\end{proof}

\begin{fluff}
  In the following subsections we will consider families of symmetric polynomials which generalize the polynomials given in Example~\ref{example: symmetric polynomials}.
  
  We will start off with the so called \emph{elemantary symmetric polynomials}.
  We prove the famous \emph{Fundamental Theorem of Symmetric Polynomials}, which roughly states that every every symmetric polynomial can be uniquely expressed in terms of the elementary symmetric polynomials.
  
  We will then use the elementary symmetric polynomials to study other kinds of symmetric polynomials:
  Namely the \emph{complete homogeneous symmetric polynomials}, \emph{power sums} \emph{monomial symmetric polynomials} and \emph{Schur polynomials}.
  Along the way we will also introduce \emph{partitions} as a natural way for labeling these different kinds of symmetric polynomials.
\end{fluff}





\subsection{Elementary Symmetric Polynomials \& Fundamental Theorem of Symmetric Polynomials}


\begin{definition}
  For all $n, r \in \Natural$ the \emph{$r$-th elementary symmetric polynomial} in $n$ variables is
  \[
              e^{(n)}_r
    \defined  \sum_{1 \leq i_1 \leq \dotsb \leq i_r \leq n} X_{i_1} \dotsm X_{i_r}
    =         \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \, \prod_{i \in I} X_i \,.
  \]
  In particular $e^{(n)}_0 = 1$ and $e^{(n)}_r = 0$ for every $r > n$.
\end{definition}


\begin{fluff}
  For all $a_1, \dotsc, a_n \in k$ we have that
  \begin{align*}
     &\, (t-a_1) \dotsm (t-a_n) \\
    =&\, t^n  - (a_1 + \dotsb + a_n) t^{n-1}
              + \left( \sum_{1 \leq i_1 < i_2 \leq n} a_{i_1} a_{i_2} \right) t^{n-2}
              + \dotsb
              + (-1)^n a_1 \dotsm a_n \\ 
    =&\,    t^n
          - e^{(n)}_1(a_1, \dotsc, a_n) t^{n-1}
          + e^{(n)}_2(a_1, \dotsc, a_n) t^{n-2}
          - \dotsb
          + (-1)^n e^{(n)}_n(a_1, \dotsc, a_n)
  \end{align*}
  in $k[t]$.
  We will formalize this observation in the following lemma:
\end{fluff}


\begin{lemma}
  \label{lemma: natural occurence of elementary symmetric polynomials}
  For all $n \in \Natural$ we have in $k[X_1, \dotsc, X_n][t]$ the equality
  \begin{align*}
        \prod_{i=1}^n (t-X_i)
    &=    e^{(n)}_0 t^n
        - e^{(n)}_1 t^{n-1}
        + e^{(n)}_2 t^{n-2}
        - \dotsb
        + (-1)^n e^{(n)}_n  \\
    &=    t^n
        - e^{(n)}_1 t^{n-1}
        + e^{(n)}_2 t^{n-2}
        - \dotsb
        + (-1)^n e^{(n)}_n
  \end{align*}
\end{lemma}
\begin{proof}
  On both sides the $r$-th coefficient is given by $\prod_{I \subseteq \{1, \dotsc, n\}, |I| = r} \prod_{i \in I} X_i $.
\end{proof}


\subsubsection{The Fundamental Theorem of Symmetric Functions}


\begin{theorem}[Fundamental Theorem of Symmetric Functions]
  The symmetric polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ generate the $k$-algebra of symmetric functions $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent, i.e.\ the unique $k$-algebra homomorphism
  \[
            k[Y_1, \dotsc, Y_n]
    \to     k[X_1, \dotsc, X_n]^{S_n} \,,
    \quad   Y_r
    \mapsto e^{(n)}_r
  \]
  is an isomorphism of $k$-algebras.
\end{theorem}


\begin{fluff}
  We will give two proofs of the fundamental theorem.
  The one given in the lecture is the second one.
\end{fluff}


\begin{proof}[First Proof of the Fundamental Theorem]
  During this proof we will abbreviate $e^{(n)}_k$ as $e_k$.
  To makes our lifes easier we introduce an ordering on the set monomials in $k[X_1, \dotsc, X_n]$:
  
  For this we first order the monomials by their power of $X_1$ in decreasing order.
  The monomials with the same power of $X_1$ are then ordered in decreasing order by their power of $X_2$.
  We then continue this process trough the variables $X_3, \dotsc, X_n$.
  
  For any two monomials $X^\alpha = X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ and $X^\beta = X_1^{\beta_1} \dotsm X_n^{\beta_n}$ we thus have $X^\alpha > X^\beta$ if and only if there exists some $1 \leq i \leq n$ such that $\alpha_j = \beta_j$ for all $j < i$ and $\alpha_i > \beta_i$.
  This gives a well-ordering on the set of monomials in $k[X_1, \dotsc, X_n]$.
  
  For any polynomial $p \in k[X_1, \dotsc X_n]$ with $p \neq 0$ we define the initial term $\init p$ to be the highest monomial occuring in $p$, including its coefficient.
  Then the following properties hold:
  \begin{enumerate}
    \item
      If $p \neq 0$ is symmetric then for $\init p = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ one has $\alpha_1 \geq \alpha_2 \geq \dotsb \geq \alpha_n$.
    \item
      For $p, q \in k[X_1, \dotsc, X_n]$ with $p, q \neq 0$ one has $\init (p \cdot q) = \init p \cdot \init q$.
    \item
      For all $1 \leq k \leq n$ one has $\init e_k = X_1 \dotsm X_k$ .
  \end{enumerate}
  With this we are now well-equipped to prove the theorem:
  
  We first show that $e_1, \dotsc, e_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra.
  For this let $f \in k[X_1, \dotsc, X_n]^{S_n}$ with $f \neq 0$.
  By Lemma~\ref{lemma: symmetric iff all homogeneous parts are symmetric} we may assume that $f$ is homogeneous of degree $d \geq 0$.
  For
  \[
      \init f
    = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
  \]
  we then have $d = \alpha_1 + \dotsb + \alpha_n$.
  
  We consider the polynomial
  \[
      p
    =         c
              \left( e_1 \right)^{\alpha_1 - \alpha_2}
      \dotsm  \left( e_{n-1} \right)^{\alpha_{n-1} - \alpha_n}
              \left( e_n \right)^{\alpha_n} \,.
  \]
  Then $\init p = \init f$ by the above properties of $\init$.
  Because $e_k$ is homogenous of degree $k$ it follows that $p$ is homogeneous of degree
  \begin{align*}
     &\,  (\alpha_1-\alpha_2) + 2(\alpha_2-\alpha_3) + \dotsb + (n-1)(\alpha_{n-1}-\alpha_n) + n\alpha_n \\
    =&\,  \alpha_1 + \dotsb + \alpha_n
    =     d \,.
  \end{align*}
  Combining these observations we find that $f-p$ is a homogeneous symmetric polynomial of degree $d$ with either $f-p = 0$ or at least $\init (f-p) < \init f$.
  
  Because there are only finitely many monomials of homogeneous degree $d$ we can repeat the above process to arrive at the zero polynomial in finitely many steps.
  Hence $f$ can be expressed as a poylynomial in $e_1, \dotsc, e_n$.
  
  To show that $e_1, \dotsc, e_n$ are algebraically independent we need to show that the monomials in $e_1, \dotsc, e_n$, i.e.\ the polynomials
  \[
      e^{\,\underline{\alpha}}
    = e_1^{\alpha_1} \dotsm e_n^{\alpha_n}
    \quad\text{for}\quad
        \underline{\alpha}
    =   (\alpha_1, \dotsc, \alpha_n)
    \in \Natural^n
  \]
  are linearly independent.
  For this we notice that for all $\underline{\alpha} \neq \underline{\beta}$ we have that
  \begin{align*}
            \init e^{\,\underline{\alpha}}
       =&\, \init e_1^{\alpha_1} \dotsm e_n^{\alpha_n}   \\
       =&\, X_1^{\alpha_1 + \dotsb + \alpha_n} X_2^{\alpha_2 + \dotsb + \alpha_n} \dotsm X_n^{\alpha_n} \\
    \neq&\, X_1^{\beta_1 + \dotsb + \beta_n}   X_2^{\beta_2  + \dotsb + \beta_n}  \dotsm X_n^{\beta_n}  \\
       =&\, \init e_1^{\beta_1} \dotsm e_n^{\beta_n}
       =    \init e^{\,\underline{\beta}}
  \end{align*}
  so that the polynomials $e^{\,\underline{\alpha}}$ for $\underline{\alpha} \in \Natural^n$ are pairwise different.
  
  Now suppose that
  \[
      0
    = \lambda_1 e^{\,\underline{\alpha}_1} + \dotsb + \lambda_s e^{\,\underline{\alpha}_s}
  \]
  with $s \geq 1$, $\underline{\alpha}_i \neq \underline{\alpha}_j$ for $i \neq j$ and $\lambda_i \neq 0$ for all $1 \leq i \leq s$.
  We can assume w.l.o.g.\ that 
  \[
      \init e^{\,\underline{\alpha}_1}
    > \init e^{\,\underline{\alpha}_2}
    > \dotsb
    > \init e^{\,\underline{\alpha}_s}
  \]
  since all of these terms are pairwise different.
  It follows that the initial term $\init e^{\underline{\alpha}_1}$ occures only in $e^{\,\underline{\alpha}_1}$ and in no ther of the $e^{\,\underline{\alpha}_i}$.
  From $\lambda_1 = 0$, in contradiction to $\lambda_1 \neq 0$.
\end{proof}


\begin{proof}[Second Proof of the Fundamental Theorem]
  Note that
  \begin{equation}
    \label{equation: recursive formel for elementary symmetric polynomials}
    \tag{$\ast$}
    \begin{aligned}
          e^{(n)}_r
      =  \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i
      &=    \sum_{\substack{I \subseteq \{1, \dotsc, n-1\} \\ |I| = r}} \prod_{i \in I} X_i
          + \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r-1}} \left( \prod_{i \in I} X_i \right) X_n  \\
      &=  e^{(n-1)}_r + e^{(n)}_{r-1} X_n \,.
    \end{aligned}
  \end{equation}
  \begin{claim}
    A polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if $f$ can be written as a polyonmial in $e^{(n)}_1, \dotsc, e^{(n)}_n$, i.e.\ we have that
    \[
        k\left[ e^{(n)}_1, \dotsc, e^{(n)}_n \right]
      = k[X_1, \dotsc, X_n]^{S_n} \,.
    \]
  \end{claim}
  \begin{proof}[Proof of claim]
    Because the polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n \in k[X_1, \dotsc, X_n]^{S_n}$ are symmetric it follows that $k[ e^{(n)}_1, \dotsc, e^{(n)}_n ] \subseteq k[X_1, \dotsc, X_n]^{S_n}$.
    We show the other inclusion by induction over $n$.
    For $n = 1$ we have that $k[ e^{(1)}_1 ] = k[X_1] = k[X_1]^{S_1}$.
    
    Let $n \geq 2$ and suppose that the claim holds for $n-1$.
    We show the claim for $n$ by induction over the (total) degree $d \defined \deg f$.
    If $f$ is constant than the claim holds.
    So let $d \geq 1$ and suppose the claim holds for degrees $0, \dotsc, d-1$.
    By the Lemma~\ref{lemma: symmetric iff all homogeneous parts are symmetric} we may assume that $f$ is homogenous.
    (The homogenous parts of lower degree are by induction hypothesis expressable as polynomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$.)
    
    Let
    \[
              \Phi
      \colon  k[X_1, \dotsc, X_n]
      \to     k[X_1, \dotsc, X_{n-1}] \,,
      \quad   f(X_1, \dotsc, X_n)
      \mapsto f(X_1, \dotsc, X_{n-1}, 0)
    \]
    be the evaluation at $X_n = 0$.
    By \eqref{equation: recursive formel for elementary symmetric polynomials} we have that
    \begin{align*}
          q\left( e^{(n)}_r \right)
      &=  e^{(n-1)}_r
      \quad \text{for all $1 \leq r < n$} \,,
      \\
          q\left( e^{(n)}_n \right)
      &=  0 \,.
    \end{align*}
    Note that $\Phi(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric:
    Because $f$ is symmetric we have that
    \[
        f
      = f(X_1, \dotsc, X_n)
      = f(X_{\sigma(1)}, \dotsc, X_{\sigma(n)})
    \]
    for every $\sigma \in S_n$, und thus we have that
    \[
        f(X_1, \dotsc, X_{n-1}, 0)
      = f(X_{\tau(1)}, \dotsc, X_{\tau(n-1)}, 0)
    \]
    for every $\tau \in S_{n-1}$.
    Because $\Phi(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric we can use the induction hypothesis (from the induction on $n$) to write
    \[
        \Phi(f)
      = P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right)
    \]
    for some polynomial $P \in k[Y_1, \dotsc, Y_{n-1}]$.
    Consider the symmetric polynomial
    \[
                g
      \coloneqq P\left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1} \right)
      \in       k[X_1, \dotsc, X_n] \,.
    \]
    
    Because $\Phi$ is a homomorphism of $k$-algebras we find that
    \begin{align*}
         \Phi(g)
      &= \Phi\left( P\left(e^{(n)}_1, \dotsc, e^{(n)}_{n-1}\right) \right) \\
      &= P\left( \Phi\left(e^{(n)}_1\right), \dotsc, \left(e^{(n)}_{n-1}\right) \right) \\
      &= P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right)
       = \Phi(f)
    \end{align*}
    and therefore that $\Phi(f - g) = 0$.
    Note that $\ker \Phi = (X_n)$ by the commutativity of the following diagram:
    \[
      \begin{tikzcd}
          k[X_1, \dotsc, X_n]
          \arrow{rr}[above]{\Phi}
          \arrow{rd}[below left]{p \mapsto \class{p}}
        & {}
        & k[X_1, \dotsc, X_{n-1}]
        \\
          {}
        & k[X_1, \dotsc, X_n]/(X_n)
          \arrow{ru}[above,rotate=20]{\sim}[below right]{\class{p} \mapsto p(X_1, \dotsc, X_n, 0)}
        & {}
      \end{tikzcd}
    \]
    It therefore follows from $\Phi(f - g) = 0$ that $X_n \mid (f-g)$.
    Because $f-g$ is symmetric (because both $f$ and $g$ are symmetric) it follows that $X_i \mid (f-g)$ for all $1 \leq i \leq n$, and therefore that $X_1 \dotsm X_n \mid (f-g)$.
    We can thus consider the polynomial
    \[
                h
      \defined  \frac{f-g}{X_1 \dotsm X_n}
      =         \frac{f-g}{e^{(n)}_n} \,.
    \]
    (This quotient is well-defined because $k[X_1, \dotsc, X_n]$ is an integral domain.)
    
    \begin{claim}
      The polynomial $h$ is symmetric.
    \end{claim}
    \begin{proof}
      From $h e^{(n)}_n = f-g$ it follows for every $\sigma \in S_n$ that
      \[
          (\sigma.h) e^{(n)}_n
        = (\sigma.h) (\sigma.e^{(n)}_n)
        = \sigma.(h e^{(n)}_n)
        = \sigma(f-g)
        = \sigma.f - \sigma.g
        = f - g \,.
      \]
      Hence it follows that $\sigma.h = (f-g)/e^{(n)}_n = h$.
    \end{proof}
    
    \begin{claim}
      We have that $\deg g \leq \deg f$ and therefore that $\deg h < \deg f$.
    \end{claim}
    \begin{proof}
      ?
%     TODO: Adding a proof.
    \end{proof}
    
    By induction hypothesis (of the induction on $d$) we can write $h$ as a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$.
    Because $g$ is also a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$ it further follows that $f = e^{(n)}_n h + g$ is a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$.
  \end{proof}
  
  We now prove that the polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ are algebraically independent by induction over $n$.
  It holds for $n = 1$ because $e^{(1)}_1 = X_1$.
  
  Now suppose $n \geq 2$ and that the elements $e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1}$ are algebraically independent.
  Suppose that
  \[
      F\left(e^{(n)}_1, \dotsc, e^{(n)}_n\right)
    = 0
  \]
  for some polynomial $F \in k[Y_1, \dotsc, Y_n]$ with $F \neq 0$ of minimal possible degree.
  Then
  \begin{align*}
        0
    &=  \Phi \left( F \left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1}, e^{(n)}_n \right) \right) \\
    &=  F \left(
            \Phi\left( e^{(n)}_1 \right),
            \dotsc,
            \Phi\left( e^{(n)}_{n-1} \right),
            \Phi\left( e^{(n)}_n \right)
          \right) \\
    &=  F \left( e^{n-1}_1, \dotsc, e^{(n-1)}_{n-1}, e^{(n-1)}_n \right)
     =  F \left( e^{n-1}_1, \dotsc, e^{(n-1)}_{n-1}, 0 \right).
  \end{align*}
  From the induction hypothesis it follows that $F(Y_1, \dotsc, Y_{n-1}, 0) = 0$, and therefore that $Y_n \mid F$.
  So there exists some polynomial $\hat{F} \in k[Y_1, \dotsc, Y_n]$ with $F = Y_n \hat{F}$.
  Note that $\hat{F} \neq 0$ since $F \neq 0$, and that $\deg \hat{F}  <\deg F$.
  We then have
  \[
      0
    = F\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = e^{(n)}_n \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right).
  \]
  Because $k[X_1, \dotsc, X_n]$ is an integral domain it now further follows from $e^{(n)}_n \neq 0$ that
  \[
      \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = 0 \,.
  \]
  This contradits the minimality of $F$.
\end{proof}


\begin{remark}
  Each of the proofs gives us an algorithm how to express a symmetric polynomial in terms of $e^{(n)}_1, \dotsc, e^{(n)}_n$.
\end{remark}


\begin{remark}
  The first proof shows that the fundemental theorem does not only hold if $k$ is a field, but for every nonzero commutative ring $R$.
  It does in particular hold for $k = \Integer$.
  
  The second proof can be slightly modified to also work for $R$:
  Instead of using that $R[X_1, \dotsc, X_n]$ is an integral domain (which holds if and only if $R$ itself is an intgeral domain), it sufficies to realize that the polynomial $X_1 \dotsm X_n = e^{(n)}_n$ is a non-zero divisor.
\end{remark}


\begin{example}
  Let $p(t) = t^n + a_{n-1} t^{n-1} + \dotsb + a_1 t + a_0 \in k[t]$ be a polynomial, and let $\lambda_1, \dotsc, \lambda_n$ be the roots of $p(t)$ is an algebraic closure $\overline{k}$ of $k$.
  Then
  \[
      a_i
    = (-1)^{n-i} e^{(n)}_{n-i}(\lambda_1, \dotsc, \lambda_n)
  \]
  for all $i$ by Lemma~\ref{lemma: natural occurence of elementary symmetric polynomials}.
  It follows from the fundamental theorem that every symmetric polynomial in the roots $\lambda_1, \dotsc, \lambda_n$ can already be expressed as a polynomial in the coefficients $a_0, \dotsc, a_{n-1}$.
  
  Consider for example the \emph{discriminant}
  \[
      \Delta(p)
    = \prod_{i < j} (\lambda_i - \lambda_j)^2
    = (-1)^{n(n-1)/2} \prod_{i \neq j} (\lambda_i - \lambda_j)
  \]
  The polynomial $D(X_1, \dotsc, X_n) \defined \prod_{i < j} (X_i - X_j)^2$ is symmetric, which is why there exists a (unique) polynomial $f \in k[Y_1, \dotsc, Y_n]$ with $D = f(e^{(n)}_1, \dotsc, e^{(n)}_n)$.
  Then
  \[
      \Delta(p)
    = D(\lambda_1, \dotsc, \lambda_n)
    = f(e^{(n)}_1(\lambda_1, \dotsc, \lambda_n), \dotsc, e^{(n)}_n(\lambda_1, \dotsc, \lambda_n))
    = f(a_{n-1}, \dotsc, a_0) \,.
  \]
  This shows that $\Delta(p)$ can be expressed as a polynomial in the coefficients of $p$.
  
  Note that $\Delta(p) = \prod_{i < j} (\lambda_i - \lambda_j)^2$ vanishes if and only if $f$ has some multiple root.
  Alltogether we have found that there exists a polynomial expression in the coefficients of $p$, namely $f(a_{n-1}, \dotsc, a_0)$, by which we can describe if $f$ has multiple roots (in an algebraic closure $\overline{k}$ of $k$).
  
  Consider for example the case $n = 2$.
  Then
  \begin{align*}
        D(X_1, X_2)
     =  (X_1 - X_2)^2
     =  (X_1 + X_2)^2 - 4 X_1 X_2
     =  \left( e^{(2)}_1 \right)^2 - 4 e^{(2)}_2 \,,
  \end{align*}
  so that $f(Y_1, Y_2) = Y_1^2 - 4 Y_2$.
  Thus for $p(t) = t^2 + a t + b$ one has that
  \[
      \Delta(p)
    = f(a,b)
    = a^2 - 4 b \,.
  \]
  Note that by the usual solution formula for quadratic equations, the roots $\lambda_1$, $\lambda_2 \in \overline{k}$ of $p(t)$ are given by
  \[
      \frac{-a \pm \sqrt{a^2 - 4b}}{2}
    = \frac{-a \pm \Delta(p)}{2}
    = -\frac{1}{2} a \pm \frac{\sqrt{\Delta(p)}}{2} \,.
  \]
  We can see explicitly that the roots $\lambda_1$, $\lambda_2$ differ by $\sqrt{\Delta(p)}$, so that they are distinct if and only if $\Delta(p) \neq 0$.
\end{example}


\begin{fluff}
  Let $R$ be a ring.
  Then every sequence of elements $a_0, a_1, a_2, \dotsc \in R$ can be considered as coefficients of a (formal) power series
  \[
        \sum_{r=0}^\infty a_r t^r
    \in R\!\dblbrack{t} \,.
  \]
  This power series is the \emph{generating series} or \emph{generating function} of the sequence $(a_n)_{n \in \Natural}$.
  
  In the following we will consider the generating series $E^{(n)}(t)$, $H^{(n)}(t)$, $P^{(n)}(t)$ of families of symmetric polynomials $(e^{(n)}_r)_{r \in \Natural}$, $(h^{(n)}_r)_{r \in \Natural}$, $(p^{(n)}_r)_{r \in \Natural}$, and then use identities involving these generating series $E^{(n)}(t)$, $H^{(n)}(t)$, $P^{(n)}(t)$ to derive formulas for their coefficients, i.e.\ the symmetric polynomials $e^{(n)}_r$, $h^{(n)}_r$, $p^{(n)}_r$.
  
  For this we will start with the elementary symmetric polynomials $e^{(n)}_r$ and their generating series:
\end{fluff}


\begin{definition}
  For every $n \in \Natural$ the power series $E^{(n)}(t) \in k[X_1, \dotsc, X_n]\!\dblbrack{t}$ is the generating series of the sequence $(e^{(n)}_r)_{r \in \Natural}$, that is
  \[
              E^{(n)}(t)
    \defined  \sum_{r=0}^\infty e^{(n)}_r t^r \,.
  \]
\end{definition}


\begin{lemma}
  \label{lemma: explicit formula for E}
  One has the equality of power series
  \[
      E^{(n)}(t)
    = \prod_{i=1}^n (1 + X_i t) \,.
  \]
\end{lemma}


\begin{proof}
  The coefficient of $t^r$ on the right hand side of the equation is given by
  \[
    \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i \,,
  \]
  which is precisely $e^{(n)}_r$.
\end{proof}





\subsection{Completely Symmetric Polynomials}


\begin{definition}
  For all $n, r \in \Natural$ the \emph{$r$-th complete homogeneous symmetric polynomial} in $n$-variables is the sum of all monomials of $k[X_1, \dotsc, X_n]$ of degree $r$, that is
  \[
              h^{(n)}_r
    \defined  \sum_{\substack{\underline{\alpha} \in \Natural^n \\ |\underline{\alpha}| = r}}
              X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
  \]
\end{definition}


\begin{definition}
  For every $n \in \Natural$ the power series $H^{(n)}(t) \in k[X_1, \dotsc, X_n]\!\dblbrack{t}$ is the generating series of the sequence $(h^{(n)}_r)_{r \in \Natural}$, that is
  \[
              H^{(n)}(t)
    \defined  \sum_{r=0}^\infty h^{(n)}_r t^r \,.
  \]
\end{definition}


\begin{lemma}
  \label{lemma: explicit formula for H}
  One has the equality of power series
  \[
      H^{(n)}(t)
    = \prod_{i=1}^n \frac{1}{1 - X_i t}
  \]
\end{lemma}


\begin{proof}
  Note that the inverse of $1 - X_i t$ is for every $i$ given by the geometric series
  \[
              Q_i
    \defined  1 + X_i t + X_i^2 t^2 + X_i^3 t^3 + \dotsb
    \in       k[X_1, \dotsc, X_n]\!\dblbrack{t} \,,
  \]
  so that
  \[
      \prod_{i=1}^n \frac{1}{1 - X_i t}
    = \prod_{i=1}^n Q_i
    = Q_1 \dotsb Q_n \,.
  \]
  The coefficient of $t^r$ in $Q_1 \dotsm Q_n$ is given by $\sum_{|\underline{\alpha}| = r}  X_1^{\alpha_1} \dotsm X_n^{\alpha_n} = h^{(n)}_r$.
\end{proof}


\begin{fluff}
  By comparing the closed expressions of the power series $E^{(n)}(t)$ and $H^{(n)}(t)$ from from Lemma~\ref{lemma: explicit formula for E} and Lemma~\ref{lemma: explicit formula for H} we find that
  \[
      E^{(n)}(-t)H^{(n)}(t)
    = 1
    = H^{(n)}(-t)E^{(n)}(t) \,.
  \]
  By comparing the $s$-th coefficients of these power series we arrive at the following relation between the elementary symmetric polynomials $e^{(n)}_r$ and the complete homogeneous symmetric polynomials $h^{(n)}_r$:
\end{fluff}


\begin{corollary}
  \label{corollary: combinatorical formula for e and h}
  For all $s \geq 1$ we have that
  \begin{align*}
          h^{(n)}_s
        - e^{(n)}_1 h^{(n)}_{s-1}
        + e^{(n)}_2 h^{(n)}_{s-2}
        - \dotsb
        + (-1)^{s-1} e^{(n)}_{s-1} h^{(n)}_1
        + (-1)^s     e^{(n)}_s
    &=  0
  \intertext{as well as}
          e^{(n)}_s
        - h^{(n)}_1 e^{(n)}_{s-1}
        + h^{(n)}_2 e^{(n)}_{s-2}
        - \dotsb
        + (-1)^{s-1} h^{(n)}_{s-1} e^{(n)}_1
        + (-1)^s     h^{(n)}_s
    &=  0 \,.
  \end{align*}
\end{corollary}


\begin{fluff}
  From the Fundamental Theorem of Symmetric Polynomials we know that the complete homogeneous symmetric polynomials $h^{(n)}_i$ can be expressed uniquely as polynomials in the elementary symmetric polynomials $e^{(n)}_i$, so that there exist unique polynomials $P_1, \dotsc, P_n \in k[Y_1, \dotsc, Y_n]$ with
  \[
      h^{(n)}_i
    = P_i\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
  \]
  for all $i = 1, \dotsc, n$.
  
  By rearranging the first formula of Corollary~\ref{corollary: combinatorical formula for e and h} to the equality
  \[
      h^{(n)}_s
    =   e^{(n)}_1 h^{(n)}_{s-1}
      - e^{(n)}_2 h^{(n)}_{s-2}
      + \dotsb
      - (-1)^{s-1} e^{(n)}_{s-1} h^{(n)}_1
      - (-1)^s e^{(n)}_s
  \]
  we can recursively express the $h^{(n)}_i$ in terms of the $e^{(n)}_i$, starting off with $e^{(n)}_1 = h^{(n)}_1$ for $s = 1$, and thus inductively determine the polynomials $P_1, \dotsc, P_n$.
  
  Note that the second formula of Corollary~\ref{corollary: combinatorical formula for e and h} results from the first by swapping $h^{(n)}_i$ and $e^{(n)}_i$.
  We can therefore swap the $h^{(n)}_i$ and $e^{(n)}_i$ in the previous paragraph to find that the $e^{(i)}_n$ can be expressed in terms of the $h^{(n)}_i$, and that this can be done in exactly the same way as the $h^{(n)}_i$ are expressed in terms of the $e^{(n)}_i$.
  In other words, we have that
  \[
      e^{(n)}_i
    = P_i\left( h^{(n)}_1, \dotsc, h^{(n)}_n \right)
  \]
  for all $i = 1, \dotsc, n$.
  
  This seems to suggest that the elementary symmetric polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ and the homomogeneous symmetric polynomials $h^{(n)}_1, \dotsc, h^{(n)}_n$ are somehow dual to each other.
  To make this notion of duality more precise note that by the Fundamental Theorem of Symmetric Polynomials there exists a unique $k$-algebra homomorphism
  \[
            \Phi
    \colon  k[X_1, \dotsc, X_n]^{S_n}
    \to     k[X_1, \dotsc, X_n]^{S_n}∀
  \]
  with $\Phi(e^{(n)}_i) = h^{(n)}_i$ for every $i = 1, \dotsc, n$.
  (This follows from combining the universal property of the polynomial ring $k[Y_1, \dotsc, Y_n]$ with the $k$-algebra isomorphism $k[Y_1, \dotsc, Y_n] \to k[X_1, \dotsc, X_n]^{S_n}$, $Y_i \mapsto e^{(n)}_i$.)
  We then have that
  \begin{align*}
        \Phi\left( h^{(n)}_i \right)
    &=  \Phi
        \left(
          P_i\left(
            e^{(n)}_1,
            \dotsc,
            e^{(n)}_n \right)
        \right) \\
    &=  P_i\left(
          \Phi\left(e^{(n)}_1\right),
          \dotsc,
          \Phi\left(e^{(n)}_n\right)
        \right)
      = P_i
        \left(
          h^{(n)}_1, \dotsc, h^{(n)}_n
        \right)
      = e^{(n)}_n \,.
  \end{align*}
  Hence the homomorphism $\Phi$ swaps $e^{(n)}_i$ with $h^{(n)}_i$ for every $i = 1, \dotsc, n$.
  It follows that $\Phi^2(e^{(n)}_i) = e^{(n)}_i$ for every $i = 1, \dotsc, n$, and therefore that $\Phi^2 = \id$ because $k[X_1, \dotsc, X_n]^{S_n}$ is generated by $e^{(n)}_1, \dotsc, e^{(n)}_n$.
  Thus we find the following:
\end{fluff}

\begin{corollary}
  There exists an unique $k$-algebra homomorphism
  \[
            \Phi
    \colon  k[X_1, \dotsc, X_n]^{S_n}
    \to     k[X_1, \dotsc, X_n]^{S_n}
  \]
  with $\Phi\left(e^{(n)}_i\right) = h^{(n)}_i$ for every $i = 1, \dotsc, n$, and $\Phi$ is an involutive automorphism.
\end{corollary}


\begin{corollary}
  The homogeneous symmetric polynomials $h^{(n)}_1, \dotsc, h^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent.
\end{corollary}


\begin{remark}
  As for the Fundamental Theorem of Symmetric Polynomials these results remain valid we replace $k$ with any non-zero commutative ring.
\end{remark}





\subsection{Power Symmetric Polynomials}


\begin{definition}
  For all $n, r \in \Natural$ the \emph{$r$-th power symmetric polynomial}, or \emph{$r$-th power sum} in $n$-variables is
  \[
              p_r^{(n)}
    \coloneqq X_1^r + \dotsb + X_n^r \,.
  \]
\end{definition}


\begin{definition}
  For all $n \in \Natural$ the power series $P^{(n)}(t) \in k[X_1, \dotsc, X_n]\!\dblbrack{t}$ is the generating series of the sequence $(p^{(n)}_r)_{r \geq 1}$, that is
  \[
            P^{(n)}(t)
  \defined  \sum_{r=0}^\infty p^{(n)}_{r+1} t^r \,.
  \]
  (Note the shift compared to $E^{(n)}$ and $H^{(n)}$.)
\end{definition}


\begin{lemma}
  \label{lemma: explicit formula for P}
  One has the equality of power series
  \[
      P^{(n)}(t)
    = \sum_{i=1}^n \frac{X_i}{1 - X_i t} \,.
  \]
  More generally, one has that for every $s \geq 0$ that
  \[
      \sum_{r=0}^\infty p_{r+s} t^r
    = \sum_{i=1}^n \frac{X_i^s}{1 - X_i t} \,.
  \]
\end{lemma}


\begin{proof}
  We have that
  \begin{align*}
        \sum_{i=1}^n \frac{X_i^s}{1-X_i t}
    &=  \sum_{i=1}^n X_i^s (1 + X_i t + X_i^2 t^2 + X_i^3 t^3 + \dotsb) \\
    &=  \sum_{i=1}^n (X_i^s + X_i^{s+1} t + X_i^{s+2} t^2 + X_i^{s+3} t^3 + \dotsb) \\
    &=  p^{(n)}_s + p^{(n)}_{s+1} t + p^{(n)}_{s+2} t^2 + p^{(n)}_{s+3} t^3 + \dotsb
    \qedhere
  \end{align*}
\end{proof}


\begin{fluff}
  \label{fluff: connection between E and P}
  From the explicit formulas for $E^{(n)}(t)$ and $P^{(n)}(t)$ from Lemma~\ref{lemma: explicit formula for E} and Lemma~\ref{lemma: explicit formula for P} it follows that
  \[
      \left( E^{(n)} \right)'(t)
    = \sum_{i=1}^n X_i \prod_{j \neq i} (1 + X_j t)
    = \sum_{i=1}^n \frac{X_i}{1 + X_i t} \prod_{j=1}^n (1 + X_j t)
    = P^{(n)}(-t)E^{(n)}(t) \,.
  \]
  The power series $(E^{(n)})'(t)$ is given by
  \[
      \left( E^{(n)} \right)'(t)
    = \sum_{r=1}^\infty r e^{(n)}_r t^{r-1}
    = \sum_{r=0}^\infty (r+1) e^{(n)}_{r+1} t^r \,,
  \]
  so by comparing the $(r-1)$-th coefficient we arrive at the \emph{Newton’s identities}.
\end{fluff}


\begin{corollary}[Newton’s identities]
  \label{corollary: Newtons identities}
  For every $r \geq 1$ one has that
  \[
      r e^{(n)}_r
    =   p^{(n)}_1 e^{(n)}_{r-1}
      - p^{(n)}_2 e^{(n)}_{r-2}
      + \dotsb
      + (-1)^{r-2}  p^{(n)}_{r-1} e^{(n)}_1
      + (-1)^{r-1}  p^{(n)}_r \,,
  \]
  and equivalently
  \[
        p^{(n)}_r
      - e^{(n)}_1 p^{(n)}_{r-1}
      + \dotsb
      + (-1)^{r-1} e^{(n)}_{r-1} p^{(n)}_1
      + (-1)^r r e^{(n)}_r
    = 0 \,.
  \]
\end{corollary}


\begin{fluff}
  We can proceed similiar as in \ref{fluff: connection between E and P} for the genarating functions $H^{(n)}(t)$ and $P^{(n)}(t)$:
  It follows from the explicit formulas for $H^{(n)}(t)$ and $P^{(n)}(t)$ from Lemma~\ref{lemma: explicit formula for H} and Lemma~\ref{lemma: explicit formula for P} that
  \begin{align*}
        \left( H^{(n)} \right)'(t)
    &=  \sum_{i=1}^n \frac{X_i}{(1-X_i t)^2} \prod_{j \neq i} \frac{1}{1 - X_j t} \\
    &=  \sum_{i=1}^n \frac{X_i}{1 - X_i t} \prod_{j=1}^n \frac{1}{1 - X_j t}
     =  P^{(n)}(t) H^{(n)}(t) \,.
  \end{align*}
  Since the power series $(H^{(n)})'(t)$ is given by
  \[
      \left( H^{(n)} \right)'(t)
    = \sum_{k \geq 1} k h^{(n)}_k t^{k-1}
  \]
  we get the following result by comparing the $r$-th coefficient:
\end{fluff}


\begin{corollary}
  \label{corollary: relation between h and p}
  For all $r \geq 1$ we have that
  \[
      r h^{(n)}_r
    =   p^{(n)}_1 h^{(n)}_{r-1}
      + p^{(n)}_2 h^{(n)}_{r-2}
      + \dotsb
      + p^{(n)}_{r-1} h^{(n)}_1
      + p^{(n)}_r.
  \]
\end{corollary}


\begin{fluff}
  We have seen that the symmetric polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ and $h^{(n)}_1, \dotsc, h^{(n)}_n$ each generate $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent.
  It is now only natural to ask if this also holds true for the power sums $p^{(n)}_1, \dotsc, p^{(n)}_n$.
  The next theorem shows that this holds under additional assumptions.
\end{fluff}


\begin{theorem}
  Let $k$ be a field with either $\kchar k = 0$ or $\kchar k > n$.
  Then $p^{(n)}_1, \dotsc, p^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent.
\end{theorem}
\begin{proof}
  Since $2, \dotsc, n$ are invertible in $k$ one can use the Newton identities (Corollary~\ref{corollary: Newtons identities}) to recursively express the elementary symmetric polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ in terms of the power sums $p^{(n)}_1, \dotsc, p^{(n)}_n$, starting off with $e^{(n)}_1 = p^{(n)}_1$.
  It follows that $p^{(n)}_1, \dotsc, p^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra.
  
  To show that $p^{(n)}_1, \dotsc, p^{(n)}_n$ are algebraically independent we need to show that the monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$, i.e.\ the polynomials
  \[
      \left(p^{(n)}\right)^{\underline{\alpha}}
    = \left(p^{(n)}_1\right)^{\alpha_1} \dotsm \left(p^{(n)}_n\right)^{\alpha_n}
    \quad\text{with}\quad
        \underline{\alpha}
    =   (\alpha_1, \dotsc, \alpha_n)
    \in \Natural^n
  \]
  are linearly independent.
  For this it sufficies to show for every $N \geq 1$ that the monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$ of degree $\leq N$ form a $k$-basis of the $k$-linear space of symmetric polynomials of degree $\leq N$, which we will denote by $V_N$.
  
  We also denote the number of (not necessarily distinct) monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$ of degree $\leq N$ by $P_N$, i.e.\ $P_N$ is the number of multi-indices $\underline{\alpha} \in \Natural^n$ with $\deg (p^{(n)})^{\underline{\alpha}} \leq N$.
  
  Note that for every $\underline{\alpha} \in \Natural^n$ we have that
  \begin{align*}
        \deg \left( p^{(n)} \right)^{\underline{\alpha}}
    &=  \deg
        \left( p^{(n)}_1 \right)^{\alpha_1}
        \dotsm 
        \left( p^{(n)}_n \right)^{\alpha_n} \\
    &=    \alpha_1 \deg p^{(n)}_1
        + \dotsb 
        + \alpha_n \deg p^{(n)}_n \\
    &=    \alpha_1 \cdot 1
        + \alpha_2 \cdot 2
        + \dotsb
        + \alpha_n \cdot n  \\
    &=    \alpha_1 \deg e^{(n)}_1
        + \dotsb 
        + \alpha_n \deg e^{(n)}_n \\
    &=  \deg
        \left( e^{(n)}_1 \right)^{\alpha_1}
        \dotsm 
        \left( e^{(n)}_n \right)^{\alpha_n}
     =  \left( e^{(n)} \right)^{\underline{\alpha}} \,,
  \end{align*}
  so that $P_N$ is also the number of monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$ of degree $\leq N$.
  Note that these monomials in the $e^{(n)}_i$ are pairwise distinct because the $e^{(n)}_i$ are algebraically independent.
  Hence $P_N$ is also the number of monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$ of degree $\leq N$.
  With the Fundamental Theorem of symmetric functions it follows that $\dim V_N = P_N$.
  
  Because $k[X_1, \dotsc, X_n]^{S_n}$ is generated as a $k$-algebra by the homogeneous elements $p^{(n)}_1, \dotsc, p^{(n)}_n$ it we find that $V_N$ is spanned as a $k$-linear subspace of $K[X_1, \dotsc, X_n]^{S_n}$ by the monomials in $p{(n)}_1, \dotsc, p^{(n)}_n$ of degree $\leq N$, of which they are $\leq P_N$ many distinct ones.
  It therefore follows from $\dim V_N = P_N$ that $V_N$ is a $k$-basis von $V_N$.
\end{proof}


\begin{remark}
  Note that the above theorem cannot hold for $k = \Integer$:
  To see this, note that in $\Rational[X_1, X_2]^{S_2}$ we have that
  \[
      e^{(2)}_2
    = \frac{1}{2} \left( p^{(2)}_1 \right)^2 - \frac{1}{2} p^{(2)}_2 \,.
  \]
  If $\Integer[X_1, X_2]^{S_2}$ would be generated by $p^{(n)}_1, p^{(n)}_2$ as a $\Integer$-algebra (i.e.\ ring) then there would exists some polynomial $F \in \Integer[Y_1, Y_2]$ with $e^{(2)}_2 = F( p^{(2)}_1, p^{(2)}_2)$.
  But this would then contradict the algebraic independence of $p^{(2)}_1, p^{(2)}_2$ in $\Rational[X_1, X_2]^{S_2}$, since $F(X_1, X_2) \neq \frac{1}{2} X_1^2 - \frac{1}{2} X_2$.
\end{remark}


% Using the same argumentation we find that for a symmetric polynomial $f \in \Rational[X_1, \dotsc, X_n]^{S_n}$ with integer coefficients and $F,G \in \Rational[X_1, \dotsc, X_n]^{S_n}$ with
% \[
%     f
%   = F\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
%   = G\left( h^{(n)}_1, \dotsc, h^{(n)}_n \right)
% \]
% both $F$ and $G$ must have integer coefficients.
% 


\begin{fluff}
  We have seen that the elementary symmetric polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ and the complete homogeneous symmetric polynomials $h^{(n)}_1, \dotsc, h^{(n)}_n$ are dual to each other in the sense that there exists a involutive algebra automorphism $\Phi$ of $k[X_1, \dotsc, X_n]^{S_n}$ which swaps $e^{(n)}_i$ and $h^{(n)}_i$ for every $i = 1, \dotsc, n$.
  We can determine the action of $\Phi$ on the power sums $p^{(n)}_1, \dotsc, p^{(n)}_n$.
  
  Applying $\Phi$ to Newton’s identities (Corollary~\ref{corollary: Newtons identities}) and comparing the result with Corollary~\ref{corollary: relation between h and p} seems to suggest that
  \[
      \Phi\left( p^{(n)}_r \right)
    = (-1)^{r-1} p^{(n)}_r
  \]
  for all $r = 1, \dotsc, n$.
  We can show this by induction on $r$:
  
  For $r = 1$ we have that
  \[
      \Phi\left( p^{(n)}_1 \right)
    = \Phi\left( e^{(n)}_1 \right)
    = h^{(n)}_1
    = p^{(n)}_1 \,.
    = (-1)^{r-1} p^{(n)}_1
  \]
  For $r > 1$ we apply $\Phi$ to the Newton identity
  \[
      r e^{(n)}_r
    =   p^{(n)}_1 e^{(n)}_{r-1}
      - p^{(n)}_2 e^{(n)}_{r-2}
      + \dotsb
      + (-1)^{r-2}  p^{(n)}_{r-1} e^{(n)}_1
      + (-1)^{r-1}  p^{(n)}_r \,,
  \]
  which by induction results in the identity
  \[
      r h^{(n)}_r
    =   p^{(n)}_1 h^{(n)}_{r-1}
      + p^{(n)}_2 h^{(n)}_{r-2}
      + \dotsb
      + p^{(n)}_{r-1} h^{(n)}_1
      + (-1)^{r-1} \Phi\left( p^{(n)}_r \right) \,.
  \]
  By comparing this to Corollary~\ref{corollary: relation between h and p} it follows that $\Phi( p^{(n)}_r ) = (-1)^{r-1} p^{(n)}_r$.
\end{fluff}


\noindent\hrulefill \, Current progress of reworking these notes. \hrulefill





\subsection{Symmetric Polynomials Associated to Partitions}















%%%%%%  Previous text


% \begin{example}
%   Another example of formal power series are Hilbert series.
%   Given a graded $k$-algebra $A = \bigoplus_{d \geq 0} A_d$ ($A_d = 0$ for $d < 0$) with $\dim_k A_d < \infty$ for all $d$ the corresponding Hilbert series is defined as
%   \[
%               P_A(t)
%     \coloneqq \sum_{d \geq 0} \left( \dim_k A_d \right) t^d
%     \in       k\dblbrack{t}.
%   \]
%   If $\dim_k A < \infty$ we have $P_A(t) \in k[t] \subseteq k\dblbrack{t}$.
%   
%   If $A = \bigoplus_{d \geq 0} A_d$ and $B = \bigoplus_{d \geq 0} B_d$ are graded $k$-algebras then $A \otimes_k B$ is a $k$-algebra via
%   \[
%       (a_1 \otimes b_1) (a_2 \otimes b_2)
%     = (a_1 a_2) \otimes (b_1 b_2)
%   \]
%   and a graded $k$-algebra $A \otimes B = \bigoplus_{d \geq 0} (A \otimes B)_d$ by setting
%   \[
%       (A \otimes B)_d
%     = \bigoplus_{i=0}^d (A_i \otimes B_{d-i}) \,.
%   \]
%   We than have
%   \[
%       \dim_k (A \otimes B)_d
%     = \sum_{i=0}^d \dim_k (A_i \otimes B_{d-i})
%     = \sum_{i=0}^d (\dim_k A_i) (\dim_k B_{d-i})
%   \]
%   for all $d \geq 0$ and thus
%   \[
%       P_{A \otimes B}(t)
%     = P_A(t) P_B(t) \,.
%   \]
% \end{example}



\begin{definition}
  $\lambda = (\lambda_1, \dotsc, \lambda_s) \in \Natural^s$ is \emph{a partition of $|\lambda| \coloneqq \sum_{i=1}^s \lambda_i$} if
  \[
          \lambda_1
    \geq  \lambda_2
    \geq  \dotsb
    \geq  \lambda_s
    \geq  0 \,.
  \]
  The $\lambda_i$ are the \emph{parts of $\lambda$} and $l(\lambda) \coloneqq s$ is the \emph{length of $\lambda$}.
  
  An \emph{infinite partition} is a sequence $\lambda_1, \lambda_2, \dotsc \in \Natural$ with $\lambda_i = 0$ for all $i \geq s$ for some $s$ such that $(\lambda_1, \dotsc, \lambda_s)$ is a partition.
  For a partition $(\lambda_1, \dotsc, \lambda_s) \in \Natural^s$ the \emph{infinite partition associated to $\lambda$} is defined as
  \[
              \hat{\lambda}_i
    \coloneqq \begin{cases}
                \lambda_i & \text{for } 1 \leq i \leq l(\lambda) \,,  \\
                        0 & \text{otherwise} \,.
              \end{cases}
  \]
  
  The \emph{transposed partition of a partition $\lambda$} is defined as
  \[
              \lambda'_i
    \coloneqq |\{j \mid \lambda_j \geq i\}| \,.
  \]
\end{definition}


Partitions are often displayed in terms of \emph{Young diagrams}.
The Young diagram corresponding to a partition $\lambda$ is an array of boxes, left adjusted, such that the $i$-th row consists of $\lambda_i$ boxes.


\begin{example}
  $\lambda = (4,2,2)$ is a partition of $8$ and $\lambda' = (3,3,1,1)$ is the transposed partion.
  The Young diagram corresponding to $\lambda'$ is the `transposition' of the Young diagram corresponding to $\lambda$.
  (See figure \ref{figure: Young diagram example}.)
  \begin{figure}
    \centering
    \ydiagram{4,2,2}
    \qquad
    \ydiagram{3,3,1,1}
    \caption{The Young diagrams corresponding to $\lambda$ (left) and to $\lambda'$ (right).}
    \label{figure: Young diagram example}
  \end{figure}
\end{example}


\begin{definition}
  For $n \in \Natural$ we write
  \[
              \Par(n)
    \coloneqq \{\text{partitions of $n$}\}
  \]
  and we set
  \[
              \Par
    \coloneqq \bigcup_{n \in \Natural} \Par(n) \,.
  \]
\end{definition}
  

\begin{definition}
  Let $\lambda$ and $\mu$ be partitions.
  We say that $\lambda \geq \mu$ if $|\lambda| = |\mu|$ and $\sum_{i=1}^r \hat{\lambda}_i \geq \sum_{i=1}^r \hat{\mu}_i$ for all $r$.
\end{definition}


\begin{example}
  The following is a simple example of partitions of $6$.
  \[
      \ydiagram{6}
    > \ydiagram{4,2}
    > \ydiagram{3,3}
    > \ydiagram{3,2,1}
    > \ydiagram{1,1,1,1,1,1}
  \]
  The partitions
  \[
    \ydiagram{2,2}
    \quad \text{and} \quad
    \ydiagram{1,1}
  \]
  are not comparable (because $4 \neq 2$ in $\Natural$). The partitions
  \[
    \ydiagram{4,2,1,1,1}
    \quad \text{and} \quad
    \ydiagram{3,3,2,1}
  \]
  are also not comparable.
  (Because $4+2+1 = 7 < 8 = 3+3+2$ in $\Natural$.)
\end{example}
%TODO: Adding a better explanation of this partial order. Bessere Erklärung der partiellen Ordnung hinzufügen.

\begin{lemma}
  $\geq$ defines a partial ordering on $\Par$.
\end{lemma}
\begin{proof}
  It is clear that $\geq$ is reflexive.
  
  If $\lambda$ and $\mu$ are partitions with $\lambda \geq \mu$ and $\lambda \leq \mu$ then $\lambda = \mu$:
  Because $\lambda \geq \mu$ we have $\lambda_1 \geq \mu_1$ and because $\lambda \leq \mu$ we have $\lambda_1 \leq \mu_1$.
  Thus we have $\lambda_1 = \mu_1$.
  In the same way we find that $\lambda_1 + \lambda_2 = \mu_1 + \mu_2$, and from $\lambda_1 = \mu_1$ we get that $\lambda_2 = \mu_2$.
  Because $|\lambda| = |\mu|$ we find inductively that $l(\lambda) = l(\mu)$ and $\lambda_i = \mu_i$ for all $1 \leq i \leq l(\lambda)$.
  
  If $\lambda, \mu$ and $\sigma$ are partitions with $\lambda \geq \mu$ and $\mu \geq \sigma$ then $\lambda \geq \sigma$:
  Because $\lambda \geq \mu$ we have $|\lambda| = |\mu|$ and because $\mu \geq \sigma$ we have $|\mu| = |\sigma|$.
  Thus we have $|\lambda| = |\sigma|$.
  For all $r \geq 1$ we have
  \[
          \sum_{i=1}^r \lambda_i
    \geq  \sum_{i=1}^r \mu_i
    \text{ and }
          \sum_{i=1}^r \mu_i
    \geq  \sum_{i=1}^r \sigma_i
  \]
  because $\lambda \geq \mu$ and $\mu \geq \sigma$, and therefore
  \[
          \sum_{i=1}^r \lambda_i
    \geq  \sum_{i=1}^r \sigma_i \,.
    \qedhere
  \]
\end{proof}


\begin{definition}
  Let $\lambda = (\lambda_1, \dotsc, \lambda_r)$ be a partition. We define the \emph{elementary symmetric polynomial}
  \[
              e^{(n)}_\lambda
    \coloneqq e^{(n)}_{\lambda_1} \dotsm e^{(n)}_{\lambda_r} \,,
  \]
  the \emph{complete symmetric polynomial}
  \[
              h^{(n)}_\lambda
    \coloneqq h^{(n)}_{\lambda_1} \dotsm h^{(n)}_{\lambda_r} \,,
  \]
  the \emph{power symmetric polynomial}
  \[
              p^{(n)}_\lambda
    \coloneqq p^{(n)}_{\lambda_1} \dotsm p^{(n)}_{\lambda_r}
  \]
  and the \emph{monomial symmetric polynomial}
  \[
              m_\lambda
    \coloneqq   X_1^{\lambda_1} \dotsm X_r^{\lambda_r}
              + \text{ all distinct permutations of this monomial} \,.
  \]
\end{definition}


One can also define $m_\lambda$ in a formal way:
Instead of adding up all distinct permutations of the monomial $X_1^{\lambda_1} \dotsm X_r^{\lambda_r}$ we can also take all distinct permutations of the tupel $\lambda$ and add up the corresponding monomials.
To formalize this we let $S_r$ act on $\Natural^r$ by permuting the entries, i.e.\
\[
    \pi.(a_1, \dotsc, a_r)
  = \left( a_{\pi^{-1}(1)}, \dotsc, a_{\pi^{-1}(r)} \right)
\]
for all $\pi \in S_r$ and $(a_1, \dotsc, a_r) \in \Natural^r$.
The set of all distinct permutations of $\lambda$ is precisely the orbit of $\lambda$ under this action.
As we know from basic group theory we have an isomorphism of $G$-sets
\[
          S_r / U
  \to     S_r \lambda,
  \quad   [\pi]
  \mapsto \pi.\lambda
\]
where $U$ is the stabilizer group of $\lambda$ and $S_r \lambda$ is the orbit of $\lambda$.
(Note that $U$ is not necessarily a normal subgroup in $S_r$ and $S_r/U$ is only the set of left cosets.)
Thus we can write
\[
    m_\lambda
  = \sum_{[\pi] \in S_r/U} X_1^{\lambda_{\pi^{-1}(1)}} \dotsm X_r^{\lambda_{\pi^{-1}(r)}}
  = \sum_{[\pi] \in S_r/U} X_{\pi(1)}^{\lambda_1} \dotsm X_{\pi(r)}^{\lambda_r} \,.
\]
Also notice that
\[
        U
  \cong S_{\nu_0} \times \dotsb \times S_{\nu_m}
\]
where
\[
            \nu_n
  \coloneqq \left|
              \left\{
                1 \leq i \leq r
              \mid
                  \lambda_i
                = n
              \right\}
            \right|
\]
and $m \coloneqq \max_{i=1,\dotsc,r} \lambda_i$.


\begin{example}[]
  Let $k$ be a field with $\kchar k \neq 2$.
  Let $\lambda = (\lambda_1, \dotsc, \lambda_n) \in \Par$ be a partition.
  The \emph{Schur polynomial corresponding to $\lambda$} is the symmetric polynomial $s_\lambda \in k[X_1, \dotsc, X_n]^{S_n}$ of homogenous degree $|\lambda|$ defined as
  \[
              s_\lambda
    \coloneqq \frac
              {
                \sum_{\sigma \in S_n} \sgn(\sigma)          X_{\sigma(1)}^{\lambda_1}
                                                            X_{\sigma(2)}^{\lambda_2 + 1}
                                                    \dotsm  X_{\sigma(n)}^{\lambda_n + n-1}
               }{
                \prod_{1 \leq i < j \leq n} (X_i - X_j)
               }
  \]
  (In the lecture the same statements were made for the ring of integers $\Integer$ and arbitrary fields, but the following argumentation does not work in these cases.)
  
  To show that $s_\lambda$ is well-defined we first notice the numerator
  \[
              N
    \coloneqq \sum_{\sigma \in S_n} \sgn(\sigma)          X_{\sigma(1)}^{\lambda_1}
                                                          X_{\sigma(2)}^{\lambda_2 + 1}
                                                  \dotsm  X_{\sigma(n)}^{\lambda_n + n-1}
  \]
  and the denumerator
  \[
              D
    \coloneqq \prod_{1 \leq i < j \leq n} (X_i - X_j)
  \]
  are alternating polynomials, i.e.\ $\sigma.N = \sgn(\sigma) N$ and $\sigma.D = \sgn(\sigma) D$ for every $\sigma \in S_n$, because
  \begin{gather*}
    N = \det
    \begin{pmatrix}
      X_1^{\lambda_1}       & X_2^{\lambda_1}       & \cdots & X_n^{\lambda_1}       \\
      X_1^{\lambda_2 + 1}   & X_2^{\lambda_2 + 1}   & \cdots & X_n^{\lambda_2 + 1}   \\
      \vdots                & \vdots                & \ddots & \vdots                \\
      X_1^{\lambda_n + n-1} & X_2^{\lambda_n + n-1} & \cdots & X_n^{\lambda_n + n-1}
    \end{pmatrix}
  \shortintertext{and}
    D = \det
    \begin{pmatrix}
      1      & X_1    & X_1^2  & \cdots & X_1^{n-1} \\
      1      & X_2    & X_2^2  & \cdots & X_2^{n-1} \\
      \vdots & \vdots & \vdots & \ddots & \vdots    \\
      1      & X_n    & X_n^2  & \cdots & X_n^{n-1}
    \end{pmatrix}.
  \end{gather*}
  That $D$ divides $N$ follows from the following claim:
  \begin{claim}
    Let $f \in k[X_1, \dotsc, X_n]$ be an alternating polynomial and
    \[
                V
      \coloneqq \prod_{1 \leq i < j \leq n} (X_i - X_j)
      \in       k[X_1, \dotsc, X_n] \,.
    \]
    Then $V$ divides $f$.
  \end{claim}
  \begin{proof}
    Since the polynomials $X_i - X_j$ with $1 \leq i < j \leq n$ are pairwise non-equivalent primes it sufficies to show that $X_i-X_j$ divides $f$ for all $1 \leq i < j \leq n$.
    Because $f$ is alternating it is enough to show that $X_1 - X_2$ divides $f$.
    
    For $R \coloneqq k[X_3, \dotsc, X_n]$, $u = X_1 + X_2$ and $x = X_1 - X_2$ we have
    \[
        k[X_1, \dotsc, X_n]
      = R[X_1, X_2]
      = R[u,v] \,,
    \]
    so we can write $f = \sum_{i \in \Natural} f_i v^i$ with $f_i \in R[u]$ for every $i \in \Natural$.
    Because $f$ is alternating we have
    \begin{align*}
           \sum_{i \in \Natural} f_i v^i
      &=   f(X_1, X_2, \dotsc, X_n)
       =  -f(X_2, X_1, \dotsc, X_n) \\
      &=  -\sum_{i \in \Natural} (-1)^i f_i v^i
       =   \sum_{i \in \Natural} (-1)^{i+1} f_i v^i \,.
    \end{align*}
    So $f_i = 0$ if $i$ is even.
    Therefore $v$ divides $f$ .
  \end{proof}
  Since $D$ and $N$ are both alternating it is also clear that $s_\lambda = N/D$ is symmetric.
  To see that $s_\lambda$ is homogeneous of degree $|\lambda|$ notice that $N$ is homogeneous of degree
  \[
      \lambda_1 + (\lambda_2 + 1) + \dotsb + (\lambda_n + n-1)
    = |\lambda| + \binom{n}{2}
  \]
  and that $D$ is homogeneous of degree $\binom{n}{2}$.
  \begin{claim}
    Let $f, g \in k[X_1, \dotsc, X_n]$ be polynomials such that $f$ is homogenous of degree $d_1$ and $g$ homogeneous of degree $d_2$.
    If $g$ divides $f$ then $f/g$ is homogenous of degree $d_1 - d_2$.
  \end{claim}
  \begin{proof}
    We can write $f/g = \sum_{d \in \Natural} h_d$ where $h_d \in k[X_1, \dotsc, X_n]$ is homogenous of degree $d$.
    Then $f = (f/g)g = \sum_{d \in \Natural} h_d g$ where $h_d g$ is homogeneous of degree $d + d_2$.
    Because $f$ is homogenous of degree $d_1$ we find that $h_d = 0$ for $d \neq d_1 - d_2$.
    Thus $f/g = h_d$ is homogeneous of degree $d_1 - d_2$.
  \end{proof}
\end{example}


\begin{lemma}
  The set of monomial symmetric polynomials
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par,
      l(\lambda) = n
    \}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$ (as a $k$-vector space). More precisely
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par(d),
      l(\lambda) = n
    \}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}_d$.
\end{lemma}
\begin{proof}
  Is is clear that
  \[
            \vspan_k \{
                        m_\lambda
                      \mid
                        \lambda \in \Par,
                        l(\lambda) = n,
                        |\lambda| = d
                      \}
  \subseteq k[X_1, \dotsc, X_n]^{S_n}_d \,.
  \]
  On the other side let $f \in k[X_1, \dotsc, X_n]^{S_n}_d$. By induction on the number of monomials of which $f$ consists we show that
  \[
        f
    \in \vspan_k  \{
                    m_\lambda
                  \mid
                    \lambda \in \Par,
                    l(\lambda) = n,
                    |\lambda| = d
                  \} \,.
  \]
  For $f = 0$ this is clear.
  Suppose that $f \neq 0$ and that the statement is true for every polynomial in $k[X_1, \dotsc, X_n]^{S_n}_d$ which consists of fewer monomials than $f$.
  Because $\{X^\alpha \mid \alpha \in \Natural^n, |\alpha| = d \}$ is a $k$-basis of $k[X_1, \dotsc, X_n]_d$ we can write
  \[
      f
    = \sum_{\substack{\alpha \in \Natural^n \\ |\alpha| = d}} c_\alpha X^\alpha \,.
  \]
  with unique $c_\alpha \in k$ such that $c_\alpha \neq 0$ for only finitely many $\alpha$.
  Because $f$ is symmetric we find that
  \[
      c_\alpha
    = c_{\pi.\alpha}
    \text{ for all }
    \alpha \in \Natural^n,
    \pi \in S_n
  \]
  (where the action of $S_n$ on $\Natural^n$ is defined as above).
  Let $X^\beta$ be a monomial of $f$.
  Because $f \in k[X_1, \dotsc, X_n]^{S_n}_d$ we have $X^\beta \in k[X_1, \dotsc, X_n]_d$ and thus $c_\beta m_\beta \in k[X_1, \dotsc, X_n]^{S_n}_d$.
  Because $c_\beta \neq 0$ and $c_\beta = c_{\pi.\beta}$ for every $\pi \in S_n$ we find that $f - c_{\beta} m_\beta$ consists of fewer monomials than $f$.
  Because $f-c_{\beta} m_\beta$ is symmetric we find by induction hypothesis that
  \[
        f - c_{\beta} m_\beta
    \in \vspan_k  \{
                    m_\lambda
                  \mid
                    \lambda \in \Par,
                    l(\lambda) = n,
                    |\lambda| = d
                  \} \,.
  \]
  The statement for $f$ follows directly.
  
  To show that
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par,
      l(\lambda) = n
    \}
  \]
  is linear independent we notice that for $\lambda, \mu \in \Par$ with $\lambda \neq \mu$ the polynomials $m_\lambda$ and $m_\mu$ have no monomials in common. Because
  \[
    \{
      X^\alpha
    \mid
      \alpha \in \Natural^n
    \}
  \]
  is linear independent it then follows that
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par,
      l(\lambda) = n
    \}
  \]
  is linear independent.
\end{proof}


\begin{definition}
  Let $\lambda, \mu \in \Par$.
  We define the partition $\lambda+\mu$ of length $l(\lambda+\mu) = \max\{ l(\lambda), l(\mu) \}$ as
  \[
      (\lambda+\mu)_i
    = \begin{cases}
        \lambda_i + \mu_i & \text{if } i \leq l(\lambda),l(\mu)       \,, \\
        \lambda_i         & \text{if } i \leq l(\lambda), i > l(\mu)  \,, \\
        \mu_i             & \text{if } i \leq l(\mu), i > l(\lambda)  \,.
      \end{cases}
  \]
  So $\lambda+\mu$ is the partition of minimal length with
  \[
      \hat{\lambda}_i + \hat{\mu}_i
    = \widehat{\lambda + \mu}_i
    \text{ for all }
    i \geq 0 \,.
  \]
\end{definition}


\begin{example}
  For $\lambda = (4,4,2,2)$ and $\mu = (3,2,1)$ we have $\lambda + \mu = (7,6,3,2)$.
  The addition of two partitions can also easily be visualized with Young diagrams, see figure \ref{figure: addition partition young diagrams}.
  \begin{figure}\centering
    \[
        \ydiagram[*(gray)]{4,4,2,2} + \ydiagram[*(light-gray)]{3,2,1}
      = \ydiagram[*(light-gray)] {4+3,4+2,2+1} *[*(gray)]{7,6,3,2}
    \]
    \caption{Addition of partitions in term of Young diagrams.}
    \label{figure: addition partition young diagrams}
  \end{figure}
\end{example}


\begin{lemma}
  Let $\lambda, \mu \in \Par$.
  Then
  \[
      m_{\lambda} m_{\mu}
    =   m_{\lambda + \mu}
      + \sum_{\nu < \lambda + \mu} a^\nu_{\lambda,\mu} m_\nu
  \]
  for suitable $a^\nu_{\lambda,\mu} \in k$.
  (This also holds for $k = \Integer$.)
\end{lemma}
\begin{proof}
  This is an exercise on the 4th exercise sheet.
\end{proof}


We know from the Fundamental Theorem that $e^{(n)}_1, \dotsc, e^{(n)}_n$ generate the $k$-algebra of symmetric polynomials $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent. This is equivalent to saying that the monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$, i.e.\
\[
    \left( e^{(n)} \right)^\alpha
  = \left( e^{(n)}_1 \right)^{\alpha_1} \dotsm \left( e^{(n)}_n \right)^{\alpha_n}
  \text{ with }
  \alpha = (\alpha_1, \dotsc, \alpha_n)
\]
form a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$.
We can also describe these polynomials in term of partitions.


\begin{proposition}
  The monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$ are precisely the polynomials $e^{(n)}_\lambda$ with $\lambda \in \Par$ such that $\lambda_1 \leq n$ and $\lambda_{l(\lambda)} \neq 0$.
\end{proposition}
\begin{proof}
  On the one side the monomial $e_1^{\alpha_1} \dotsm e_n^{\alpha_n}$ is the same as $e_\lambda$ for the partition
  \[
      \lambda
    = (
        \underbrace{n, \dotsc, n}_{\alpha_n},
        \underbrace{n-1, \dotsc, n-1}_{\alpha_{n-1}},
        \dotsc,
        \underbrace{1, \dotsc, 1}_{\alpha_1}
      ).
  \]
  On the other side the polynomial $e_\lambda$ equals the monomial $e_1^{\nu_1(\lambda)} \dotsm e_n^{\nu_n(\lambda)}$ for the exponents
  \[
      \nu_j(\lambda)
    = |
        \{
          1 \leq i \leq l(\lambda)
        \mid
          \lambda_i = j
        \}
      |.
  \]
  
  Notice that for $\lambda \neq \mu$ there exists some $1 \leq j \leq n$ with $\nu_j(\lambda) \neq \nu_j(\mu)$ (here we use that $1 \leq \lambda_i \leq n$ for all $1 \leq i \leq l(\lambda)$ and the same for $\mu$) and therefore
  \[
          e_1^{\nu_1(\lambda)} \dotsm e_n^{\nu_n(\lambda)}
    \neq  e_1^{\nu_1(\mu)} \dotsm e_n^{\nu_n(\mu)}
  \]
  because the monomials in $e_1, \dotsc, e_n$ are linearly independent.
\end{proof}


\begin{corollary}
  The set
  \[
              B
    \coloneqq \left\{
                e^{(n)}_\lambda
              \,\middle|\,
                \lambda \in \Par,
                \lambda_1 \leq n,
                \lambda_{l(\lambda)} \neq 0
              \right\}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$ and the $e^{(n)}_\lambda$ are pairwise different.
  More precisely
  \[
              B_d
    \coloneqq \left\{
                e^{(n)}_\lambda
              \,\middle|\,
                \lambda \in \Par,
                \lambda_1 \leq n,
                \lambda_{l(\lambda)} \neq 0,
                |\lambda| = d
              \right\}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}_d$ for all $d \geq 0$.
\end{corollary}


\begin{remark}
  Since $h^{(n)}_1, \dotsc, h^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent we can show the same statements for the polynomials $h^{(n)}_\lambda$ with $\lambda \in \Par$ such that $\lambda_1 \leq n$ and $\lambda_{l(\lambda)} \neq 0$.
  
  In the case that $k$ is a field with $\kchar k = 0$ or $\kchar k > n$ we can show the same for the polynomials $p^{(n)}_\lambda$ with $\lambda \in \Par$ such that $\lambda_1 \leq n$ and $\lambda_{l(\lambda)} \neq 0$.
\end{remark}


% TODO: Adding the basis of schur polynomials.
