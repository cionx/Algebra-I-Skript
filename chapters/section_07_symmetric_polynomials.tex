\section{Symmetric Polynomials}

%%%%%%  New text

\begin{fluff}
  The symmetric group $S_n$ acts by $k$-algebra automorphisms on the polynomial ring $k[X_1, \dotsc, X_n]$ by
  \[
      \sigma.f(X_1, \dotsc, X_n)
    = f(X_{\sigma(1)}, \dotsc, X_{\sigma(n)}) \,.
  \]
  In this section we will be concerned by the $k$-algebra of invariants $k[X_1, \dotsc, X_n]^{S_n}$.
\end{fluff}


\begin{definition}
  Let $k$ be a field.
  The polynomial $f \in k[X_1, \dotsc, X_n]^{S_n}$ are \emph{symmetric}, and $k[X_1, \dotsc, X_n]^{S_n}$ is the \emph{ring of symmetric polynomials \textup(in $n$ variables\textup) \textup(over $k$\textup)}.
\end{definition}


\begin{example}
  \label{example: symmetric polynomials}
  In $k[X_1, X_2, X_3]$ we have the symmetric polynomials
  \begin{align*}
                p_2
    &\coloneqq  X_1^2 + X_2^2 + X_3^2 \,,
    \\
                h_2
    &\coloneqq  X_1^2 + X_1 X_2 + X_1 X_3 + X_2^2 + X_2 X_3 + X_3^2 \,,
    \\
                e_2
    &\coloneqq  X_1 X_2 + X_1 X_3 + X_2 X_3 \,,
    \\
                m_{(4,4,2)}
    &\coloneqq  X_1^4 X_2^2 X_3^2 + X_1^2 X_2^4 X_3^2 + X_1^2 X_2^2 X_3^4 \,.
  \end{align*}
  In the next subsections we will generalize these examples.
\end{example}


\begin{lemma}
  \label{lemma: symmetric iff all homogeneous parts are symmetric}
  With respect to the usual grading $k[X_1, \dotsc, X_n] = \bigoplus_{d \in \Natural} k[X_1, \dotsc, X_n]_d$ a polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if all of its homogeneous parts are symmetric.
\end{lemma}


\begin{proof}
  The decomposition $k[X_1, \dotsc, X_n] = \bigoplus_{d \geq 0} k[X_1, \dotsc, X_n]_d$ is a decomposition into subrepresentations of $S_n$, thus the claim follows from Lemma~\ref{lemma: direct sum and invariants commute}.
\end{proof}

\begin{fluff}
  In the following subsections we will consider families of symmetric polynomials which generalize the polynomials given in Example~\ref{example: symmetric polynomials}.
  
  We will start off with the so called \emph{elemantary symmetric polynomials}.
  We prove the famous \emph{Fundamental Theorem of Symmetric Polynomials}, which roughly states that every every symmetric polynomial can be uniquely expressed in terms of the elementary symmetric polynomials.
  
  We will then use the elementary symmetric polynomials to study other kinds of symmetric polynomials:
  Namely the \emph{complete homogeneous symmetric polynomials}, \emph{power sums} \emph{monomial symmetric polynomials}.
  Along the way we will also introduce \emph{partitions} as a natural way for labeling these different kinds of symmetric polynomials.
\end{fluff}


\begin{notation}
  For this section we will fix a number of polynomials $n \in \Natural$.
\end{notation}





\subsection[Elementary Symmetric Polynomials \\ \& Fundamental Theorem of Symmetric Polynomials]{Elementary Symmetric Polynomials \& Fundamental Theorem of Symmetric Polynomials}


\begin{definition}
  For all $r \in \Natural$ the \emph{$r$-th elementary symmetric polynomial} (in $n$ variables) is
  \[
              e_r
    \defined  \sum_{1 \leq i_1 \leq \dotsb \leq i_r \leq n} X_{i_1} \dotsm X_{i_r}
    =         \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \, \prod_{i \in I} X_i \,.
  \]
  In particular $e_0 = 1$ and $e_r = 0$ for every $r > n$.
\end{definition}


\begin{fluff}
  For all $a_1, \dotsc, a_n \in k$ we have that
  \begin{align*}
     &\, (t-a_1) \dotsm (t-a_n) \\
    =&\, t^n  - (a_1 + \dotsb + a_n) t^{n-1}
              + \left( \sum_{1 \leq i_1 < i_2 \leq n} a_{i_1} a_{i_2} \right) t^{n-2}
              + \dotsb
              + (-1)^n a_1 \dotsm a_n \\ 
    =&\,    t^n
          - e_1(a_1, \dotsc, a_n) t^{n-1}
          + e_2(a_1, \dotsc, a_n) t^{n-2}
          - \dotsb
          + (-1)^n e_n(a_1, \dotsc, a_n)
  \end{align*}
  in $k[t]$.
  We will formalize this observation in the following lemma:
\end{fluff}


\begin{lemma}
  \label{lemma: natural occurence of elementary symmetric polynomials}
  For all $n \in \Natural$ we have in $k[X_1, \dotsc, X_n][t]$ the equality
  \begin{align*}
        \prod_{i=1}^n (t-X_i)
    &=    e_0 t^n
        - e_1 t^{n-1}
        + e_2 t^{n-2}
        - \dotsb
        + (-1)^n e_n  \\
    &=    t^n
        - e_1 t^{n-1}
        + e_2 t^{n-2}
        - \dotsb
        + (-1)^n e_n
  \end{align*}
\end{lemma}
\begin{proof}
  On both sides the $r$-th coefficient is given by $\prod_{I \subseteq \{1, \dotsc, n\}, |I| = r} \prod_{i \in I} X_i $.
\end{proof}


\begin{theorem}[Fundamental Theorem of Symmetric Functions]
  The symmetric polynomials $e_1, \dotsc, e_n$ generate the $k$-algebra of symmetric functions $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent, i.e.\ the unique $k$-algebra homomorphism
  \[
            k[Y_1, \dotsc, Y_n]
    \to     k[X_1, \dotsc, X_n]^{S_n} \,,
    \quad   Y_r
    \mapsto e_r
  \]
  is an isomorphism of $k$-algebras.
\end{theorem}


\begin{fluff}
  We will give two proofs of the fundamental theorem.
  The one given in the lecture is the second one.
\end{fluff}


\begin{proof}[First Proof of the Fundamental Theorem]
  \label{label: first proof of fundamental theorem}
  To makes our lifes easier we introduce an ordering on the set monomials in $k[X_1, \dotsc, X_n]$:
  
  For this we first order the monomials by their power of $X_1$ in decreasing order.
  The monomials with the same power of $X_1$ are then ordered in decreasing order by their power of $X_2$.
  We then continue this process trough the variables $X_3, \dotsc, X_n$.
  
  For any two monomials $X^\alpha = X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ and $X^\beta = X_1^{\beta_1} \dotsm X_n^{\beta_n}$ we thus have $X^\alpha > X^\beta$ if and only if there exists some $1 \leq i \leq n$ such that $\alpha_j = \beta_j$ for all $j < i$ and $\alpha_i > \beta_i$.
  This gives a well-ordering on the set of monomials in $k[X_1, \dotsc, X_n]$.
  
  For any polynomial $p \in k[X_1, \dotsc X_n]$ with $p \neq 0$ we define the initial term $\init p$ to be the highest monomial occuring in $p$, including its coefficient.
  Then the following properties hold:
  \begin{itemize}
    \item
      If $p \neq 0$ is symmetric then for $\init p = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ one has $\alpha_1 \geq \alpha_2 \geq \dotsb \geq \alpha_n$.
    \item
      For $p, q \in k[X_1, \dotsc, X_n]$ with $p, q \neq 0$ one has $\init (p \cdot q) = \init p \cdot \init q$.
    \item
      For all $1 \leq k \leq n$ one has $\init e_k = X_1 \dotsm X_k$ .
  \end{itemize}
  With this we are now well-equipped to prove the theorem:
  
  We first show that $e_1, \dotsc, e_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra.
  For this let $f \in k[X_1, \dotsc, X_n]^{S_n}$ with $f \neq 0$.
  By Lemma~\ref{lemma: symmetric iff all homogeneous parts are symmetric} we may assume that $f$ is homogeneous of degree $d \geq 0$.
  For
  \[
      \init f
    = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
  \]
  we then have $d = \alpha_1 + \dotsb + \alpha_n$.
  
  We consider the polynomial
  \[
      p
    =         c
              e_1^{\alpha_1 - \alpha_2}
      \dotsm  e_{n-1}^{\alpha_{n-1} - \alpha_n}
              e_n^{\alpha_n} \,.
  \]
  Then $\init p = \init f$ by the above properties of $\init$.
  Because $e_k$ is homogenous of degree $k$ it follows that $p$ is homogeneous of degree
  \begin{align*}
     &\,  (\alpha_1-\alpha_2) + 2(\alpha_2-\alpha_3) + \dotsb + (n-1)(\alpha_{n-1}-\alpha_n) + n\alpha_n \\
    =&\,  \alpha_1 + \dotsb + \alpha_n
    =     d \,.
  \end{align*}
  Combining these observations we find that $f-p$ is a homogeneous symmetric polynomial of degree $d$ with either $f-p = 0$ or at least $\init (f-p) < \init f$.
  
  Because there are only finitely many monomials of homogeneous degree $d$ we can repeat the above process to arrive at the zero polynomial in finitely many steps.
  Hence $f$ can be expressed as a poylynomial in $e_1, \dotsc, e_n$.
  
  To show that $e_1, \dotsc, e_n$ are algebraically independent we need to show that the monomials in $e_1, \dotsc, e_n$, i.e.\ the polynomials
  \[
      e^{\,\underline{\alpha}}
    = e_1^{\alpha_1} \dotsm e_n^{\alpha_n}
    \quad\text{for}\quad
        \underline{\alpha}
    =   (\alpha_1, \dotsc, \alpha_n)
    \in \Natural^n
  \]
  are linearly independent.
  For this we notice that for all $\underline{\alpha} \neq \underline{\beta}$ we have that
  \begin{align*}
            \init e^{\,\underline{\alpha}}
       =&\, \init e_1^{\alpha_1} \dotsm e_n^{\alpha_n}   \\
       =&\, X_1^{\alpha_1 + \dotsb + \alpha_n} X_2^{\alpha_2 + \dotsb + \alpha_n} \dotsm X_n^{\alpha_n} \\
    \neq&\, X_1^{\beta_1 + \dotsb + \beta_n}   X_2^{\beta_2  + \dotsb + \beta_n}  \dotsm X_n^{\beta_n}  \\
       =&\, \init e_1^{\beta_1} \dotsm e_n^{\beta_n}
       =    \init e^{\,\underline{\beta}}
  \end{align*}
  so that the polynomials $e^{\,\underline{\alpha}}$ for $\underline{\alpha} \in \Natural^n$ are pairwise different.
  
  Now suppose that
  \[
      0
    = \lambda_1 e^{\,\underline{\alpha}_1} + \dotsb + \lambda_s e^{\,\underline{\alpha}_s}
  \]
  with $s \geq 1$, $\underline{\alpha}_i \neq \underline{\alpha}_j$ for $i \neq j$ and $\lambda_i \neq 0$ for all $1 \leq i \leq s$.
  We can assume w.l.o.g.\ that 
  \[
      \init e^{\,\underline{\alpha}_1}
    > \init e^{\,\underline{\alpha}_2}
    > \dotsb
    > \init e^{\,\underline{\alpha}_s}
  \]
  since all of these terms are pairwise different.
  It follows that the initial term $\init e^{\,\underline{\alpha}_1}$ occures only in $e^{\,\underline{\alpha}_1}$ and in no ther of the $e^{\,\underline{\alpha}_i}$.
  From $\lambda_1 = 0$, in contradiction to $\lambda_1 \neq 0$.
\end{proof}


\begin{proof}[Second Proof of the Fundamental Theorem]
  During this proof we will denote the $r$-th elementary symmetric polynomial in $n$ variables by $e^{(n)}_r$.
  
  Note that
  \begin{equation}
    \label{equation: recursive formel for elementary symmetric polynomials}
    \tag{$\ast$}
    \begin{aligned}
          e^{(n)}_r
      =  \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i
      &=    \sum_{\substack{I \subseteq \{1, \dotsc, n-1\} \\ |I| = r}} \prod_{i \in I} X_i
          + \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r-1}} \left( \prod_{i \in I} X_i \right) X_n  \\
      &=  e^{(n-1)}_r + e^{(n)}_{r-1} X_n \,.
    \end{aligned}
  \end{equation}
  \begin{claim}
    A polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if $f$ can be written as a polyonmial in $e^{(n)}_1, \dotsc, e^{(n)}_n$, i.e.\ we have that
    \[
        k\left[ e^{(n)}_1, \dotsc, e^{(n)}_n \right]
      = k[X_1, \dotsc, X_n]^{S_n} \,.
    \]
  \end{claim}
  \begin{proof}[Proof of claim]
    Because the polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n \in k[X_1, \dotsc, X_n]^{S_n}$ are symmetric it follows that $k[ e^{(n)}_1, \dotsc, e^{(n)}_n ] \subseteq k[X_1, \dotsc, X_n]^{S_n}$.
    We show the other inclusion by induction over $n$.
    For $n = 1$ we have that $k[ e^{(1)}_1 ] = k[X_1] = k[X_1]^{S_1}$.
    
    Let $n \geq 2$ and suppose that the claim holds for $n-1$.
    We show the claim for $n$ by induction over the (total) degree $d \defined \deg f$.
    If $f$ is constant than the claim holds.
    So let $d \geq 1$ and suppose the claim holds for degrees $0, \dotsc, d-1$.
    By the Lemma~\ref{lemma: symmetric iff all homogeneous parts are symmetric} we may assume that $f$ is homogenous.
    (The homogenous parts of lower degree are by induction hypothesis expressable as polynomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$.)
    
    Let
    \[
              \Phi
      \colon  k[X_1, \dotsc, X_n]
      \to     k[X_1, \dotsc, X_{n-1}] \,,
      \quad   f(X_1, \dotsc, X_n)
      \mapsto f(X_1, \dotsc, X_{n-1}, 0)
    \]
    be the evaluation at $X_n = 0$.
    By \eqref{equation: recursive formel for elementary symmetric polynomials} we have that
    \begin{align*}
          \Phi\left( e^{(n)}_r \right)
      &=  e^{(n-1)}_r
      \quad \text{for all $1 \leq r < n$} \,,
      \\
          \Phi\left( e^{(n)}_n \right)
      &=  0 \,.
    \end{align*}
    Note that $\Phi(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric:
    Because $f$ is symmetric we have that
    \[
        f
      = f(X_1, \dotsc, X_n)
      = f(X_{\sigma(1)}, \dotsc, X_{\sigma(n)})
    \]
    for every $\sigma \in S_n$, und thus we have that
    \[
        f(X_1, \dotsc, X_{n-1}, 0)
      = f(X_{\tau(1)}, \dotsc, X_{\tau(n-1)}, 0)
    \]
    for every $\tau \in S_{n-1}$.
    Because $\Phi(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric we can use the induction hypothesis (from the induction on $n$) to write
    \[
        \Phi(f)
      = P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right)
    \]
    for some polynomial $P \in k[Y_1, \dotsc, Y_{n-1}]$.
    Consider the symmetric polynomial
    \[
                g
      \coloneqq P\left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1} \right)
      \in       k[X_1, \dotsc, X_n] \,.
    \]
    
    Because $\Phi$ is a homomorphism of $k$-algebras we find that
    \begin{align*}
         \Phi(g)
      &= \Phi\left( P\left(e^{(n)}_1, \dotsc, e^{(n)}_{n-1}\right) \right) \\
      &= P\left( \Phi\left(e^{(n)}_1\right), \dotsc, \left(e^{(n)}_{n-1}\right) \right) \\
      &= P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right)
       = \Phi(f)
    \end{align*}
    and therefore that $\Phi(f - g) = 0$.
    Note that $\ker \Phi = (X_n)$ by the commutativity of the following diagram:
    \[
      \begin{tikzcd}
          k[X_1, \dotsc, X_n]
          \arrow{rr}[above]{\Phi}
          \arrow{rd}[below left]{p \mapsto \class{p}}
        & {}
        & k[X_1, \dotsc, X_{n-1}]
        \\
          {}
        & k[X_1, \dotsc, X_n]/(X_n)
          \arrow{ru}[above,rotate=20]{\sim}[below right]{\class{p} \mapsto p(X_1, \dotsc, X_n, 0)}
        & {}
      \end{tikzcd}
    \]
    It therefore follows from $\Phi(f - g) = 0$ that $X_n \mid (f-g)$.
    Because $f-g$ is symmetric (because both $f$ and $g$ are symmetric) it follows that $X_i \mid (f-g)$ for all $1 \leq i \leq n$, and therefore that $X_1 \dotsm X_n \mid (f-g)$.
    We can thus consider the polynomial
    \[
                h
      \defined  \frac{f-g}{X_1 \dotsm X_n}
      =         \frac{f-g}{e^{(n)}_n} \,.
    \]
    (This quotient is well-defined because $k[X_1, \dotsc, X_n]$ is an integral domain.)
    
    \begin{claim}
      The polynomial $h$ is symmetric.
    \end{claim}
    \begin{proof}
      From $h e^{(n)}_n = f-g$ it follows for every $\sigma \in S_n$ that
      \[
          (\sigma.h) e^{(n)}_n
        = (\sigma.h) (\sigma.e^{(n)}_n)
        = \sigma.(h e^{(n)}_n)
        = \sigma(f-g)
        = \sigma.f - \sigma.g
        = f - g \,.
      \]
      Hence it follows that $\sigma.h = (f-g)/e^{(n)}_n = h$.
    \end{proof}
    
    \begin{claim}
      We have that $\deg g \leq \deg f$ and therefore that $\deg h < \deg f$.
    \end{claim}
    \begin{proof}
      ?
%     TODO: Adding a proof.
    \end{proof}
    
    By induction hypothesis (of the induction on $d$) we can write $h$ as a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$.
    Because $g$ is also a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$ it further follows that $f = e^{(n)}_n h + g$ is a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$.
  \end{proof}
  
  We now prove that the polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ are algebraically independent by induction over $n$.
  It holds for $n = 1$ because $e^{(1)}_1 = X_1$.
  
  Now suppose $n \geq 2$ and that the elements $e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1}$ are algebraically independent.
  Suppose that
  \[
      F\left(e^{(n)}_1, \dotsc, e^{(n)}_n\right)
    = 0
  \]
  for some polynomial $F \in k[Y_1, \dotsc, Y_n]$ with $F \neq 0$ of minimal possible degree.
  Then
  \begin{align*}
        0
    &=  \Phi \left( F \left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1}, e^{(n)}_n \right) \right) \\
    &=  F \left(
            \Phi\left( e^{(n)}_1 \right),
            \dotsc,
            \Phi\left( e^{(n)}_{n-1} \right),
            \Phi\left( e^{(n)}_n \right)
          \right) \\
    &=  F \left( e^{n-1}_1, \dotsc, e^{(n-1)}_{n-1}, e^{(n-1)}_n \right)
     =  F \left( e^{n-1}_1, \dotsc, e^{(n-1)}_{n-1}, 0 \right).
  \end{align*}
  From the induction hypothesis it follows that $F(Y_1, \dotsc, Y_{n-1}, 0) = 0$, and therefore that $Y_n \mid F$.
  So there exists some polynomial $\hat{F} \in k[Y_1, \dotsc, Y_n]$ with $F = Y_n \hat{F}$.
  Note that $\hat{F} \neq 0$ since $F \neq 0$, and that $\deg \hat{F}  <\deg F$.
  We then have
  \[
      0
    = F\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = e^{(n)}_n \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right).
  \]
  Because $k[X_1, \dotsc, X_n]$ is an integral domain it now further follows from $e^{(n)}_n \neq 0$ that
  \[
      \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = 0 \,.
  \]
  This contradits the minimality of $F$.
\end{proof}


\begin{remark}
  Each of the proofs gives us an algorithm how to express a symmetric polynomial in terms of $e_1, \dotsc, e_n$.
\end{remark}


\begin{remark}
  The first proof shows that the fundemental theorem does not only hold if $k$ is a field, but for every nonzero commutative ring $R$.
  It does in particular hold for $k = \Integer$.
  
  The second proof can be slightly modified to also work for $R$:
  Instead of using that $R[X_1, \dotsc, X_n]$ is an integral domain (which holds if and only if $R$ itself is an intgeral domain), it sufficies to realize that the polynomial $X_1 \dotsm X_n = e_n$ is a non-zero divisor.
\end{remark}


\begin{example}
  Let $p(t) = t^n + a_{n-1} t^{n-1} + \dotsb + a_1 t + a_0 \in k[t]$ be a polynomial, and let $\lambda_1, \dotsc, \lambda_n$ be the roots of $p(t)$ is an algebraic closure $\overline{k}$ of $k$.
  Then
  \[
      a_i
    = (-1)^{n-i} e_{n-i}(\lambda_1, \dotsc, \lambda_n)
  \]
  for all $i$ by Lemma~\ref{lemma: natural occurence of elementary symmetric polynomials}.
  It follows from the fundamental theorem that every symmetric polynomial in the roots $\lambda_1, \dotsc, \lambda_n$ can already be expressed as a polynomial in the coefficients $a_0, \dotsc, a_{n-1}$.
  
  Consider for example the \emph{discriminant}
  \[
      \Delta(p)
    = \prod_{i < j} (\lambda_i - \lambda_j)^2
    = (-1)^{n(n-1)/2} \prod_{i \neq j} (\lambda_i - \lambda_j)
  \]
  The polynomial $D(X_1, \dotsc, X_n) \defined \prod_{i < j} (X_i - X_j)^2$ is symmetric, which is why there exists a (unique) polynomial $f \in k[Y_1, \dotsc, Y_n]$ with $D = f(e_1, \dotsc, e_n)$.
  Then
  \[
      \Delta(p)
    = D(\lambda_1, \dotsc, \lambda_n)
    = f(e_1(\lambda_1, \dotsc, \lambda_n), \dotsc, e_n(\lambda_1, \dotsc, \lambda_n))
    = f(a_{n-1}, \dotsc, a_0) \,.
  \]
  This shows that $\Delta(p)$ can be expressed as a polynomial in the coefficients of $p$.
  
  Note that $\Delta(p) = \prod_{i < j} (\lambda_i - \lambda_j)^2$ vanishes if and only if $f$ has some multiple root.
  Alltogether we have found that there exists a polynomial expression in the coefficients of $p$, namely $f(a_{n-1}, \dotsc, a_0)$, by which we can describe if $f$ has multiple roots (in an algebraic closure $\overline{k}$ of $k$).
  
  Consider for example the case $n = 2$.
  Then
  \begin{align*}
        D(X_1, X_2)
     =  (X_1 - X_2)^2
     =  (X_1 + X_2)^2 - 4 X_1 X_2
     =  e_1^2 - 4 e_2 \,,
  \end{align*}
  so that $f(Y_1, Y_2) = Y_1^2 - 4 Y_2$.
  Thus for $p(t) = t^2 + a t + b$ one has that
  \[
      \Delta(p)
    = f(a,b)
    = a^2 - 4 b \,.
  \]
  Note that by the usual solution formula for quadratic equations, the roots $\lambda_1$, $\lambda_2 \in \overline{k}$ of $p(t)$ are given by
  \[
      \frac{-a \pm \sqrt{a^2 - 4b}}{2}
    = \frac{-a \pm \Delta(p)}{2}
    = -\frac{1}{2} a \pm \frac{\sqrt{\Delta(p)}}{2} \,.
  \]
  We can see explicitly that the roots $\lambda_1$, $\lambda_2$ differ by $\sqrt{\Delta(p)}$, so that they are distinct if and only if $\Delta(p) \neq 0$.
\end{example}


\begin{fluff}
  Let $R$ be a ring.
  Then every sequence of elements $a_0, a_1, a_2, \dotsc \in R$ can be considered as coefficients of a (formal) power series
  \[
        \sum_{r=0}^\infty a_r t^r
    \in R\!\dblbrack{t} \,.
  \]
  This power series is the \emph{generating series} or \emph{generating function} of the sequence $(a_n)_{n \in \Natural}$.
  
  In the following we will consider the generating series $E(t)$, $H(t)$, $P(t)$ of families of symmetric polynomials $(e_r)_{r \in \Natural}$, $(h_r)_{r \in \Natural}$, $(p_r)_{r \in \Natural}$, and then use identities involving these generating series $E(t)$, $H(t)$, $P(t)$ to derive formulas for their coefficients, i.e.\ the symmetric polynomials $e_r$, $h_r$, $p_r$.
  
  For this we will start with the elementary symmetric polynomials $e_r$ and their generating series:
\end{fluff}


\begin{definition}
  For every $n \in \Natural$ the power series $E(t) \in k[X_1, \dotsc, X_n]\!\dblbrack{t}$ is the generating series of the sequence $(e_r)_{r \in \Natural}$, that is
  \[
              E(t)
    \defined  \sum_{r=0}^\infty e_r t^r \,.
  \]
\end{definition}


\begin{lemma}
  \label{lemma: explicit formula for E}
  One has the equality of power series
  \[
      E(t)
    = \prod_{i=1}^n (1 + X_i t) \,.
  \]
\end{lemma}


\begin{proof}
  The coefficient of $t^r$ on the right hand side of the equation is given by
  \[
    \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i \,,
  \]
  which is precisely $e_r$.
\end{proof}


% \begin{example}
%   Another example of formal power series are Hilbert series.
%   Given a graded $k$-algebra $A = \bigoplus_{d \geq 0} A_d$ ($A_d = 0$ for $d < 0$) with $\dim_k A_d < \infty$ for all $d$ the corresponding Hilbert series is defined as
%   \[
%               P_A(t)
%     \coloneqq \sum_{d \geq 0} \left( \dim_k A_d \right) t^d
%     \in       k\dblbrack{t}.
%   \]
%   If $\dim_k A < \infty$ we have $P_A(t) \in k[t] \subseteq k\dblbrack{t}$.
%   
%   If $A = \bigoplus_{d \geq 0} A_d$ and $B = \bigoplus_{d \geq 0} B_d$ are graded $k$-algebras then $A \otimes_k B$ is a $k$-algebra via
%   \[
%       (a_1 \otimes b_1) (a_2 \otimes b_2)
%     = (a_1 a_2) \otimes (b_1 b_2)
%   \]
%   and a graded $k$-algebra $A \otimes B = \bigoplus_{d \geq 0} (A \otimes B)_d$ by setting
%   \[
%       (A \otimes B)_d
%     = \bigoplus_{i=0}^d (A_i \otimes B_{d-i}) \,.
%   \]
%   We than have
%   \[
%       \dim_k (A \otimes B)_d
%     = \sum_{i=0}^d \dim_k (A_i \otimes B_{d-i})
%     = \sum_{i=0}^d (\dim_k A_i) (\dim_k B_{d-i})
%   \]
%   for all $d \geq 0$ and thus
%   \[
%       P_{A \otimes B}(t)
%     = P_A(t) P_B(t) \,.
%   \]
% \end{example}





\subsection{Complete Homogeneous Symmetric Polynomials}


\begin{definition}
  For all $r \in \Natural$ the \emph{$r$-th complete homogeneous symmetric polynomial} (in $n$-variables) is the sum of all monomials of $k[X_1, \dotsc, X_n]$ of degree $r$, that is
  \[
              h_r
    \defined  \sum_{\substack{\underline{\alpha} \in \Natural^n \\ |\underline{\alpha}| = r}}
              X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
  \]
\end{definition}


\begin{definition}
  For every $n \in \Natural$ the power series $H(t) \in k[X_1, \dotsc, X_n]\!\dblbrack{t}$ is the generating series of the sequence $(h_r)_{r \in \Natural}$, that is
  \[
              H(t)
    \defined  \sum_{r=0}^\infty h_r t^r \,.
  \]
\end{definition}


\begin{lemma}
  \label{lemma: explicit formula for H}
  One has the equality of power series
  \[
      H(t)
    = \prod_{i=1}^n \frac{1}{1 - X_i t}
  \]
\end{lemma}


\begin{proof}
  Note that the inverse of $1 - X_i t$ is for every $i$ given by the geometric series
  \[
              Q_i
    \defined  1 + X_i t + X_i^2 t^2 + X_i^3 t^3 + \dotsb
    \in       k[X_1, \dotsc, X_n]\!\dblbrack{t} \,,
  \]
  so that
  \[
      \prod_{i=1}^n \frac{1}{1 - X_i t}
    = \prod_{i=1}^n Q_i
    = Q_1 \dotsb Q_n \,.
  \]
  The coefficient of $t^r$ in $Q_1 \dotsm Q_n$ is given by $\sum_{|\underline{\alpha}| = r}  X_1^{\alpha_1} \dotsm X_n^{\alpha_n} = h_r$.
\end{proof}


\begin{fluff}
  By comparing the closed expressions of the power series $E(t)$ and $H(t)$ from from Lemma~\ref{lemma: explicit formula for E} and Lemma~\ref{lemma: explicit formula for H} we find that
  \[
      E(-t)H(t)
    = 1
    = H(-t)E(t) \,.
  \]
  By comparing the $s$-th coefficients of these power series we arrive at the following relation between the elementary symmetric polynomials $e_r$ and the complete homogeneous symmetric polynomials $h_r$:
\end{fluff}


\begin{corollary}
  \label{corollary: combinatorical formula for e and h}
  For all $s \geq 1$ we have that
  \begin{align*}
          h_s
        - e_1 h_{s-1}
        + e_2 h_{s-2}
        - \dotsb
        + (-1)^{s-1} e_{s-1} h_1
        + (-1)^s     e_s
    &=  0
  \intertext{as well as}
          e_s
        - h_1 e_{s-1}
        + h_2 e_{s-2}
        - \dotsb
        + (-1)^{s-1} h_{s-1} e_1
        + (-1)^s     h_s
    &=  0 \,.
  \end{align*}
\end{corollary}


\begin{fluff}
  From the Fundamental Theorem of Symmetric Polynomials we know that the complete homogeneous symmetric polynomials $h_i$ can be expressed uniquely as polynomials in the elementary symmetric polynomials $e_i$, so that there exist unique polynomials $P_1, \dotsc, P_n \in k[Y_1, \dotsc, Y_n]$ with
  \[
      h_i
    = P_i(e_1, \dotsc, e_n)
  \]
  for all $i = 1, \dotsc, n$.
  
  By rearranging the first formula of Corollary~\ref{corollary: combinatorical formula for e and h} to the equality
  \[
      h_s
    =   e_1 h_{s-1}
      - e_2 h_{s-2}
      + \dotsb
      - (-1)^{s-1} e_{s-1} h_1
      - (-1)^s e_s
  \]
  we can recursively express the $h_i$ in terms of the $e_i$, starting off with $e_1 = h_1$ for $s = 1$, and thus inductively determine the polynomials $P_1, \dotsc, P_n$.
  
  Note that the second formula of Corollary~\ref{corollary: combinatorical formula for e and h} results from the first by swapping $h_i$ and $e_i$.
  We can therefore swap the $h_i$ and $e_i$ in the previous paragraph to find that the $e_i$ can be expressed in terms of the $h_i$, and that this can be done in exactly the same way as the $h_i$ are expressed in terms of the $e_i$.
  In other words, we have that
  \[
      e_i
    = P_i(h_1, \dotsc, h_n)
  \]
  for all $i = 1, \dotsc, n$.
  
  This seems to suggest that the elementary symmetric polynomials $e_1, \dotsc, e_n$ and the homomogeneous symmetric polynomials $h_1, \dotsc, h_n$ are somehow dual to each other.
  To make this notion of duality more precise note that by the Fundamental Theorem of Symmetric Polynomials there exists a unique $k$-algebra homomorphism
  \[
            \Phi
    \colon  k[X_1, \dotsc, X_n]^{S_n}
    \to     k[X_1, \dotsc, X_n]^{S_n}∀
  \]
  with $\Phi(e_i) = h_i$ for every $i = 1, \dotsc, n$.
  (This follows from combining the universal property of the polynomial ring $k[Y_1, \dotsc, Y_n]$ with the $k$-algebra isomorphism $k[Y_1, \dotsc, Y_n] \to k[X_1, \dotsc, X_n]^{S_n}$, $Y_i \mapsto e_i$.)
  We then have that
  \[
      \Phi(h_i)
    = \Phi( P_i(e_1, \dotsc, e_n) )
    = P_i( \Phi(e_1), \dotsc, \Phi(e_n) )
    = P_i( h_1, \dotsc, h_n )
    = e_n \,.
  \]
  Hence the homomorphism $\Phi$ swaps $e_i$ with $h_i$ for every $i = 1, \dotsc, n$.
  It follows that $\Phi^2(e_i) = e_i$ for every $i = 1, \dotsc, n$, and therefore that $\Phi^2 = \id$ because $k[X_1, \dotsc, X_n]^{S_n}$ is generated by $e_1, \dotsc, e_n$.
  Thus we find the following:
\end{fluff}

\begin{corollary}
  There exists an unique $k$-algebra homomorphism
  \[
            \Phi
    \colon  k[X_1, \dotsc, X_n]^{S_n}
    \to     k[X_1, \dotsc, X_n]^{S_n}
  \]
  with $\Phi(e_i) = h_i$ for every $i = 1, \dotsc, n$, and $\Phi$ is an involutive automorphism.
\end{corollary}


\begin{corollary}
  The homogeneous symmetric polynomials $h_1, \dotsc, h_n$ generate the $k$-algebra $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent.
\end{corollary}


\begin{remark}
  As for the Fundamental Theorem of Symmetric Polynomials these results remain valid we replace $k$ with any non-zero commutative ring.
\end{remark}





\subsection{Power Symmetric Polynomials}


\begin{definition}
  For all $n, r \in \Natural$ the \emph{$r$-th power symmetric polynomial}, or \emph{$r$-th power sum} in $n$-variables is
  \[
              p_r
    \coloneqq X_1^r + \dotsb + X_n^r \,.
  \]
\end{definition}


\begin{definition}
  For all $n \in \Natural$ the power series $P(t) \in k[X_1, \dotsc, X_n]\!\dblbrack{t}$ is the generating series of the sequence $(p_r)_{r \geq 1}$, that is
  \[
            P(t)
  \defined  \sum_{r=0}^\infty p_{r+1} t^r \,.
  \]
  (Note the shift compared to $E$ and $H$.)
\end{definition}


\begin{lemma}
  \label{lemma: explicit formula for P}
  One has the equality of power series
  \[
      P(t)
    = \sum_{i=1}^n \frac{X_i}{1 - X_i t} \,.
  \]
  More generally, one has that for every $s \geq 0$ that
  \[
      \sum_{r=0}^\infty p_{r+s} t^r
    = \sum_{i=1}^n \frac{X_i^s}{1 - X_i t} \,.
  \]
\end{lemma}


\begin{proof}
  We have that
  \begin{align*}
        \sum_{i=1}^n \frac{X_i^s}{1-X_i t}
    &=  \sum_{i=1}^n X_i^s (1 + X_i t + X_i^2 t^2 + X_i^3 t^3 + \dotsb) \\
    &=  \sum_{i=1}^n (X_i^s + X_i^{s+1} t + X_i^{s+2} t^2 + X_i^{s+3} t^3 + \dotsb) \\
    &=  p_s + p_{s+1} t + p_{s+2} t^2 + p_{s+3} t^3 + \dotsb
    \qedhere
  \end{align*}
\end{proof}


\begin{fluff}
  \label{fluff: connection between E and P}
  From the explicit formulas for $E(t)$ and $P(t)$ from Lemma~\ref{lemma: explicit formula for E} and Lemma~\ref{lemma: explicit formula for P} it follows that
  \[
      E'(t)
    = \sum_{i=1}^n X_i \prod_{j \neq i} (1 + X_j t)
    = \sum_{i=1}^n \frac{X_i}{1 + X_i t} \prod_{j=1}^n (1 + X_j t)
    = P(-t)E(t) \,.
  \]
  The power series $E'(t)$ is given by
  \[
      E'(t)
    = \sum_{r=1}^\infty r e_r t^{r-1}
    = \sum_{r=0}^\infty (r+1) e_{r+1} t^r \,,
  \]
  so by comparing the $(r-1)$-th coefficient we arrive at the \emph{Newton’s identities}.
\end{fluff}


\begin{corollary}[Newton’s identities]
  \label{corollary: Newtons identities}
  For every $r \geq 1$ one has that
  \[
      r e_r
    =   p_1 e_{r-1}
      - p_2 e_{r-2}
      + \dotsb
      + (-1)^{r-2}  p_{r-1} e_1
      + (-1)^{r-1}  p_r \,,
  \]
  and equivalently
  \[
        p_r
      - e_1 p_{r-1}
      + \dotsb
      + (-1)^{r-1} e_{r-1} p_1
      + (-1)^r r e_r
    = 0 \,.
  \]
\end{corollary}


\begin{fluff}
  We can proceed similiar as in \ref{fluff: connection between E and P} for the genarating functions $H(t)$ and $P(t)$:
  It follows from the explicit formulas for $H(t)$ and $P(t)$ from Lemma~\ref{lemma: explicit formula for H} and Lemma~\ref{lemma: explicit formula for P} that
  \[
      H'(t)
    = \sum_{i=1}^n \frac{X_i}{(1-X_i t)^2} \prod_{j \neq i} \frac{1}{1 - X_j t} \\
    = \sum_{i=1}^n \frac{X_i}{1 - X_i t} \prod_{j=1}^n \frac{1}{1 - X_j t}
    = P(t) H(t) \,.
  \]
  Since the power series $H'(t)$ is given by
  \[
      H'(t)
    = \sum_{k \geq 1} k h_k t^{k-1}
  \]
  we get the following result by comparing the $r$-th coefficient:
\end{fluff}


\begin{corollary}
  \label{corollary: relation between h and p}
  For all $r \geq 1$ we have that
  \[
      r h_r
    =   p_1 h_{r-1}
      + p_2 h_{r-2}
      + \dotsb
      + p_{r-1} h_1
      + p_r.
  \]
\end{corollary}


\begin{fluff}
  We have seen that the symmetric polynomials $e_1, \dotsc, e_n$ and $h_1, \dotsc, h_n$ each generate $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent.
  It is now only natural to ask if this also holds true for the power sums $p_1, \dotsc, p_n$.
  The next theorem shows that this holds under additional assumptions.
\end{fluff}


\begin{theorem}
  Let $k$ be a field with either $\kchar k = 0$ or $\kchar k > n$.
  Then $p_1, \dotsc, p_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent.
\end{theorem}
\begin{proof}
  Since $2, \dotsc, n$ are invertible in $k$ one can use the Newton identities (Corollary~\ref{corollary: Newtons identities}) to recursively express the elementary symmetric polynomials $e_1, \dotsc, e_n$ in terms of the power sums $p_1, \dotsc, p_n$, starting off with $e_1 = p_1$.
  It follows that $p_1, \dotsc, p_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra.
  
  To show that $p_1, \dotsc, p_n$ are algebraically independent we need to show that the monomials in $p_1, \dotsc, p_n$, i.e.\ the polynomials
  \[
      p^{\,\underline{\alpha}}
    = p_1^{\alpha_1} \dotsm p_n^{\alpha_n}
    \quad\text{with}\quad
        \underline{\alpha}
    =   (\alpha_1, \dotsc, \alpha_n)
    \in \Natural^n
  \]
  are linearly independent.
  For this it sufficies to show for every $N \geq 1$ that the monomials in $p_1, \dotsc, p_n$ of degree $\leq N$ form a $k$-basis of the $k$-linear space of symmetric polynomials of degree $\leq N$, which we will denote by $V_N$.
  
  We also denote the number of (not necessarily distinct) monomials in $p_1, \dotsc, p_n$ of degree $\leq N$ by $P_N$, i.e.\ $P_N$ is the number of multi-indices $\underline{\alpha} \in \Natural^n$ with $\deg p^{\,\underline{\alpha}} \leq N$.
  
  Note that for every $\underline{\alpha} \in \Natural^n$ we have that
  \begin{align*}
        \deg p^{\,\underline{\alpha}}
    &=  \deg
        p_1^{\alpha_1}
        \dotsm 
        p_n^{\alpha_n} \\
    &=    \alpha_1 \deg p_1
        + \dotsb 
        + \alpha_n \deg p_n \\
    &=    \alpha_1 \cdot 1
        + \alpha_2 \cdot 2
        + \dotsb
        + \alpha_n \cdot n  \\
    &=    \alpha_1 \deg e_1
        + \dotsb 
        + \alpha_n \deg e_n \\
    &=  \deg
        e_1^{\alpha_1}
        \dotsm 
        e_n^{\alpha_n}
     =  e^{\,\underline{\alpha}} \,,
  \end{align*}
  so that $P_N$ is also the number of monomials in $e_1, \dotsc, e_n$ of degree $\leq N$.
  Note that these monomials in the $e_i$ are pairwise distinct because the $e_i$ are algebraically independent.
  Hence $P_N$ is also the number of monomials in $e_1, \dotsc, e_n$ of degree $\leq N$.
  With the Fundamental Theorem of symmetric functions it follows that $\dim V_N = P_N$.
  
  Because $k[X_1, \dotsc, X_n]^{S_n}$ is generated as a $k$-algebra by the homogeneous elements $p_1, \dotsc, p_n$ it we find that $V_N$ is spanned as a $k$-linear subspace of $K[X_1, \dotsc, X_n]^{S_n}$ by the monomials in $p{(n)}_1, \dotsc, p_n$ of degree $\leq N$, of which they are $\leq P_N$ many distinct ones.
  It therefore follows from $\dim V_N = P_N$ that $V_N$ is a $k$-basis von $V_N$.
\end{proof}


\begin{remark}
  Note that the above theorem cannot hold for $k = \Integer$:
  To see this, note that in $\Rational[X_1, X_2]^{S_2}$ we have that
  \[
      e_2
    = \frac{1}{2}  p_1^2 - \frac{1}{2} p_2 \,.
  \]
  If $\Integer[X_1, X_2]^{S_2}$ would be generated by $p_1, p_2$ as a $\Integer$-algebra (i.e.\ ring) then there would exists some polynomial $F \in \Integer[Y_1, Y_2]$ with $e_2 = F( p_1, p_2)$.
  But this would then contradict the algebraic independence of $p_1, p_2$ in $\Rational[X_1, X_2]^{S_2}$, since $F(X_1, X_2) \neq \frac{1}{2} X_1^2 - \frac{1}{2} X_2$.
\end{remark}


% Using the same argumentation we find that for a symmetric polynomial $f \in \Rational[X_1, \dotsc, X_n]^{S_n}$ with integer coefficients and $F,G \in \Rational[X_1, \dotsc, X_n]^{S_n}$ with
% \[
%     f
%   = F(e_1, \dotsc, e_n)
%   = G(h_1, \dotsc, h_n)
% \]
% both $F$ and $G$ must have integer coefficients.
% 


\begin{fluff}
  We have seen that the elementary symmetric polynomials $e_1, \dotsc, e_n$ and the complete homogeneous symmetric polynomials $h_1, \dotsc, h_n$ are dual to each other in the sense that there exists a involutive algebra automorphism $\Phi$ of $k[X_1, \dotsc, X_n]^{S_n}$ which swaps $e_i$ and $h_i$ for every $i = 1, \dotsc, n$.
  We can determine the action of $\Phi$ on the power sums $p_1, \dotsc, p_n$.
  
  Applying $\Phi$ to Newton’s identities (Corollary~\ref{corollary: Newtons identities}) and comparing the result with Corollary~\ref{corollary: relation between h and p} seems to suggest that
  \[
      \Phi(p_r)
    = (-1)^{r-1} p_r
  \]
  for all $r = 1, \dotsc, n$.
  We can show this by induction on $r$:
  
  For $r = 1$ we have that
  \[
      \Phi(p_1)
    = \Phi(e_1)
    = h_1
    = p_1 \,.
    = (-1)^{r-1} p_1
  \]
  For $r > 1$ we apply $\Phi$ to the Newton identity
  \[
      r e_r
    =   p_1 e_{r-1}
      - p_2 e_{r-2}
      + \dotsb
      + (-1)^{r-2}  p_{r-1} e_1
      + (-1)^{r-1}  p_r \,,
  \]
  which by induction results in the identity
  \[
      r h_r
    =   p_1 h_{r-1}
      + p_2 h_{r-2}
      + \dotsb
      + p_{r-1} h_1
      + (-1)^{r-1} \Phi(p_r) \,.
  \]
  By comparing this to Corollary~\ref{corollary: relation between h and p} it follows that $\Phi( p_r ) = (-1)^{r-1} p_r$.
\end{fluff}





\subsection{Partitions}

\begin{definition}
  Let $n \in \Natural$.
  A partition of $n$ is a tupel $\lambda = (\lambda_1, \dotsc, \lambda_s)$ of natural numbers $\lambda_i \in \Natural$ with $n = \sum_{i=1}^s \lambda_i$ and
  \[
          \lambda_1
    \geq  \lambda_2
    \geq  \dotsb
    \geq  \lambda_s
    >  0 \,.
  \]
  Then $|\lambda| \defined \sum_{i=1}^n \lambda_i$, the $\lambda_i$ are the \emph{parts of $\lambda$} and $\ell(\lambda) \defined s$ is the \emph{length of $\lambda$}.
\end{definition}


\begin{example}
  The partitions of $4$ are $(4)$, $(3,1)$, $(2,2)$, $(2,1,1)$, $(1,1,1,1)$.
\end{example}


\begin{fluff}
  Partitions are often displayed in terms of \emph{Young diagrams}.
  The Young diagram corresponding to a partition $\lambda$ is an array of boxes, left adjusted, such that the $i$-th row consists of $\lambda_i$ boxes.
\end{fluff}


\begin{example}
  The Young diagrams of the partitions of $4$ are as follows:
  \[
    \renewcommand{\arraystretch}{2}
    \begin{matrix}
        \ydiagram{4}
      & \quad
      & \ydiagram{3,1}
      & \quad
      & \ydiagram{2,2}
      & \quad
      & \ydiagram{2,1,1}
      & \quad
      & \ydiagram{1,1,1,1}
      \\
        (4)
      & {}
      & (3,1)
      & {}
      & (2,2)
      & {}
      & (2,1,1)
      & {}
      & (1,1,1,1)
    \end{matrix}
  \]
\end{example}


\begin{fluff}
  Note that transposing the Young diagram of a partition $\lambda$ of $n$ gives again the Young-diagram of a partition $\lambda'$ of $n$.
  If $\lambda = (\lambda_1, \dotsc, \lambda_s)$ then $\lambda' = (\lambda'_1, \dotsc, \lambda'_t)$ for $t = \lambda_1$ with $\lambda'_i = |\{j \mid \lambda_j \geq i\}|$.
\end{fluff}

\begin{definition}
  The partition $\lambda'$ is the \emph{transposed} of the partition $\lambda$.
\end{definition}


\begin{definition}
  An \emph{infinite partition} is a decreasing sequence $\lambda_1, \lambda_2, \dotsc \in \Natural$ with $\lambda_i = 0$ for all but finitely many $i$.
  For a partition $(\lambda_1, \dotsc, \lambda_s)$ the \emph{infinite partition associated to $\lambda$} is given by
  \[
      \hat{\lambda}
    = (\lambda_1, \dotsc, \lambda_s, 0, 0, \dotsc) \,.
  \]
\end{definition}


\begin{example}
  The partitions $\lambda = (4,2,2)$ and $\lambda' = (3,3,1,1)$ are transposed to each other.
  \[
    \renewcommand{\arraystretch}{2}
    \begin{matrix}
        \ydiagram{4,2,2}
      & \quad
      & \ydiagram{3,3,1,1}
      \\
        (4,2,2)
      & {}
      & (3,3,1,1)
    \end{matrix}
  \]
\end{example}


\begin{definition}
  For $n \in \Natural$ we write
  \[
              \Par(n)
    \coloneqq \{\text{partitions of $n$}\}
  \]
  and we set
  \[
              \Par
    \coloneqq \bigcup_{n \in \Natural} \Par(n) \,.
  \]
\end{definition}


\begin{definition}
  If $\lambda, \mu \in P(n)$ then $\lambda \geq \mu$ if $\sum_{i=1}^r \hat{\lambda}_i \geq \sum_{i=1}^r \hat{\mu}_i$ for all $r$.
\end{definition}


\begin{example}
  The following are partitions of $6$:
  \[
      \ydiagram{6}
    > \ydiagram{4,2}
    > \ydiagram{3,3}
    > \ydiagram{3,2,1}
    > \ydiagram{1,1,1,1,1,1}
  \]
  The partitions
  \[
    \ydiagram{2,2}
    \quad\text{and}\quad
    \ydiagram{1,1}
  \]
  are not comparable because the first is a partition of $4$ while the second is a partititon of $2$.
  The partitions
  \[
    \ydiagram{4,2,1,1,1}
    \quad \text{and} \quad
    \ydiagram{3,3,2,1}
  \]
  are also not comparable because $4 > 3$ but $4+2+1 = 7 < 8 = 3+3+2$.
\end{example}


\begin{lemma}
  For every $n \in \Natural$, $\leq$ defines a partial ordering on $\Par(n)$.
\end{lemma}
\begin{proof}
  The relation $\leq$ is reflexive.
  
  Let $\lambda, \mu \in \Par(n)$ with $\lambda \geq \mu$ and $\lambda \leq \mu$.
  Because $\lambda \geq \mu$ we have $\hat{\lambda}_1 \geq \hat{\mu}_1$ and because $\lambda \leq \mu$ we have $\hat{\lambda}_1 \leq \hat{\mu}_1$.
  Thus we have $\hat{\lambda}_1 = \hat{\mu}_1$.
  In the same way we find that $\hat{\lambda}_1 + \hat{\lambda}_2 = \hat{\mu}_1 + \hat{\mu}_2$, and with $\hat{\lambda}_1 = \hat{\mu}_1$ we get that $\hat{\lambda}_2 = \hat{\mu}_2$.
  It follows inductively that $\hat{\lambda}_i = \hat{\mu}_i$ for every $i$.
  We then have that $\hat{\lambda} = \hat{\mu}$, and therefore that $\lambda = \mu$.
  This shows that $\leq$ is antisymmetric.
  
  Let $\lambda, \mu, \nu \in \Par(n)$ with $\lambda \geq \mu$ and $\mu \geq \nu$.
  For all $r \geq 1$ we then have
  \[
          \sum_{i=1}^r \hat{\lambda}_i
    \geq  \sum_{i=1}^r \hat{\mu}_i
    \quad\text{and}\quad
          \sum_{i=1}^r \hat{\mu}_i
    \geq  \sum_{i=1}^r \hat{\nu}_i
  \]
  and therefore
  \[
          \sum_{i=1}^r \hat{\lambda}_i
    \geq  \sum_{i=1}^r \hat{\nu}_i \,,
  \]
  so that $\lambda \geq \nu$.
  This shows that $\leq$ is transitive.
\end{proof}

\begin{definition}
  For any two infinite partitions $\lambda, \mu$ their \emph{sum} $\lambda + \mu$ is given by
  \[
      (\lambda + \mu)_i
    = \lambda_i + \mu_i
  \]
  for all $i$.
  For any two partitions $\lambda, \mu \in \Par$ their \emph{sum} $\lambda + \mu$ is the partition with $\widehat{\lambda + \mu} = \hat{\lambda} + \hat{\mu}$, i.e.\ the partitition with $\ell(\lambda + \mu) = \max( \ell(\lambda), \ell(\mu) )$ and
  \[
      (\lambda+\mu)_i
    = \begin{cases}
        \lambda_i + \mu_i & \text{if $i \leq \ell(\lambda), \ell(\mu)$}       \,, \\
        \lambda_i         & \text{if $i \leq \ell(\lambda)$, $i > \ell(\mu)$} \,, \\
        \mu_i             & \text{if $i \leq \ell(\mu)$, $i > \ell(\lambda)$} \,.
      \end{cases}
  \]
\end{definition}


\begin{example}
  For $\lambda = (4,3,2,2)$ and $\mu = (3,2,2)$ we have $\lambda + \mu = (7,5,4,2)$.
  The addition of two partitions can also be visualized “putting together” their Young diagrams row-wise:
  \[
                \ydiagram[*(gray)]{4,3,2,2}
          \;+\; \ydiagram[*(light-gray)]{3,2,2,0}
    \;=\; \ydiagram[*(light-gray)]{4+3,3+2,2+2} * [*(gray)]{7,5,4,2}
  \]
\end{example}



\subsection{Monomial Symmetric Polynomials}

\begin{definition}
  For a partition $\lambda = (\lambda_1, \dotsc, \lambda_r)$ the corresponding \emph{monomial symmetric polynomial} is given by
  \[
              m_\lambda
    \coloneqq   X_1^{\lambda_1} \dotsm X_r^{\lambda_r}
              + \text{ all distinct permutations of this monomial} \,.
  \]
\end{definition}


\begin{remark}
  The monomial symmetric polynomial $m_\lambda$ can also be defined in a more formal way:
  
  Instead of adding up all distinct permutations of the monomial $X_1^{\lambda_1} \dotsm X_r^{\lambda_r}$ we can also take all distinct permutations of the tupel $\lambda$ and add up the corresponding monomials.
  To formalize this we let $S_r$ act on $\Natural^r$ by permuting the entries, i.e.\
  \[
      \pi.(a_1, \dotsc, a_r)
    = ( a_{\pi^{-1}(1)}, \dotsc, a_{\pi^{-1}(r)} )
  \]
  for all $\pi \in S_r$, $(a_1, \dotsc, a_r) \in \Natural^r$.
  The set of all distinct permutations of $\lambda$ is precisely the orbit of $\lambda$ under this action.
  For the stabilizer subgroup $U \subseteq S_n$ there exists an isomorphism of $G$-sets
  \[
            S_r / U
    \to     S_r \lambda,
    \quad   \class{\pi}
    \mapsto \pi.\lambda
  \]
  where $S_r.\lambda$ denotes the orbit of $\lambda$.
  Thus we can write
  \[
      m_\lambda
    = \sum_{\class{\pi} \in S_r/U} X_1^{\lambda_{\pi^{-1}(1)}} \dotsm X_r^{\lambda_{\pi^{-1}(r)}}
    = \sum_{\class{\pi} \in S_r/U} X_{\pi(1)}^{\lambda_1} \dotsm X_{\pi(r)}^{\lambda_r} \,.
  \]
%   Also notice that
%   \[
%           U
%     \cong S_{\nu_0} \times \dotsb \times S_{\nu_m}
%   \]
%   where
%   \[
%               \nu_n
%     \coloneqq \left|
%                 \left\{
%                   1 \leq i \leq r
%                 \mid
%                     \lambda_i
%                   = n
%                 \right\}
%               \right|
%   \]
%   and $m \coloneqq \max_{i=1,\dotsc,r} \lambda_i$.
\end{remark}


\begin{fluff}
  Note that every multi-index $\underline{\alpha} \in \Natural^n$ can be reordered uniquely to a partition $\lambda$ of length $\ell(\lambda) = n$.
  Then $m_\lambda$ is the “smallest” symmetric polynomial containing the monomial $X^{\,\underline{\alpha}} = X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$.
  The following result should therefore not be too surprising:
\end{fluff}


\begin{proposition}
  \label{proposition: m_lambda give a basis}
  The monomial symmetric polynomials
  \[
      m_\lambda
    \quad\text{with}\quad
      \lambda \in \Par, \,
      \ell(\lambda) = n
  \]
  form a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$.
\end{proposition}


\begin{notation}
  Every multi-index $\underline{\alpha} \in \Natural^n$ can be permuted to a unique partition $\lambda \in \Par$ of length $n$.
  We will refer to $\lambda$ as the \emph{partition associated to $\underline{\alpha}$}.
  
  We will sometimes want to consider a partition $\lambda = (\lambda_1, \dotsc, \lambda_n)$ as a multi-index.
  When doing so, we will write $\underline{\lambda}$ instead of just $\lambda$.
  (So technically speaking both $\underline{\lambda}$ and $\lambda$ are the same thing.)
  Note that $\lambda$ is then the partition associated to $\underline{\lambda}$.
\end{notation}


\begin{proof}
  Note that for every monomial $X_1^{\alpha_1} \dotsm X_n^{\alpha}$ the polynomial
  \[
      X_1^{\alpha_1} \dotsm X_n^{\alpha}
    + \text{all distinct permutations of this monomial}
  \]
  is precisely the monomial symmetric polynomial $m_\lambda$  of the partition $\lambda$ associated to $\underline{\alpha} = (\alpha_1, \dotsc, \alpha_n)$.
  
  Let $f \in k[X_1, \dotsc, X_n]^{S_n}$ be a symmetric polynomial.
  Then for every monomial $X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ occuring in $f$, all of its permutations must also occur in $f$, all of them with the same coefficient $c$.
  By the above observation all of these monomials can be grouped together to the symmetric polynomial $c m_{\lambda}$, where $\lambda$ is the partition associated to $\underline{\alpha} = (\alpha_1, \dotsc, \alpha_n)$.
  
  Since $f - c m_\lambda$ is again symmetric one can then inductively continue this process of grouping together permutated monomials to ultimately express $f$ as a linear combination of monomial symmetric polynomials.
  (Note that no new monomials are introduced during this process, so that it eventually terminates.)
  
  The beginning observation also shows that a partition $\lambda$ is uniquely determined by any of the monomials $X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ occuring in $m_\lambda$.
  It follows that for any two distinct partitions $\lambda \neq \mu$ their monomial symmetric polynomials $m_\lambda$, $m_\mu$ have no common monomonials.
  As the collection of all monomials $X^{\,\underline{\alpha}}$, $\underline{\alpha} \in \Natural^n$ is linearly independent, it follows that the collection of monomial symmetric polynomials $m_\lambda$, $\lambda \in \Par$ is also linearly independent.
\end{proof}



% \begin{proof}
%   The polynomial ring $k[X_1, \dotsc, X_n]$ has the usual monomial basis
%   \[
%               B
%     \defined  \{
%                   X^{\,\underline{\alpha}}
%                 = X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
%               \mid
%                     \underline{\alpha}
%                 =   (\alpha_1, \dotsc, \alpha_n)
%                 \in \Natural^n
%               \} \,.
%   \]
%   The action of $S_n$ on $k[X_1, \dotsc, X_n]$ restrict to an action of $S_n$ on the basis $B$.
%   It follows that a polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if its coefficients are constant on the $S_n$-orbits of $B$.
%   Hence a basis of $k[X_1, \dotsc, X_n]^{S_n}$ is given by the polynomials $f_{\mc{O}}$ whose coefficents are $1$ on an orbit $\mc{O} \in B/S_n$ and $0$ otherwise.
%   
%   The action of $S_n$ on $B$ is given by
%   \[
%       \sigma.X^{\,\underline{\alpha}}
%     = \sigma.X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
%     = X_{\sigma(1)}^{\alpha_1} \dotsm X_{\sigma(n)}^{\alpha_n}
%     = X_1^{\alpha_{\sigma^{-1}(1)}} \dotsm X_n^{\alpha_{\sigma^{-1}(n)}}
%     = X^{\sigma.\underline{\alpha}} \,,
%   \]
%   and thus corresponds to the permutation action of $S_n$ on $\Natural^n$.
%   The bijection $\Natural^n \to B$, $\underline{\alpha} \mapsto X^{\,\underline{\alpha}}$ therefore induces a bijection
%   \[
%           \Natural^n / S_n
%     \to   B / S_n \,,
%     \quad [\underline{\alpha}]
%     \to   [X^{\,\underline{\alpha}}]
%   \]
%   With this we can reparametrize the basis
%   \begin{align*}
%     f_{\mc{O}}
%     \quad&\text{with}\quad
%     \mc{O} \in B/S_n
%   \shortintertext{as}
%               g_{[\underline{\alpha}]}
%     \defined  f_{[X^{\,\underline{\alpha}}]}
%     \quad&\text{with}\quad
%     [\underline{\alpha}] \in \Natural^n/S_n \,.
%   \intertext{
%   The $S_n$-orbits of $\Natural^n$ have the partitions of length $n$ as a representative system.
%   Thus a basis of $k[X_1, \dotsc, X_n]^{S_n}$ is given by the polynomials
%   }
%     g_{[\lambda]}
%     \quad&\text{with}\quad
%     \lambda \in \Par \,,
%     \ell(\lambda) = n
%   \intertext{
%   For every partition $\lambda$ of length $\ell(\lambda) = n$ the polynomial $g_{[\lambda]}$ has coefficient $1$ for the monomial $X_1^{\lambda_1} \dotsm X_n^{\lambda_n}$ and its permutations, and $0$ otherwise.
%   Thus $g_{[\lambda]}$ is precisely the monomial symmetric polynomial $m_\lambda$.
%   We thus arrive at the basis
%   }
%     m_\lambda
%     \quad&\text{with}\quad
%     \lambda \in \Par \,,
%     \ell(\lambda) = n
%   \qedhere
%   \end{align*}
% \end{proof}


% Previous Proof:
% \begin{proof}
%   Is is clear that
%   \[
%             \vspan_k \{
%                         m_\lambda
%                       \mid
%                         \lambda \in \Par,
%                         l(\lambda) = n,
%                         |\lambda| = d
%                       \}
%   \subseteq k[X_1, \dotsc, X_n]^{S_n}_d \,.
%   \]
%   On the other side let $f \in k[X_1, \dotsc, X_n]^{S_n}_d$. By induction on the number of monomials of which $f$ consists we show that
%   \[
%         f
%     \in \vspan_k  \{
%                     m_\lambda
%                   \mid
%                     \lambda \in \Par,
%                     l(\lambda) = n,
%                     |\lambda| = d
%                   \} \,.
%   \]
%   For $f = 0$ this is clear.
%   Suppose that $f \neq 0$ and that the statement is true for every polynomial in $k[X_1, \dotsc, X_n]^{S_n}_d$ which consists of fewer monomials than $f$.
%   Because $\{X^\alpha \mid \alpha \in \Natural^n, |\alpha| = d \}$ is a $k$-basis of $k[X_1, \dotsc, X_n]_d$ we can write
%   \[
%       f
%     = \sum_{\substack{\alpha \in \Natural^n \\ |\alpha| = d}} c_\alpha X^\alpha \,.
%   \]
%   with unique $c_\alpha \in k$ such that $c_\alpha \neq 0$ for only finitely many $\alpha$.
%   Because $f$ is symmetric we find that
%   \[
%       c_\alpha
%     = c_{\pi.\alpha}
%     \text{ for all }
%     \alpha \in \Natural^n,
%     \pi \in S_n
%   \]
%   (where the action of $S_n$ on $\Natural^n$ is defined as above).
%   Let $X^\beta$ be a monomial of $f$.
%   Because $f \in k[X_1, \dotsc, X_n]^{S_n}_d$ we have $X^\beta \in k[X_1, \dotsc, X_n]_d$ and thus $c_\beta m_\beta \in k[X_1, \dotsc, X_n]^{S_n}_d$.
%   Because $c_\beta \neq 0$ and $c_\beta = c_{\pi.\beta}$ for every $\pi \in S_n$ we find that $f - c_{\beta} m_\beta$ consists of fewer monomials than $f$.
%   Because $f-c_{\beta} m_\beta$ is symmetric we find by induction hypothesis that
%   \[
%         f - c_{\beta} m_\beta
%     \in \vspan_k  \{
%                     m_\lambda
%                   \mid
%                     \lambda \in \Par,
%                     l(\lambda) = n,
%                     |\lambda| = d
%                   \} \,.
%   \]
%   The statement for $f$ follows directly.
%   
%   To show that
%   \[
%     \{
%       m_\lambda
%     \mid
%       \lambda \in \Par,
%       l(\lambda) = n
%     \}
%   \]
%   is linear independent we notice that for $\lambda, \mu \in \Par$ with $\lambda \neq \mu$ the polynomials $m_\lambda$ and $m_\mu$ have no monomials in common. Because
%   \[
%     \{
%       X^\alpha
%     \mid
%       \alpha \in \Natural^n
%     \}
%   \]
%   is linear independent it then follows that
%   \[
%     \{
%       m_\lambda
%     \mid
%       \lambda \in \Par,
%       l(\lambda) = n
%     \}
%   \]
%   is linear independent.
% \end{proof}


\begin{fluff}
  Note that the proof of Proposition~\ref{proposition: m_lambda give a basis} gives an easy way to express a symmetric polynomial $f \in k[X_1, \dotsc, X_n]^{S_n}$ in terms of the monomial symmetric polynomials:
  Simply group together all monomial which are permutated to each other.
  
  We will use this to describe the product $m_\lambda m_\mu$ for two partitions $\lambda, \mu \in \Par$ of length $n$ as a linear combination of the basis $m_\nu$, $\nu \in \Par$:
  
  The monomials $X^{\,\underline{\alpha}} = X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ occuring in $m_\lambda$ are those for the multi-indices $\underline{\alpha} = (\alpha_1, \dotsc, \alpha_n)$ with associated partition $\lambda$, and the monomials $X^{\,\underline{\beta}}$ occuring in $m_\mu$ are those for the multi-indices $\underline{\beta}$ with associated partition $\mu$.
  
  In follows that all monomials $X^{\,\underline{\gamma}}$ occuring in $m_\lambda m_\mu$ are of the form $\underline{\gamma} = \underline{\alpha} + \underline{\beta}$ for some $\underline{\alpha}$, $\underline{\beta}$ as above.
  Given such a $\underline{\gamma}$ and corresponding $\underline{\alpha}, \underline{\beta}$, let $\nu \in \Par$ be the partition associated to $\underline{\gamma}$.
  
  \begin{claim}
    The partition $\nu$ satisfies $\nu \leq \lambda + \mu$.
  \end{claim}
  
  \begin{proof}
    Let $\lambda = (\lambda_1, \dotsc, \lambda_n)$,  $\mu = (\mu_1, \dotsc, \mu_n)$ and $\nu = (\nu_1, \dotsc, \nu_n)$.
    By definition of $\underline{\alpha}$ and $\underline{\beta}$ there exist permutations $\sigma, \tau \in S_n$ with
    \[
        \underline{\alpha}
      = ( \lambda_{\sigma(1)}, \dotsc, \lambda_{\sigma(n)} )
      \quad\text{and}\quad
        \underline{\beta}
      = ( \mu_{\tau(1)}, \dotsc, \mu_{\tau(n)} )
    \]
    and by definition of $\nu$ there exists some permutation $\omega \in S_n$ with
    \[
        \nu
      = (\nu_1, \dotsc, \nu_n)
      = (\alpha_{\omega(1)} + \beta_{\omega(1)},
         \dotsc,
         \alpha_{\omega(n)} + \beta_{\omega(n)})
    \]
    For every $r = 1, \dotsc, n$ we therefore have that
    \begin{align*}
          \sum_{i=1}^r \nu_r
       =  \sum_{i=1}^r ( \alpha_{\omega(i)} + \beta_{\omega(i)} )
       =    \sum_{i=1}^r \alpha_{\omega(i)}
          + \sum_{i=1}^r \beta_{\omega(i)}
      &=    \sum_{i=1}^r \lambda_{\sigma(\omega(i))}
          + \sum_{i=1}^r \mu_{\tau(\omega(i))}  \\
      &=    \sum_{i=1}^r \lambda_{\sigma'(i)}
          + \sum_{i=1}^r \mu_{\tau'(i)}
    \end{align*}
    for the permutations $\sigma' \defined \sigma \omega$ and $\tau' \defined \tau \omega$.
    Because the entries of the partitions $\lambda$ and $\mu$ are decreasing we have that $\sum_{i=1}^r \lambda_{\sigma'(i)} \leq \sum_{i=1}^r \lambda_i$ and $\sum_{i=0}^r \mu_{\tau'(i)} \leq \sum_{i=1}^r \mu_i$, so that
    \[
            \sum_{i=1}^r \nu_i
      \leq    \sum_{i=1}^r \lambda_i
            + \sum_{i=1}^r \mu_i
      =     \sum_{i=1}^r (\lambda_i + \mu_i)
      =     \sum_{i=1}^r (\lambda + \mu)_i \,.
    \]
    As this holds for every $r = 1, \dotsc, n$, this shows that $\nu \leq \mu + \lambda$.
  \end{proof}
  
  We have shown that for every monomial $X^{\,\underline{\gamma}}$ in $m_\lambda m_\mu$ the partition $\nu$ associated to $\underline{\gamma}$ satisfies $\nu \leq \lambda + \mu$.
  Thus we find that $m_\lambda m_\mu$ is already a linear combination of those $m_\nu$ for which $\nu \leq \mu + \lambda$, i.e. that
  \[
      m_\lambda m_\mu
    = \sum_{\nu \leq \lambda + \mu} a_\nu m_\nu \,.
  \]
  for suitable coecffients $a_\nu \in k$.
  
  We can also determine the coefficients $a_{\lambda + \mu}$:
  As in the \hyperref[label: first proof of fundamental theorem]{first proof of the Fundamental Theorem of Symmetric Polynomials} we introduce an ordering on the set of monomials in $k[X_1, \dotsc, X_n]$ by $X_1^{\alpha_1} \dotsm X_n^{\alpha_n} > X_1^{\beta_1} \dotsm X_n^{\beta_n}$ if they exists some $i$ with $\alpha_1 = \beta_1, \dotsc, \alpha_n = \beta_n$ and $\alpha_i > \beta_i$.
  For every polynomial non-zero $f \in k[X_1, \dotsc, X_n]$ we then denote by $\init f$ the inital term of $f$, that is the biggest monomial occuring in $f$ together with its coefficient.
  Then
  \begin{itemize}
    \item
      $\init(f \cdot g) = (\init f) \cdot (\init g)$ for all $f, g \in k[X_1, \dotsc, X_n]$ with $f, g \neq 0$, and
    \item
      $\init m_\nu = X^{\,\underline{\nu}}$ for every partition $\nu \in \Par$ of length $n$.
  \end{itemize}
  With this we find that
  \[
      \init(m_\lambda m_\mu)
    = (\init m_\lambda) (\init m_\mu)
    = X^{\,\underline{\lambda}} X^{\,\underline{\mu}}
    = X^{\,\underline{\lambda} + \underline{\mu}}
    = X^{\,\underline{\lambda + \mu}} \,.
  \]
  Hence the monomial $X^{\,\underline{\lambda + \mu}}$ occurs in $m_\lambda m_\mu$ with coefficient $1$.
  The partition associated to $\underline{\lambda + \mu}$ is $\lambda + \mu$, so the coefficient of $m_{\lambda + \mu}$ in $m_\lambda m_\mu$ is $1$, i.e.\ $a_{\lambda + \mu} = 1$.
  
  Alltogether we have proven the following result:
\end{fluff}


\begin{lemma}
  Let $\lambda, \mu \in \Par$ be partitions of length $\ell(\lambda), \ell(\mu) = n$.
  Then
  \[
        m_{\lambda} m_{\mu}
    =   m_{\lambda + \mu}
      + \sum_{\nu < \lambda + \mu} a^\nu_{\lambda,\mu} m_\nu
  \]
  for suitable $a^\nu_{\lambda,\mu} \in k$.
\end{lemma}


\begin{remark}
  Note that the above results about monomial symmetric polynomials also hold when the field $k$ is replaced by an arbitrary non-zero commutative ring.
\end{remark}

% TODO: Reread and rework this section some time in the future.





% TODO: Finding out how Schur polynomials work.
%
% \subsection{Schur Polynomials}
% 
% \begin{example}
%   Let $k$ be a field with $\kchar k \neq 2$.
%   Let $\lambda = (\lambda_1, \dotsc, \lambda_n) \in \Par$ be a partition.
%   The \emph{Schur polynomial corresponding to $\lambda$} is the symmetric polynomial $s_\lambda \in k[X_1, \dotsc, X_n]^{S_n}$ of homogenous degree $|\lambda|$ defined as
%   \[
%               s_\lambda
%     \coloneqq \frac
%               {
%                 \sum_{\sigma \in S_n} \sgn(\sigma)          X_{\sigma(1)}^{\lambda_1}
%                                                             X_{\sigma(2)}^{\lambda_2 + 1}
%                                                     \dotsm  X_{\sigma(n)}^{\lambda_n + n-1}
%                }{
%                 \prod_{1 \leq i < j \leq n} (X_i - X_j)
%                }
%   \]
%   (In the lecture the same statements were made for the ring of integers $\Integer$ and arbitrary fields, but the following argumentation does not work in these cases.)
%   
%   To show that $s_\lambda$ is well-defined we first notice the numerator
%   \[
%               N
%     \coloneqq \sum_{\sigma \in S_n} \sgn(\sigma)          X_{\sigma(1)}^{\lambda_1}
%                                                           X_{\sigma(2)}^{\lambda_2 + 1}
%                                                   \dotsm  X_{\sigma(n)}^{\lambda_n + n-1}
%   \]
%   and the denumerator
%   \[
%               D
%     \coloneqq \prod_{1 \leq i < j \leq n} (X_i - X_j)
%   \]
%   are alternating polynomials, i.e.\ $\sigma.N = \sgn(\sigma) N$ and $\sigma.D = \sgn(\sigma) D$ for every $\sigma \in S_n$, because
%   \begin{gather*}
%     N = \det
%     \begin{pmatrix}
%       X_1^{\lambda_1}       & X_2^{\lambda_1}       & \cdots & X_n^{\lambda_1}       \\
%       X_1^{\lambda_2 + 1}   & X_2^{\lambda_2 + 1}   & \cdots & X_n^{\lambda_2 + 1}   \\
%       \vdots                & \vdots                & \ddots & \vdots                \\
%       X_1^{\lambda_n + n-1} & X_2^{\lambda_n + n-1} & \cdots & X_n^{\lambda_n + n-1}
%     \end{pmatrix}
%   \shortintertext{and}
%     D = \det
%     \begin{pmatrix}
%       1      & X_1    & X_1^2  & \cdots & X_1^{n-1} \\
%       1      & X_2    & X_2^2  & \cdots & X_2^{n-1} \\
%       \vdots & \vdots & \vdots & \ddots & \vdots    \\
%       1      & X_n    & X_n^2  & \cdots & X_n^{n-1}
%     \end{pmatrix}.
%   \end{gather*}
%   That $D$ divides $N$ follows from the following claim:
%   \begin{claim}
%     Let $f \in k[X_1, \dotsc, X_n]$ be an alternating polynomial and
%     \[
%                 V
%       \coloneqq \prod_{1 \leq i < j \leq n} (X_i - X_j)
%       \in       k[X_1, \dotsc, X_n] \,.
%     \]
%     Then $V$ divides $f$.
%   \end{claim}
%   \begin{proof}
%     Since the polynomials $X_i - X_j$ with $1 \leq i < j \leq n$ are pairwise non-equivalent primes it sufficies to show that $X_i-X_j$ divides $f$ for all $1 \leq i < j \leq n$.
%     Because $f$ is alternating it is enough to show that $X_1 - X_2$ divides $f$.
%     
%     For $R \coloneqq k[X_3, \dotsc, X_n]$, $u = X_1 + X_2$ and $x = X_1 - X_2$ we have
%     \[
%         k[X_1, \dotsc, X_n]
%       = R[X_1, X_2]
%       = R[u,v] \,,
%     \]
%     so we can write $f = \sum_{i \in \Natural} f_i v^i$ with $f_i \in R[u]$ for every $i \in \Natural$.
%     Because $f$ is alternating we have
%     \begin{align*}
%            \sum_{i \in \Natural} f_i v^i
%       &=   f(X_1, X_2, \dotsc, X_n)
%        =  -f(X_2, X_1, \dotsc, X_n) \\
%       &=  -\sum_{i \in \Natural} (-1)^i f_i v^i
%        =   \sum_{i \in \Natural} (-1)^{i+1} f_i v^i \,.
%     \end{align*}
%     So $f_i = 0$ if $i$ is even.
%     Therefore $v$ divides $f$ .
%   \end{proof}
%   Since $D$ and $N$ are both alternating it is also clear that $s_\lambda = N/D$ is symmetric.
%   To see that $s_\lambda$ is homogeneous of degree $|\lambda|$ notice that $N$ is homogeneous of degree
%   \[
%       \lambda_1 + (\lambda_2 + 1) + \dotsb + (\lambda_n + n-1)
%     = |\lambda| + \binom{n}{2}
%   \]
%   and that $D$ is homogeneous of degree $\binom{n}{2}$.
%   \begin{claim}
%     Let $f, g \in k[X_1, \dotsc, X_n]$ be polynomials such that $f$ is homogenous of degree $d_1$ and $g$ homogeneous of degree $d_2$.
%     If $g$ divides $f$ then $f/g$ is homogenous of degree $d_1 - d_2$.
%   \end{claim}
%   \begin{proof}
%     We can write $f/g = \sum_{d \in \Natural} h_d$ where $h_d \in k[X_1, \dotsc, X_n]$ is homogenous of degree $d$.
%     Then $f = (f/g)g = \sum_{d \in \Natural} h_d g$ where $h_d g$ is homogeneous of degree $d + d_2$.
%     Because $f$ is homogenous of degree $d_1$ we find that $h_d = 0$ for $d \neq d_1 - d_2$.
%     Thus $f/g = h_d$ is homogeneous of degree $d_1 - d_2$.
%   \end{proof}
% \end{example}





\subsection{Other Symmetric Polynomials Associated to Partitions}


\begin{definition}
  For a partition $\lambda = (\lambda_1, \dotsc, \lambda_r)$ the corresponding \emph{elementary symmetric polynomial} is given by
  \[
              e_\lambda
    \defined  e_{\lambda_1} \dotsm e_{\lambda_r} \,,
  \]
  the corresponding \emph{complete symmetric polynomial} is given by
  \[
              h_\lambda
    \defined  h_{\lambda_1} \dotsm h_{\lambda_r} \,,
  \]
  the corresponding \emph{power symmetric polynomial} is given by
  \[
              p_\lambda
    \defined  p_{\lambda_1} \dotsm p_{\lambda_r} \,.
  \]
\end{definition}


\begin{fluff}
  We know from the Fundamental Theorem of Symmetric Functions that the elementary symmetric polynomials $e_1, \dotsc, e_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent. This is equivalent to saying that the monomials in $e_1, \dotsc, e_n$, i.e.\
  \[
      e^{\,\underline{\alpha}}
    = e_1^{\alpha_1} \dotsm e_n^{\alpha_n}
    \quad\text{with}\quad
      \underline{\alpha}
    = (\alpha_1, \dotsc, \alpha_n)
  \]
  form a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$.
  Note that $e^{\,\underline{\alpha}}$ coincides with $e_\lambda$ for the partition
  \[
    \lambda
  = (
      \underbrace{n, \dotsc, n}_{\alpha_n},
      \underbrace{n-1, \dotsc, n-1}_{\alpha_{n-1}},
      \dotsc,
      \underbrace{1, \dotsc, 1}_{\alpha_1}
    ) \,.
  \]
  Also note that the above formula gives a bijection
  \[
    \{ \text{multi-indices $\underline{\alpha} \in \Natural^n$} \}
    \longleftrightarrow
    \{ \text{partitions $\lambda \in \Par$ with $\lambda_1 \leq n$} \} \,.
  \]
  With this we arrive at the following reformulation of the Fundamental Theorem of Symmetric Polynomials:
\end{fluff}


\begin{corollary}
  The symmetric polynomials
  \[
      e_\lambda
    \quad\text{with}\quad
      \lambda \in \Par \,,
      \lambda_1 \leq n
  \]
  form a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$.
\end{corollary}


\begin{remark}
  We can show the same statements for the polynomials $h_\lambda$ since $h_1, \dotsc, h_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent.
  If $k$ is a field with $\kchar k = 0$ or $\kchar k > n$ we can also show the same for the polynomials $p_\lambda$.
\end{remark}


% TODO: Adding the basis of schur polynomials.
