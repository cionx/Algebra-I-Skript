\section{The Fundamental Theorem of Symmetric Functions}


\begin{conventions}
  We fix a number of variables $n \in \Natural$.
\end{conventions}


\begin{fluff}
  The symmetric group $S_n$ acts by $k$-algebra automorphisms on the polynomial ring $k[X_1, \dotsc, X_n]$ by
  \[
      \sigma.f(X_1, \dotsc, X_n)
    = f(X_{\sigma(1)}, \dotsc, X_{\sigma(n)}) \,.
  \]
  In this section we will be concerned by the $k$-algebra of invariants $k[X_1, \dotsc, X_n]^{S_n}$.
\end{fluff}


\begin{definition}
  Let $k$ be a field.
  The polynomial $f \in k[X_1, \dotsc, X_n]^{S_n}$ are \emph{symmetric}, and $k[X_1, \dotsc, X_n]^{S_n}$ is the \emph{ring of symmetric polynomials \textup(in $n$ variables\textup) \textup(over $k$\textup)}.
\end{definition}


\begin{example}
  \label{example: symmetric polynomials}
  In $k[X_1, X_2, X_3]$ we have the symmetric polynomials
  \begin{align*}
                p_2
    &\coloneqq  X_1^2 + X_2^2 + X_3^2 \,,
    \\
                h_2
    &\coloneqq  X_1^2 + X_1 X_2 + X_1 X_3 + X_2^2 + X_2 X_3 + X_3^2 \,,
    \\
                e_2
    &\coloneqq  X_1 X_2 + X_1 X_3 + X_2 X_3 \,,
    \\
                m_{(4,4,2)}
    &\coloneqq  X_1^4 X_2^2 X_3^2 + X_1^2 X_2^4 X_3^2 + X_1^2 X_2^2 X_3^4 \,.
  \end{align*}
  In the next subsections we will generalize these examples.
\end{example}


\begin{lemma}
  \label{lemma: symmetric iff all homogeneous parts are symmetric}
  With respect to the usual grading $k[X_1, \dotsc, X_n] = \bigoplus_{d \in \Natural} k[X_1, \dotsc, X_n]_d$ a polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if all of its homogeneous parts are symmetric.
\end{lemma}


\begin{proof}
  The decomposition $k[X_1, \dotsc, X_n] = \bigoplus_{d \geq 0} k[X_1, \dotsc, X_n]_d$ is a decomposition into subrepresentations of $S_n$, thus the claim follows from Lemma~\ref{lemma: direct sum and invariants commute}.
\end{proof}

\begin{fluff}
  In the following subsections we will consider families of symmetric polynomials which generalize the polynomials given in Example~\ref{example: symmetric polynomials}.
  
  We will start off with the so called \emph{elemantary symmetric polynomials}.
  We prove the famous \emph{fundamental theorem of symmetric functions}, which roughly states that every every symmetric polynomial can be uniquely expressed in terms of the elementary symmetric polynomials.
  
  We will then use the elementary symmetric polynomials to study other kinds of symmetric polynomials:
  Namely the \emph{complete homogeneous symmetric polynomials}, \emph{power sums} \emph{monomial symmetric polynomials}.
  Along the way we will also introduce \emph{partitions} as a natural way for labeling these different kinds of symmetric polynomials.
\end{fluff}


\begin{definition}
  For all $r \in \Natural$ the \emph{$r$-th elementary symmetric polynomial} (in $n$ variables) is
  \[
              e_r
    \defined  \sum_{1 \leq i_1 \leq \dotsb \leq i_r \leq n} X_{i_1} \dotsm X_{i_r}
    =         \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \, \prod_{i \in I} X_i \,.
  \]
  In particular $e_0 = 1$ and $e_r = 0$ for every $r > n$.
\end{definition}


\begin{fluff}
  For all $a_1, \dotsc, a_n \in k$ we have that
  \begin{align*}
     &\, (t-a_1) \dotsm (t-a_n) \\
    =&\, t^n  - (a_1 + \dotsb + a_n) t^{n-1}
              + \left( \sum_{1 \leq i_1 < i_2 \leq n} a_{i_1} a_{i_2} \right) t^{n-2}
              + \dotsb
              + (-1)^n a_1 \dotsm a_n \\ 
    =&\,    t^n
          - e_1(a_1, \dotsc, a_n) t^{n-1}
          + e_2(a_1, \dotsc, a_n) t^{n-2}
          - \dotsb
          + (-1)^n e_n(a_1, \dotsc, a_n)
  \end{align*}
  in $k[t]$.
  We will formalize this observation in the following lemma:
\end{fluff}


\begin{lemma}
  \label{lemma: natural occurence of elementary symmetric polynomials}
  For all $n \in \Natural$ we have in $k[X_1, \dotsc, X_n][t]$ the equality
  \begin{align*}
        \prod_{i=1}^n (t-X_i)
    &=    e_0 t^n
        - e_1 t^{n-1}
        + e_2 t^{n-2}
        - \dotsb
        + (-1)^n e_n  \\
    &=    t^n
        - e_1 t^{n-1}
        + e_2 t^{n-2}
        - \dotsb
        + (-1)^n e_n
  \end{align*}
\end{lemma}
\begin{proof}
  On both sides the $r$-th coefficient is given by $\prod_{I \subseteq \{1, \dotsc, n\}, |I| = r} \prod_{i \in I} X_i $.
\end{proof}


\begin{theorem}[Fundamental theorem of symmetric functions]
  \label{theorem: fundamental theorem of symmetric functions}
  The symmetric polynomials $e_1, \dotsc, e_n$ generate the $k$-algebra of symmetric functions $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent, i.e.\ the unique $k$-algebra homomorphism
  \[
            k[Y_1, \dotsc, Y_n]
    \to     k[X_1, \dotsc, X_n]^{S_n} \,,
    \quad   Y_r
    \mapsto e_r
  \]
  is an isomorphism of $k$-algebras.
\end{theorem}


\begin{fluff}
  We will give two proofs of the fundamental theorem.
  The one given in the lecture is the second one.
\end{fluff}


\begin{proof}[First proof of the fundamental theorem]
  \label{label: first proof of fundamental theorem}
  We introduce an ordering on the set monomials in $k[X_1, \dotsc, X_n]$.
  For this we first order the monomials by their power of $X_1$ in decreasing order.
  The monomials with the same power of $X_1$ are then ordered in decreasing order by their power of $X_2$.
  We then continue this process trough the variables $X_3, \dotsc, X_n$.
  
  For any two monomials $X^\alpha = X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ and $X^\beta = X_1^{\beta_1} \dotsm X_n^{\beta_n}$ we thus have $X^\alpha > X^\beta$ if and only if there exists some $1 \leq i \leq n$ such that $\alpha_j = \beta_j$ for all $j < i$ and $\alpha_i > \beta_i$.
  This gives a well-ordering on the set of monomials in $k[X_1, \dotsc, X_n]$.
  
  For any polynomial $p \in k[X_1, \dotsc X_n]$ with $p \neq 0$ we define the initial term $\init p$ to be the highest monomial occuring in $p$, including its coefficient.
  Then the following properties hold:
  \begin{itemize}
    \item
      If $p \neq 0$ is symmetric then for $\init p = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ one has $\alpha_1 \geq \alpha_2 \geq \dotsb \geq \alpha_n$.
    \item
      For $p, q \in k[X_1, \dotsc, X_n]$ with $p, q \neq 0$ one has $\init (p \cdot q) = \init p \cdot \init q$.
    \item
      For all $1 \leq k \leq n$ one has $\init e_k = X_1 \dotsm X_k$ .
  \end{itemize}
  With this we are now well-equipped to prove the theorem:
  
  We first show that $e_1, \dotsc, e_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra.
  For this let $f \in k[X_1, \dotsc, X_n]^{S_n}$ with $f \neq 0$.
  By Lemma~\ref{lemma: symmetric iff all homogeneous parts are symmetric} we may assume that $f$ is homogeneous of degree $d \geq 0$.
  For
  \[
      \init f
    = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
  \]
  we then have $d = \alpha_1 + \dotsb + \alpha_n$.
  
  We consider the polynomial
  \[
      p
    =         c
              e_1^{\alpha_1 - \alpha_2}
      \dotsm  e_{n-1}^{\alpha_{n-1} - \alpha_n}
              e_n^{\alpha_n} \,.
  \]
  Then $\init p = \init f$ by the above properties of $\init$.
  Because $e_k$ is homogenous of degree $k$ it follows that $p$ is homogeneous of degree
  \begin{align*}
     &\,  (\alpha_1-\alpha_2) + 2(\alpha_2-\alpha_3) + \dotsb + (n-1)(\alpha_{n-1}-\alpha_n) + n\alpha_n \\
    =&\,  \alpha_1 + \dotsb + \alpha_n
    =     d \,.
  \end{align*}
  Combining these observations we find that $f-p$ is a homogeneous symmetric polynomial of degree $d$ with either $f-p = 0$ or at least $\init (f-p) < \init f$.
  
  Because there are only finitely many monomials of homogeneous degree $d$ we can repeat the above process to arrive at the zero polynomial in finitely many steps.
  Hence $f$ can be expressed as a poylynomial in $e_1, \dotsc, e_n$.
  
  To show that $e_1, \dotsc, e_n$ are algebraically independent we need to show that the monomials in $e_1, \dotsc, e_n$, i.e.\ the polynomials
  \[
      e^{\,\underline{\alpha}}
    = e_1^{\alpha_1} \dotsm e_n^{\alpha_n}
    \quad\text{for}\quad
        \underline{\alpha}
    =   (\alpha_1, \dotsc, \alpha_n)
    \in \Natural^n
  \]
  are linearly independent.
  For this we notice that for all $\underline{\alpha} \neq \underline{\beta}$ we have that
  \begin{align*}
            \init e^{\,\underline{\alpha}}
       =&\, \init e_1^{\alpha_1} \dotsm e_n^{\alpha_n}   \\
       =&\, X_1^{\alpha_1 + \dotsb + \alpha_n} X_2^{\alpha_2 + \dotsb + \alpha_n} \dotsm X_n^{\alpha_n} \\
    \neq&\, X_1^{\beta_1 + \dotsb + \beta_n}   X_2^{\beta_2  + \dotsb + \beta_n}  \dotsm X_n^{\beta_n}  \\
       =&\, \init e_1^{\beta_1} \dotsm e_n^{\beta_n}
       =    \init e^{\,\underline{\beta}}
  \end{align*}
  so that the polynomials $e^{\,\underline{\alpha}}$ for $\underline{\alpha} \in \Natural^n$ are pairwise different.
  
  Now suppose that
  \[
      0
    = \lambda_1 e^{\,\underline{\alpha}_1} + \dotsb + \lambda_s e^{\,\underline{\alpha}_s}
  \]
  with $s \geq 1$, $\underline{\alpha}_i \neq \underline{\alpha}_j$ for $i \neq j$ and $\lambda_i \neq 0$ for all $1 \leq i \leq s$.
  We can assume w.l.o.g.\ that 
  \[
      \init e^{\,\underline{\alpha}_1}
    > \init e^{\,\underline{\alpha}_2}
    > \dotsb
    > \init e^{\,\underline{\alpha}_s}
  \]
  since all of these terms are pairwise different.
  It follows that the initial term $\init e^{\,\underline{\alpha}_1}$ occures only in $e^{\,\underline{\alpha}_1}$ and in no ther of the $e^{\,\underline{\alpha}_i}$.
  From $\lambda_1 = 0$, in contradiction to $\lambda_1 \neq 0$.
\end{proof}


\begin{proof}[Second proof of the fundamental theorem]
  We denote the $r$-th elementary symmetric polynomial in $n$ variables by $e^{(n)}_r$.
  Note that
  \begin{equation}
    \label{equation: recursive formel for elementary symmetric polynomials}
    \begin{aligned}
          e^{(n)}_r
      &=  \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i \\
      &=    \sum_{\substack{I \subseteq \{1, \dotsc, n-1\} \\ |I| = r}} \prod_{i \in I} X_i
          + \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r-1}} \left( \prod_{i \in I} X_i \right) X_n  \\
      &=  e^{(n-1)}_r + e^{(n)}_{r-1} X_n \,.
    \end{aligned}
  \end{equation}
  \begin{claim}
    A polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if $f$ can be written as a polyonmial in $e^{(n)}_1, \dotsc, e^{(n)}_n$, i.e.\ we have that
    \[
        k\left[ e^{(n)}_1, \dotsc, e^{(n)}_n \right]
      = k[X_1, \dotsc, X_n]^{S_n} \,.
    \]
  \end{claim}
  \begin{proof}[Proof of claim]
    Because the polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n \in k[X_1, \dotsc, X_n]^{S_n}$ are symmetric it follows that $k[ e^{(n)}_1, \dotsc, e^{(n)}_n ] \subseteq k[X_1, \dotsc, X_n]^{S_n}$.
    We show the other inclusion by induction over $n$.
    For $n = 1$ we have that $k[ e^{(1)}_1 ] = k[X_1] = k[X_1]^{S_1}$.
    
    Let $n \geq 2$ and suppose that the claim holds for $n-1$.
    We show the claim for $n$ by induction over the (total) degree $d \defined \deg f$.
    If $f$ is constant than the claim holds.
    So let $d \geq 1$ and suppose the claim holds for degrees $0, \dotsc, d-1$.
    By the Lemma~\ref{lemma: symmetric iff all homogeneous parts are symmetric} we may assume that $f$ is homogenous.
    (The homogenous parts of lower degree are by induction hypothesis expressable as polynomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$.)
    
    Let
    \[
              \Phi
      \colon  k[X_1, \dotsc, X_n]
      \to     k[X_1, \dotsc, X_{n-1}] \,,
      \quad   f(X_1, \dotsc, X_n)
      \mapsto f(X_1, \dotsc, X_{n-1}, 0)
    \]
    be the evaluation at $X_n = 0$.
    By \eqref{equation: recursive formel for elementary symmetric polynomials} we have that
    \begin{align*}
          \Phi\left( e^{(n)}_r \right)
      &=  e^{(n-1)}_r
      \quad \text{for all $1 \leq r < n$} \,,
      \\
          \Phi\left( e^{(n)}_n \right)
      &=  0 \,.
    \end{align*}
    Note that $\Phi(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric:
    Because $f$ is symmetric we have that
    \[
        f
      = f(X_1, \dotsc, X_n)
      = f(X_{\sigma(1)}, \dotsc, X_{\sigma(n)})
    \]
    for every $\sigma \in S_n$, und thus we have that
    \[
        f(X_1, \dotsc, X_{n-1}, 0)
      = f(X_{\tau(1)}, \dotsc, X_{\tau(n-1)}, 0)
    \]
    for every $\tau \in S_{n-1}$.
    Because $\Phi(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric we can use the induction hypothesis (from the induction on $n$) to write
    \[
        \Phi(f)
      = P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right)
    \]
    for some polynomial $P \in k[Y_1, \dotsc, Y_{n-1}]$.
    Consider the symmetric polynomial
    \[
                g
      \coloneqq P\left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1} \right)
      \in       k[X_1, \dotsc, X_n] \,.
    \]
    
    Because $\Phi$ is a homomorphism of $k$-algebras we find that
    \begin{align*}
         \Phi(g)
      &= \Phi\left( P\left(e^{(n)}_1, \dotsc, e^{(n)}_{n-1}\right) \right) \\
      &= P\left( \Phi\left(e^{(n)}_1\right), \dotsc, \left(e^{(n)}_{n-1}\right) \right) \\
      &= P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right)
       = \Phi(f)
    \end{align*}
    and therefore that $\Phi(f - g) = 0$.
    Note that $\ker \Phi = (X_n)$ by the commutativity of the following diagram:
    \[
      \begin{tikzcd}
          k[X_1, \dotsc, X_n]
          \arrow{rr}[above]{\Phi}
          \arrow{rd}[below left]{p \mapsto \class{p}}
        & {}
        & k[X_1, \dotsc, X_{n-1}]
        \\
          {}
        & k[X_1, \dotsc, X_n]/(X_n)
          \arrow{ru}[above,rotate=20]{\sim}[below right]{\class{p} \mapsto p(X_1, \dotsc, X_n, 0)}
        & {}
      \end{tikzcd}
    \]
    It therefore follows from $\Phi(f - g) = 0$ that $X_n \mid (f-g)$.
    Because $f-g$ is symmetric (because both $f$ and $g$ are symmetric) it follows that $X_i \mid (f-g)$ for all $1 \leq i \leq n$, and therefore that $X_1 \dotsm X_n \mid (f-g)$.
    We can thus consider the polynomial
    \[
                h
      \defined  \frac{f-g}{X_1 \dotsm X_n}
      =         \frac{f-g}{e^{(n)}_n} \,.
    \]
    (This quotient is well-defined because $k[X_1, \dotsc, X_n]$ is an integral domain.)
    
    \begin{claim}
      The polynomial $h$ is symmetric.
    \end{claim}
    \begin{proof}
      From $h e^{(n)}_n = f-g$ it follows for every $\sigma \in S_n$ that
      \[
          (\sigma.h) e^{(n)}_n
        = (\sigma.h) (\sigma.e^{(n)}_n)
        = \sigma.(h e^{(n)}_n)
        = \sigma(f-g)
        = \sigma.f - \sigma.g
        = f - g \,.
      \]
      Hence it follows that $\sigma.h = (f-g)/e^{(n)}_n = h$.
    \end{proof}
    
    \begin{claim}
      We have that $\deg g \leq \deg f$ and therefore that $\deg h < \deg f$.
    \end{claim}
    \begin{proof}
      ?
%     TODO: Adding a proof.
    \end{proof}
    
    By induction hypothesis (of the induction on $d$) we can write $h$ as a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$.
    Because $g$ is also a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$ it further follows that $f = e^{(n)}_n h + g$ is a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$.
  \end{proof}
  
  We now prove that the polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ are algebraically independent by induction over $n$.
  It holds for $n = 1$ because $e^{(1)}_1 = X_1$.
  
  Now suppose $n \geq 2$ and that the elements $e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1}$ are algebraically independent.
  Suppose that
  \[
      F\left(e^{(n)}_1, \dotsc, e^{(n)}_n\right)
    = 0
  \]
  for some polynomial $F \in k[Y_1, \dotsc, Y_n]$ with $F \neq 0$ of minimal possible degree.
  Then
  \begin{align*}
        0
    &=  \Phi \left( F \left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1}, e^{(n)}_n \right) \right) \\
    &=  F \left(
            \Phi\left( e^{(n)}_1 \right),
            \dotsc,
            \Phi\left( e^{(n)}_{n-1} \right),
            \Phi\left( e^{(n)}_n \right)
          \right) \\
    &=  F \left( e^{n-1}_1, \dotsc, e^{(n-1)}_{n-1}, e^{(n-1)}_n \right)
     =  F \left( e^{n-1}_1, \dotsc, e^{(n-1)}_{n-1}, 0 \right).
  \end{align*}
  From the induction hypothesis it follows that $F(Y_1, \dotsc, Y_{n-1}, 0) = 0$, and therefore that $Y_n \mid F$.
  So there exists some polynomial $\hat{F} \in k[Y_1, \dotsc, Y_n]$ with $F = Y_n \hat{F}$.
  Note that $\hat{F} \neq 0$ since $F \neq 0$, and that $\deg \hat{F}  <\deg F$.
  We then have
  \[
      0
    = F\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = e^{(n)}_n \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right).
  \]
  Because $k[X_1, \dotsc, X_n]$ is an integral domain it now further follows from $e^{(n)}_n \neq 0$ that
  \[
      \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = 0 \,.
  \]
  This contradits the minimality of $F$.
\end{proof}


\begin{remark}
  Each of the proofs gives us an algorithm how to express a symmetric polynomial in terms of $e_1, \dotsc, e_n$.
\end{remark}


\begin{remark}
  The first proof shows that the fundemental theorem does not only hold if $k$ is a field, but for every nonzero commutative ring $R$.
  It does in particular hold for $k = \Integer$.
  
  The second proof can be slightly modified to also work for $R$:
  Instead of using that $R[X_1, \dotsc, X_n]$ is an integral domain (which holds if and only if $R$ itself is an intgeral domain), it sufficies to realize that the polynomial $X_1 \dotsm X_n = e_n$ is a non-zero divisor.
\end{remark}


\begin{example}
  \label{example: multiple roots via symmetric polynomials}
  Let $p(t) = t^n + a_{n-1} t^{n-1} + \dotsb + a_1 t + a_0 \in k[t]$ be a polynomial, and let $\lambda_1, \dotsc, \lambda_n$ be the roots of $p(t)$ is an algebraic closure $\closure{k}$ of $k$.
  Then
  \[
      a_i
    = (-1)^{n-i} e_{n-i}(\lambda_1, \dotsc, \lambda_n)
  \]
  for all $i$ by Lemma~\ref{lemma: natural occurence of elementary symmetric polynomials}.
  It follows from the fundamental theorem that every symmetric polynomial in the roots $\lambda_1, \dotsc, \lambda_n$ can already be expressed as a polynomial in the coefficients $a_0, \dotsc, a_{n-1}$:
  
  Consider for example the \emph{discriminant}
  \[
      \Delta(p)
    = \prod_{i < j} (\lambda_i - \lambda_j)^2
    = (-1)^{n(n-1)/2} \prod_{i \neq j} (\lambda_i - \lambda_j)
  \]
  The polynomial $D(X_1, \dotsc, X_n) \defined \prod_{i < j} (X_i - X_j)^2$ is symmetric, which is why there exists a (unique) polynomial $f \in k[Y_1, \dotsc, Y_n]$ with $D = f(e_1, \dotsc, e_n)$.
  Then
  \[
      \Delta(p)
    = D(\lambda_1, \dotsc, \lambda_n)
    = f(e_1(\lambda_1, \dotsc, \lambda_n), \dotsc, e_n(\lambda_1, \dotsc, \lambda_n))
    = f(a_{n-1}, \dotsc, a_0) \,.
  \]
  This shows that $\Delta(p)$ can be expressed as a polynomial in the coefficients of $p$.
  
  Note that $\Delta(p) = \prod_{i < j} (\lambda_i - \lambda_j)^2$ vanishes if and only if $f$ a multiple root in $\closure{k}$.
  Altogether we have thus found that there exists a polynomial expression in the coefficients of $p$, namely $f(a_{n-1}, \dotsc, a_0)$, by which we can describe if $f$ has multiple roots in an algebraic closure $\closure{k}$ of $k$.
  
  Consider for example the case $n = 2$.
  Then
  \begin{align*}
        D(X_1, X_2)
     =  (X_1 - X_2)^2
     =  (X_1 + X_2)^2 - 4 X_1 X_2
     =  e_1^2 - 4 e_2 \,,
  \end{align*}
  and therefore $f(Y_1, Y_2) = Y_1^2 - 4 Y_2$.
  For $p(t) = t^2 + a t + b$ we therefore have that
  \[
      \Delta(p)
    = f(a,b)
    = a^2 - 4 b \,.
  \]
  Our above discussion shows that $\Delta(p) = 0$ if and only if $p$ has a multiple root in $\closure{k}$.
  Note that by the usual solution formula for quadratic equations, the roots $\lambda_1$, $\lambda_2 \in \overline{k}$ of $p(t)$ are given by
  \[  
      \lambda_{1,2}
    = \frac{-a \pm \sqrt{a^2 - 4b}}{2}
    = \frac{-a \pm \Delta(p)}{2}
    = -\frac{1}{2} a \pm \frac{\sqrt{\Delta(p)}}{2} \,.
  \]
  We can see explicitly that the roots $\lambda_1 - \lambda_2 = \pm \sqrt{\Delta(p)}$, and that $\lambda_1, \lambda_2$ are therefore distinct if and only if $\lambda_1 \neq \lambda_2$.
  The previous discussion shows that $\Delta(p)$ can be generalized to polynomials of arbitrary degree.
\end{example}


\begin{fluff}
  Let $R$ be a ring.
  Then every sequence of elements $a_0, a_1, a_2, \dotsc \in R$ can be considered as coefficients of a (formal) power series
  \[
        \sum_{r=0}^\infty a_r t^r
    \in R\!\dblbrack{t} \,.
  \]
  This power series is the \emph{generating series} or \emph{generating function} of the sequence $(a_n)_{n \in \Natural}$.
  
  In the following we will consider the generating series $E(t)$, $H(t)$, $P(t)$ of families of symmetric polynomials $(e_r)_{r \in \Natural}$, $(h_r)_{r \in \Natural}$, $(p_r)_{r \in \Natural}$, and then use identities involving these generating series $E(t)$, $H(t)$, $P(t)$ to derive formulas for their coefficients, i.e.\ the symmetric polynomials $e_r$, $h_r$, $p_r$.
  
  For this we will start with the elementary symmetric polynomials $e_r$ and their generating series:
\end{fluff}


\begin{definition}
  For every $n \in \Natural$ the power series $E(t) \in k[X_1, \dotsc, X_n]\!\dblbrack{t}$ is the generating series of the sequence $(e_r)_{r \in \Natural}$, that is
  \[
              E(t)
    \defined  \sum_{r=0}^\infty e_r t^r \,.
  \]
\end{definition}


\begin{lemma}
  \label{lemma: explicit formula for E}
  One has the equality of power series
  \[
      E(t)
    = \prod_{i=1}^n (1 + X_i t) \,.
  \]
\end{lemma}


\begin{proof}
  The coefficient of $t^r$ on the right hand side of the equation is given by
  \[
    \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i \,,
  \]
  which is precisely $e_r$.
\end{proof}


% \begin{example}
%   Another example of formal power series are Hilbert series.
%   Given a graded $k$-algebra $A = \bigoplus_{d \geq 0} A_d$ ($A_d = 0$ for $d < 0$) with $\dim_k A_d < \infty$ for all $d$ the corresponding Hilbert series is defined as
%   \[
%               P_A(t)
%     \coloneqq \sum_{d \geq 0} \left( \dim_k A_d \right) t^d
%     \in       k\dblbrack{t}.
%   \]
%   If $\dim_k A < \infty$ we have $P_A(t) \in k[t] \subseteq k\dblbrack{t}$.
%   
%   If $A = \bigoplus_{d \geq 0} A_d$ and $B = \bigoplus_{d \geq 0} B_d$ are graded $k$-algebras then $A \otimes_k B$ is a $k$-algebra via
%   \[
%       (a_1 \otimes b_1) (a_2 \otimes b_2)
%     = (a_1 a_2) \otimes (b_1 b_2)
%   \]
%   and a graded $k$-algebra $A \otimes B = \bigoplus_{d \geq 0} (A \otimes B)_d$ by setting
%   \[
%       (A \otimes B)_d
%     = \bigoplus_{i=0}^d (A_i \otimes B_{d-i}) \,.
%   \]
%   We than have
%   \[
%       \dim_k (A \otimes B)_d
%     = \sum_{i=0}^d \dim_k (A_i \otimes B_{d-i})
%     = \sum_{i=0}^d (\dim_k A_i) (\dim_k B_{d-i})
%   \]
%   for all $d \geq 0$ and thus
%   \[
%       P_{A \otimes B}(t)
%     = P_A(t) P_B(t) \,.
%   \]
% \end{example}




