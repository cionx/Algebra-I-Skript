\chapter{Invariant Polynomial Functions}





\section{Graded and Filtered \texorpdfstring{$k$}{k}-Algebras}


\begin{definition}
  A $k$-Algebra $A$ is called \emph{graded} (or more precisely \emph{$\Z$-graded}) if there is a decomposition $A = \bigoplus_{d \in \Z} A_d$ into vector subspaces $A_d$ such that $A_i A_j \subseteq A_{i+j}$ for all $i,j \in \Z$.
  
  A ring $R$ is called \emph{graded} if there is a decomposition $R = \bigoplus_{d \in \Z} R_d$ into $\Z$-modules such that $R_i R_j \subseteq R_{i+j}$ for all $i,j \in \Z$.
  
  We call $A_d$, resp.\ $R_d$, the \emph{homogeneous part of degree $d$}.
\end{definition}

\begin{rem}
  If $A$ is a $k$-Algebra with $1$, then $A$ is a graded $k$-Algebra if and only if $A$ is a graded ring such that $k1 \subseteq A_0$.
\end{rem}
\begin{proof}
  ($\Rightarrow$)
  Because $A$ is a graded $k$-Algebra there exists a decomposition $A = \bigoplus_{d \in \Z} A_d$ into vector subspaces such that $A_i A_j \subseteq A_{i+j}$ for all $i,j \in \Z$.
  It is clear that this is also a decomposition into $\Z$-modules.
  
  We first notice that $1 \in A_0$:
  Because $1 \in A = \bigoplus_{d \in \Z} A_d$ there exist unique $e_i \in A_i$ with $1 = \sum_{i \in \Z} e_i$ with $e_i = 0$ for all but finitely many $i$.
  For every $j \in \Z$ and $a \in A_j$ we have
  \[
      a
    = a \cdot 1
    = a \cdot \sum_{i \in \Z} e_i
    = \sum_{i \in \Z} \underbrace{a e_i}_{\in A_{i+j}}.
  \]
  Because the sum $A = \bigoplus_{d \in \Z} A_d$ is direct we find that $a = a e_0$.
  Because $A = \bigoplus_{d \in \Z} A_d$ we find that $a e_0 = a$ for all $a \in A$.
  This shows that $1 = e_0$ and therefore $1 \in A_0$.
  Because $A_0$ is a vector subspace it follows that $\lambda 1 \in A_0$ for all $\lambda \in k$.
  
  ($\Leftarrow$)
  Suppose $A = \bigoplus_{d \in \Z} A_d$ such that $A_d$ is a $\Z$-module for all $d \in \Z$ and $A_i A_j \subseteq A_{i+j}$ for all $i,j \in \Z$.
  We only need to check that $A_d$ is closed under scalar multiplication.
  This holds because
  \[
              \lambda A_d
    =         \lambda 1 A_d
    \subseteq A_0 A_d
    \subseteq A_d
    \text{ for all }
    \lambda \in k
  \]
  for all $d \in \Z$.
\end{proof}


\begin{expls}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      Let $A$ be a $K$-algebra.
      Then $A$ is a graded $k$-Algebra via $A_0 = A$ and $A_d = 0$ fÃ¼r $d \neq 0$.
    \item
      Let $k$ be a field (or a ring).
      $k[X_1, \dotsc, X_n]$ is a graded $k$-algebra (or a graded ring) by setting
      \[
                  A_d
        \coloneqq \begin{cases}
                    \vspan_k(
                              \{X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
                            \mid
                              \sum_{i=1}^n a_i = d\}
                            )
                            & \text{if } d \geq 0 \,, \\
                    0       & \text{otherwise} \,.
                  \end{cases}
      \]
      By definition $A_d$ is a $k$-vector space (or $\Z$-module).
      Since the monomials form a $k$-Basis of $k[X_1, \dotsc, X_n]$ we have $A = \bigoplus_{d \geq 0} A_d = \bigoplus_{d \in \Z} A_d$.
      Since
      \[
          \left( X^{\alpha_1} \dotsm X^{\alpha_n} \right)
          \left( X^{\beta_1} \dotsm X^{\beta_n} \right)
        = X_1^{\alpha_1+\beta_1} \dotsm X_n^{\alpha_n+\beta_n}
      \]
      we find that for monomials $f \in A_i$ and $g \in A_j$ $fg \in A_{i+j}$.
      By extending this linearly we get that $A_i A_j \subseteq A_{i+j}$ for all $i,j \in \Z$.
    \item
      $T(V) \coloneqq k \oplus \bigoplus_{d \geq 1} V^{\otimes d}$ is an algebra by extending
      \[
          (v_{i_1} \otimes \dotsb \otimes v_{i_k}) \cdot (v_{j_1} \otimes \dotsb \otimes v_{j_n})
        = v_{i_1} \otimes \dotsb \otimes v_{i_k} \otimes v_{j_1} \otimes \dotsb \otimes v_{j_n}
      \]
      with $v_{i_1}, \dotsc, v_{i_k}, v_{j_1}, \dotsc, v_{j_n} \in V$ linearly.
      (We leave it as an exercise to check that this is indeed a $k$-algebra.)
      $T(V)$ is a graded algebra via
      \[
          T(V)
        = \bigoplus_{d \in \Z} T(V)_d \,,
      \]
      where
      \[
          T(V)_d
        = \begin{cases}
            V^{\otimes d} & \text{if } d > 0 \,,  \\
            k             & \text{if } d = 0 \,,  \\
            0             & \text{if } d < 0 \,.
          \end{cases}
      \]
  \end{enumerate}
\end{expls}


\begin{definition}
  A $k$-algebra $A$ is \emph{filtered} if there exists a (potentially infinite) sequence
  \[
              0
    =         F_{-1}(A)
    \subseteq F_0(A)
    \subseteq F_1(A)
    \subseteq F_2(A)
    \subseteq \dotsb
    \subseteq A
  \]
  of $k$-vector subspaces $F_i(A)$ such that
  \begin{gather*}
      \bigcup_{i \geq -1} F_i(A)
    = A
  \shortintertext{and}
              F_i(A) F_j(A)
    \subseteq F_{i+j}(A)
    \text{ for all } i,j \geq -1 \,.
  \end{gather*}
  This sequence is called a \emph{filtration of $A$}.
\end{definition}


\begin{definition}
  If $A$ is a filtered algebra and
  \[
              0
    =         F_{-1}(A)
    \subseteq F_0(A)
    \subseteq F_1(A)
    \subseteq F_2(A)
    \subseteq \dotsb
    \subseteq A
  \]
  a filtration of $A$, then define for all $i \geq 0$
  \[
              (\gr_\mc{F}A)_i
    \coloneqq F_i(A)/F_{i-1}(A)
  \]
  and
  \[
              \gr_\mc{F}(A)
    \coloneqq \bigoplus_{i \geq 0} (\gr_\mc{F}A)_i \,.
  \]
  $\gr_\mc{F}(A)$ is the \emph{associated graded algebra to the filtered algebra $A$}.
\end{definition}



\begin{lemma}
  $\gr_\mc{F}(A) = \bigoplus_{i \geq 0} (\gr_\mc{F}A)_i$ is a graded algebra with the multiplication indexed from the multiplication of $A$, i.e.\
  \[
      ( a + F_{i-1}(A) )( b + F_{j-1}(A) )
    = ab + F_{i+j-1}(A) \,.
  \]
\end{lemma}
\begin{proof}
  The multiplication is well-defined: Let $a \in F_i(A)$, $b \in F_j(A)$. Note that we have
  \begin{align*}
                F_{i-1}(A)F_j(A)
    &\subseteq  F_{i+j-1}(A), \\
                F_i(A) F_{j-1}(A)
    &\subseteq  F_{i+j-1}(A) \text{ and } \\
                F_{i-i}(A)F_{j-1}(A)
    &\subseteq  F_{i+j-2}(A) \subseteq F_{i+j-1}(A) \,.
  \end{align*}
  Because of this, we have for all $x_1 \in F_{i-1}(A)$ and $x_2 \in F_{j-1}(A)$ that
  \[
      (a + x_1)(b + x_2)
    = a b + x
  \]
  for some $x \in F_{i+j-1}(A)$. So we get a well-defined multiplication
  \begin{align*}
              (\gr_\mc{F} A) \times (\gr_\mc{F} A)
    &\to      \gr_\mc{F} A \,,
    \\
              (a + F_{i-1}, b + F_{j-i})
    &\mapsto  ab + F_{i+j-1} \,.
  \end{align*}
  It now easily follows that $\gr_\mc{F} A$ is an associative algebra.
  
  It is clear, that $(\gr_\mc{F} A)_i$ is a $k$-vector subspace of $\gr_\mc{F} A$, and by the definition of the multiplication we have
  \[
              (\gr_\mc{F} A)_i (\gr_\mc{F} A)_j
    \subseteq (\gr_\mc{F} A)_{i+j}
    \text{ for all }
    i,j \geq 0 \,.
  \]
  This shows that $\gr_\mc{F} A$ is a graded algebra.
\end{proof}


\begin{lemma}
  Let $A = \bigoplus_{d \in \Z} A_d$ be a graded $k$-algebra with $A_d = 0$ for all $d < 0$.
  Then
  \[
              F_i(A)
    \coloneqq \bigoplus_{d \leq i} A_d
    \text{ for all }
    i \geq -1
  \]
  defines a filtration on $A$.
\end{lemma}
\begin{proof}
  $F_i(A)$ is a vector subspace for all $i \geq -1$, because $A_d$ is a vector subspace for all $d \in \Z$.
  It is also clear that $F_{-1}(A) = 0$ and $F_i(A) \subseteq F_{i+1}(A)$ for all $i \geq -1$.
  Since $A = \bigoplus_{d \geq 0} A_d$ we have that $A = \bigcup_{i \geq -1} F_i(A)$.
  We also have
  \begin{align*}
              F_i(A) F_j(A)
    &=        \left( \bigoplus_{d \leq i} A_d \right) \left( \bigoplus_{d \leq j} A_d \right)
    \subseteq \sum_{\substack{d_1 \leq i \\ d_2 \leq j}} A_{d_1 + d_2}
    \subseteq \bigoplus_{d \leq i+j} A_d \\
    &=        F_{i+j}(A).
    \qedhere
  \end{align*}
\end{proof}


\begin{expl}
  Consider $k[X]$ for some field $k$.
  Then the multiplication with $X$ defines an element $X \in \End_k(k[X])$.
  Let $\partial = \partial/\partial x \in \End_k(k[X])$ be the (formal) derivative with respect to $X$.
  
  Consider the subalgebra $\mc{A}_1$ of $\End_k(k[X])$ generated by $X$ and $\partial$.
  Then $\mc{A}_1 \cong k\gen{X,\partial}/\mc{I}$ where $\mc{I} \subseteq k \gen{X,\partial}$ is the two sided ideal generated by the element $\partial X - X \partial - 1$.
  The images of the monomials $X^\alpha \partial^\beta$, $\alpha, \beta \in \N$ under this isomorphism form a $k$-basis of $\mc{A}_1$.
  We can then define
  \[
              F_i(\mc{A}_1)
    \coloneqq \vspan_k( \{\text{images of $X^\alpha \partial^\beta$ where $\alpha+\beta \leq i$})
    \text{ for all }
    i \geq -1 \,.
  \]
  This gives us a filtration on $\mc{A}_1$.
  (We leave the proof of this claims as an exercise to the reader. It will also appear on the exercise sheets.)
\end{expl}





\section{Symmetric Polynomials}


\begin{definition}
  Let $k$ be a field.
  $G \coloneqq S_n$ acts linearly on $k[X_1, \dotsc, X_n]$ by extending
  \[
      g.X^{\alpha_1} \dotsm X_n^{\alpha_n}
    = X_{g(1)}^{\alpha_1} \dotsm X_{g(n)}^{\alpha_n}
    \text{ for all }
    g \in G
  \]
  linearly.
  A polynomial $f \in k[X_1, \dotsc, X_n]^{S_n}$ is called a \emph{symmetric polynomial (in $n$ variables)}.
\end{definition}


\begin{expl}
  In $k[X_1, X_2, X_3]$ we have the symmetric polynomials
  \begin{align*}
                p_2
    &\coloneqq  X_1^2 + X_2^2 + X_3^2 \,,
    \\
                h_2
    &\coloneqq  X_1^2 + X_1 X_2 + X_1 X_3 + X_2^2 + X_2 X_3 + X_3^2 \,,
    \\
                e_2
    &\coloneqq  X_1 X_2 + X_1 X_3 + X_2 X_3 \,,
    \\
                m_{(4,4,2)}
    &\coloneqq  X_1^4 X_2^2 X_3^2 + X_1^2 X_2^4 X_3^2 + X_1^2 X_2^2 X_3^4 \,.
  \end{align*}
  More generally in $k[X_1, \dotsc, X_n]$ we have the following symmmetric polynomials:
\end{expl}


\begin{definition}
  Fix $n \in \N$.
  The \emph{$r$-th power symmetric polynomial}, also called the \emph{$r$-th power sum}, is defined as
  \[
              p_r^{(n)}
    \coloneqq X_1^r + \dotsb + X_n^r \,,
  \]
  the \emph{$r$-th completely symmetric polynomial} as
  \[
              h_r^{(n)}
    \coloneqq \sum_{|\alpha|=r} X_1^{\alpha_1} \dotsm X_n^{\alpha_n} \,.
  \]
  and the \emph{$r$-th elementary symmetric polynomial} as
  \[
              e_r^{(n)}
    \coloneqq \sum_{1 \leq i_1 < \dotsb < i_r \leq n} X_{i_1} \dotsm X_{i_r} \,.
  \]
  We also set $e_0^{(n)} \coloneqq 1$ and $e_r^{(n)} = 0$ for $r > n$.
  If the number of variables in clear we also write $p_r$, $h_r$ and $e_r$.
\end{definition}

Another possible way of writing $e^{(n)}_r$ is
\[
    e^{(n)}_r
  = \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i \,.
\]
From this we get that $e^{(n)}_0 = 1$ because the only subset of $\{1, \dotsc, n\}$ which contains $0$ elements is the empty set and thus the sum consists only of the empty product.
We also get that $e^{(n)}_r = 0$ for $r > n$ since $\{1, \dotsc, n\}$ has no subsets which contain more than $n$ elements.

The elementary symmetric polynomials appear in very natural ways.
If, for example, we have elements $a_1, \dotsc, a_n \in k$ then we have
\begin{align*}
   &\, (t-a_1) \dotsm (t-a_n) \\
  =&\, t^n - (a_1 + \dotsb + a_n) t^{n-1} + \sum_{1 \leq i_1 < i_2 \leq n} a_{i_1} a_{i_2} t^{n-2} + \dotsb + (-1)^n a_1 \dotsm a_n \\ 
  =&\, t^n - e^{(n)}_1(a_1, \dotsc, a_n) t^{n-1} + e^{(n)}_2(a_1, \dotsc, a_n) t^{n-2} + \dotsb + (-1)^n e^{(n)}_n(a_1, \dotsc, a_n)
\end{align*}
in $k[t]$.
We will formalize this observation in the following lemma.


\begin{lemma}
  For all $n \in \N$ we have
  \[
      \prod_{i=1}^n (t-X_i)
    = t^n - e^{(n)}_1 t^{n-1} + e^{(n)}_2 t^{n-2} + \dotsb + (-1)^n e^{(n)}_n
  \]
  in $k[X_1, \dotsc, X_n][t]$.
\end{lemma}
\begin{proof}
  For $n = 0$ the statement is clear.
  For $n \geq 1$ the coefficient of the left-hand side of $t^{n-r}$ is $e^{(n)}_0 = 1$ for $r = 0$ and
  \[
      \sum_{1 \leq i_1 < \dotsb < i_r \leq n} (-X_{i_1}) \dotsm (-X_{i_r})
    = (-1)^r e^{(n)}_r \,.
  \]
  for $0 < r \leq n$.
\end{proof}


\begin{corollary}
  For all $n \in \N$ we have
  \[
      p^{(n)}_n - e^{(n)}_1 p^{(n)}_{n-1} + e^{(n)}_2 p^{(n)}_{n-2} + \dotsb + (-1)^n n e^{(n)}_n
    = 0 \,.
  \]
\end{corollary}
\begin{proof}
  For $n = 0$ the statement is clear. For $n > 0$ we have
  \[
      \prod_{i=1}^n (t-X_i)
    = t^n - e^{(n)}_1 t^{n-1} + e^{(n)}_2 t^{n-2} + \dotsb + (-1)^n e^{(n)}_n
  \]
  and evaluating at $t = X_j$ gives
  \[
      X_j^n - e^{(n)}_1 X_j^{n-1} + e^{(n)}_2 X_j^{n-2} + \dotsb + (-1)^n e^{(n)}_n
    = 0 \,.
  \]
  By summing over all $1 \leq j \leq n$ we get
  \[
      p^{(n)}_n - e^{(n)}_1 p^{(n)}_{n-1} + e^{(n)}_2 p^{(n)}_{n-2} + \dotsb + (-1)^n n e^{(n)}_n
    = 0 \,.
    \qedhere
  \]
\end{proof}


\begin{rem}[Newtonâs identities]
  One can show that more generally
  \[
      p^{(n)}_r - e^{(n)}_1 p^{(n)}_{r-1} + \dotsb + (-1)^{r-1} e^{(n)}_{r-1} p^{(n)}_1 + (-1)^r r e^{(n)}_r
    = 0 \,.
  \]
  for all $r \geq 0$.
  For $m = n$ this gives us the previous corollary.
\end{rem}
\begin{proof}
  The proof of these identities is an exercise on the 4th exercise sheet.
  We will also give a proof later on.
\end{proof}


\begin{theorem}[Fundamental Theorem of symmetric functions]
  The symmetric polynomials $e^{(n)}_1, \dotsc, e^{(n)}_n$ generate the $k$-algebra of symmetric functions $k[X_1, \dotsc, X_n]^{S_n}$.
  Moreover they are algebraically independent.
  In other words
  \[
            k[X_1, \dotsc, X_n]^{S_n}
    \cong   k[X_1, \dotsc, X_n]
    \quad\text{via}\quad
            e^{(n)}_k
    \mapsto X_k \,.
  \]
\end{theorem}


\begin{lemma}
  A polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if its homogeneous parts are symmetric.
\end{lemma}
\begin{proof}
  We have $k[X_1, \dotsc, X_n] = \bigoplus_{d \geq 0} k[X_1, \dotsc, X_n]_d$ as a graded $k$-algebra.
  This decomposition is also a decomposition into subrepresentations of $S_n$.
  So the statement follows from Lemma \ref{lemma: direct sum and invariants commute}.
\end{proof}


Using this Lemma we can now easily prove the Fundamental Theorem.
We will give two possible proofs, where the second one is the one given in the lecture.


\begin{proof}[Proof of the Fundamental Theorem]
  To makes our lifes easier we introduce an ordering on the monomials in $k[X_1, \dotsc, X_n]$:
  We first order the monomials by their power of $X_1$ from lowest to highest.
  The monomials with the same power of $X_1$ are then ordered in the same way by their power of $X_2$, then those with the same power of $X_2$ by their power of $X_3$, and so on.
  In a more formal way:
  For monomials $X^\alpha = X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ and $X^\beta = X_1^{\beta_1} \dotsm X_n^{\beta_n}$ we have $X^\alpha > X^\beta$ if and only if there exists $1 \leq i \leq n$ such that $\alpha_1 = \beta_1$, $\alpha_2 = \beta_2, \dotsc, \alpha_{i-1} = \beta_{i-1}$ and $\alpha_i > \beta_i$.
  This gives us a total order on the monomials in $k[X_1, \dotsc, X_n]$.
  
  For a polynomial $p \in k[X_1, \dotsc X_n]$ with $p \neq 0$ we define $\init p$ to be the highest monomial occuring in $p$, including its coefficient.
  One can easily check the following properties:
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      If $p$ is symmetrical and $\init p = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}$ we have $\alpha_1 \geq \alpha_2 \geq \dotsb \geq \alpha_n$.
    \item
      For $q \in k[X_1, \dotsc, X_n], q \neq 0$ we have $\init (p \cdot q) = \init p \cdot \init q$.
    \item
      $\init e^{(n)}_k = X_1 \dotsm X_k$ for all $1 \leq k \leq n$.
  \end{enumerate}
  We are now well-equipped to prove the Theorem. 
  
  We first show that $e^{(n)}_1, \dotsc, e^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra.
  For this let $f \in k[X_1, \dotsc, X_n]^{S_n}$ with $f \neq 0$.
  By the previous Lemma we may assume that $f$ is homogeneous of degree $d \geq 0$.
  For
  \[
      \init f
    = c X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
  \]
  we define
  \[
      p
    =         c
              \left( e^{(n)}_1 \right)^{\alpha_1 - \alpha_2}
      \dotsm \left( e^{(n)}_{n-1} \right)^{\alpha_{n-1} - \alpha_n}
              \left( e^{(n)}_n \right)^{\alpha_n}.
  \]
  From the properties listed above it directly follows that that $p$ is well-defined with $\init p = \init f$.
  Because $e^{(n)}_k$ is homogenous of degree $k$ it follows that $p$ is homogeneous of degree
  \begin{align*}
     &\,  (\alpha_1-\alpha_2) + 2(\alpha_2-\alpha_3) + \dotsb + (n-1)(\alpha_{n-1}-\alpha_n)+n\alpha_n \\
    =&\,  \alpha_1 + \dotsb + \alpha_n
    =     d \,.
  \end{align*}
  (Because $f$ is homogenous we have $d = \deg f = \deg \init f = \alpha_1 + \dotsb + \alpha_n$.)
  Combining these observations we find that $f-p$ is a homogeneous symmetric polynomial of degree $d$ with either $f-p = 0$ or at least $\init (f-p) < \init f$.
  Because there are only finitely many monomials of degree $d$ repeating this procedure must lead us to the zero polynomial in finitely many steps.
  
  To show that $e^{(n)}_1, \dotsc, e^{(n)}_n$ are algebraically independent we need to show that the monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$, i.e.\
  \[
      \left(e^{(n)}\right)^\alpha
    = \left(e^{(n)}_1\right)^{\alpha_1} \dotsm \left(e^{(n)}_n\right)^{\alpha_n}
    \text{ with }
      \alpha
    = (\alpha_1, \dotsc, \alpha_n)
  \]
  are linearly independent.
  For this we notice that for $(\alpha_1, \dotsc, \alpha_n) \neq (\beta_1, \dotsc, \beta_n)$ we have
  \begin{align*}
        &\, \init \left(e^{(n)}_1\right)^{\alpha_1} \dotsm \left(e^{(n)}_n\right)^{\alpha_n}            \\
       =&\, X_1^{\alpha_1 + \dotsb + \alpha_n} X_2^{\alpha_2 + \dotsb + \alpha_n} \dotsm X_n^{\alpha_n} \\
    \neq&\, X_1^{\beta_1 + \dotsb + \beta_n} X_2^{\beta_2  + \dotsb + \beta_n} \dotsm X_n^{\beta_n}     \\
       =&\, \init \left(e^{(n)}_1\right)^{\beta_1} \dotsm \left(e^{(n)}_n\right)^{\beta_n}.
  \end{align*}
  Now suppose that
  \[
      0
    = \lambda_1 \left(e^{(n)}\right)^{\alpha_1} + \dotsb + \lambda_s \left(e^{(n)}\right)^{\alpha_s}
  \]
  with $s \geq 1$, $\alpha_i \neq \alpha_j$ for $i \neq j$ and $\lambda_i \neq 0$ for all $1 \leq i \leq s$.
  We can assume w.l.o.g.\ that 
  \[
      \init \left(e^{(n)}\right)^{\alpha_1}
    > \init \left(e^{(n)}\right)^{\alpha_2}
    > \dotsb > \init \left(e^{(n)}\right)^{\alpha_s}.
  \]
  With this we directly find that the $\init \left(e^{(n)}\right)^{\alpha_1}$ only occures in $\left(e^{(n)}\right)^{\alpha_1}$ and thus $\lambda_1 = 0$, in contradiction to $\lambda_1 \neq 0$.
\end{proof}


\begin{proof}[Proof of the Fundamental Theorem (lecture)]
  Note that
  \begin{align*}
        e^{(n)}_1
    &=  X_1 + \dotsb + X_n = e_1^{(n-1)} + X_n \,,  \\
        e^{(n)}_2
    &=  \sum_{1 \leq i_1 < i_2 \leq n} X_{i_1} X_{i_2} = e_2^{(n-1)} + X_n e_1^{(n-1)} \,,  \\
    &   \vdots \tag{$\ast$}\\ 
        e^{(n)}_r
    &=  e^{(n-1)}_r + X_n e^{(n-1)}_{r-1} \text{ for } 1 \leq r \leq n-1 \,,  \\
    &   \vdots \\
        e^{(n)}_n
    &=  X_1 \dotsm X_{n-1} X_n = X_n e^{(n-1)}_{n-1} \,.
  \end{align*}
  \begin{claim}
    A polynomial $f \in k[X_1, \dotsc, X_n]$ is symmetric if and only if $f$ can be written as a polyonmial in $e^{(n)}_1, \dotsc, e^{(n)}_n$, i.e.\ we have
    \[
        k\left[ e^{(n)}_1, \dotsc, e^{(n)}_n \right]
      = k[X_1, \dotsc, X_n]^{S_n}.
    \]
  \end{claim}
  \begin{proof}[Proof of claim]
    It is clear that $k\left[ e^{(n)}_1, \dotsc, e^{(n)}_n \right] \subseteq k[X_1, \dotsc, X_n]$.
    We show the other inclusion by induction over $n$.
    
    For $n = 1$ the claim is clear. Let $n \geq 2$ and suppose that the claim holds for $n-1$.
    We show the claim for $n$ by induction over $\deg f$.
    For $\deg f = 0$ the claim is clear.
    Let $d \geq 1$ and suppose the claim holds for $0, \dotsc, d-1$.
    By the previous Lemma we may assume that $f$ is homogenous of degree $d$.
    (The homogenous parts of lower degree are by induction hypothesis expressable as polynomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$.)
    Let
    \[
              q
      \colon  k[X_1, \dotsc, X_n]
      \to     k[X_1, \dotsc, X_n]/(X_n)
      \cong   k[X_1, \dotsc, X_{n-1}]
    \]
    be the evaluation at $X_n = 0$.
    By $(\ast)$ we have
    \begin{align*}
          q\left( e^{(n)}_r \right)
      &=  e^{(n-1)}_r
      \text{ for all } 1 \leq r < n
      \text{ and}
      \\
          q\left( e^{(n)}_n \right)
      &=  0 \,.
    \end{align*}
    We notice that $q(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric:
    Because $f$ is symmetric we have
    \[
        f
      = f(X_1, \dotsc, X_n)
      = f(X_{\sigma(1)}, \dotsc, X_{\sigma(n)})
    \]
    for every $\sigma \in S_n$.
    Thus we have
    \[
        f(X_1, \dotsc, X_{n-1}, 0)
      = f(X_{\tau(1)}, \dotsc, X_{\tau(n-1)}, 0) \,.
    \]
    for every $\tau \in S_{n-1}$.
    Because $q(f) \in k[X_1, \dotsc, X_{n-1}]$ is symmetric we can use the induction hypothesis (from the induction on $n$) to write
    \[
        q(f)
      = P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right)
    \]
    for some polynomial $P \in k[X_1, \dotsc, X_{n-1}]$.
    Consider the symmetric polynomial
    \[
                g
      \coloneqq P\left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1} \right)
      \in       k[X_1, \dotsc, X_n] \,.
    \]
    Because $q$ is a $k$-algebra homomorphism we have
    \begin{align*}
         q(g)
      &= q\left( P\left(e^{(n)}_1, \dotsc, e^{(n)}_{n-1}\right) \right) \\
      &= P\left( q\left(e^{(n)}_1\right), \dotsc, q\left(e^{(n)}_{n-1}\right) \right) \\
      &= P\left( e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1} \right) \\
      &= q(f)
    \end{align*}
    and therefore
    \[
        q(f - g)
      = 0 \,.
    \]
    Because $\ker q = (X_n)$ this means that $X_n \mid (f-g)$.
    Because $f-g$ is symmetric (because $f$ and $g$ are symmetric) it follows that $X_i \mid (f-g)$ for all $1 \leq i \leq n$.
    Therefore $X_1 \dotsm X_n \mid (f-g)$ and we can set
    \[
                h
      \coloneqq \frac{f-g}{X_1 \dotsm X_n}
      =         \frac{f-g}{e^{(n)}_n} \,.
    \]
    (Here we use that $k[X_1, \dotsc, X_n]$ is an unique factorization domain.)
    We have $\deg h < \deg f = d$, since $\deg g \leq \deg f$.
    [I have no idea why $\deg g \leq \deg f$.]
    By induction hypothesis (of the induction on $d$) we can write $h$ as a polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$ and therefore also $f-g = h e^{(n)}_n$.
    Since $g$ is a Polynomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$ so is $f$.
    % TODO: Why is deg(g) <= deg(f)? Where do we use that f is homogeneous?
  \end{proof}
  All that is left to show is that $e^{(n)}_1, \dotsc, e^{(n)}_n$ are algebraically independent.
  We prove this by induction over $n$.
  
  For $n = 1$ this is clear, since $e^{(1)}_1 = X_1$.
  Now suppose $n \geq 2$ and that the elements $e^{(n-1)}_1, \dotsc, e^{(n-1)}_{n-1}$ are algebraically independent.
  Suppose that
  \[
      F\left(e^{(n)}_1, \dotsc, e^{(n)}_n\right)
    = 0
  \]
  for some polynomial $F \in k[X_1, \dotsc, X_n]$ with $F \neq 0$ of minimal possible degree.
  Then
  \begin{align*}
        0
    &=  q \left( F \left( e^{(n)}_1, \dotsc, e^{(n)}_{n-1}, e^{(n)}_n \right) \right) \\
    &=  F \left(
            q \left( e^{(n)}_1 \right), \dotsc, q \left( e^{(n)}_{n-1} \right), q \left( e^{(n)}_n \right)
          \right) \\
    &=  F \left( e^{n-1}_1, \dotsc, e^{(n-1)}_{n-1}, 0 \right).
  \end{align*}
  Using the induction hypothesis we find that $F(X_1, \dotsc, X_{n-1}, 0) = 0$.
  Therefore $X_n \mid F$.
  This means that there exists some $\hat{F} \in k[X_1, \dotsc, X_n]$ with $F = X_n \hat{F}$.
  In particular $\hat{F} \neq 0$.
  We have
  \[
      0
    = F\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = e^{(n)}_n \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right).
  \]
  Because $k[X_1, \dotsc, X_n]$ is an integral domain and $e^{(n)}_n \neq 0$ it follows that
  \[
      \hat{F}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
    = 0 \,.
  \]
  Because $\hat{F} \neq 0$ and $\deg \hat{F} < \deg F$ this contradicts the assumption that $F$ is of lowest possible degree.
\end{proof}


\begin{rem}
  Each of the proofs gives us an algorithm how to express a symmetric polynomial in terms of $e^{(n)}_1, \dotsc, e^{(n)}_n$.
\end{rem}


\begin{rem}
  The Fundamental Theorem also holds for $k = \Z$.
\end{rem}


To better understand the polynomials $e^{(n)}_k$, $h^{(n)}_k$ and $p^{(n)}_k$ and how they interact with each other it is helpful to have a look at the corresponding generating functions in $k[X_1, \dotsc, X_n]\dblbrack{t}$, i.e.\
\begin{align*}
              E(t)
  &\coloneqq  \sum_{k \geq 0} e^{(n)}_k t^k \,,
  \\
              H(t)
  &\coloneqq  \sum_{k \geq 0} h^{(n)}_k t^k \,,
  \\
              P(t)
  &\coloneqq  \sum_{k \geq 0} p^{(n)}_{k+1} t^k \,.
\end{align*}
(Notice that for $P(t)$ we shift the index by $1$.)


\begin{proposition}
  We have the following equalities of power series:
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      $E(t) = \prod_{i=1}^n (1 + X_i t)$.
    \item
      $H(t) = \prod_{i=1}^n \frac{1}{1 - X_i t}$.
    \item
      $P(t) = \sum_{i=1}^n \frac{X_i}{1 - X_i t}$.
      More generally $\sum_{k \geq 0} p_{k+l} t^k = \sum_{i=1}^n \frac{X_i^l}{1 - X_i t}$ for all $l \geq 0$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      The coefficient of $t^r$ on the right hand side of the equation is
      \[
        \sum_{\substack{I \subseteq \{1, \dotsc, n\} \\ |I| = r}} \prod_{i \in I} X_i \,,
      \]
      which is precisely $e^{(n)}_r$.
    \item
      It is clear that $1-X_i t$ is invertible in $k[X_1, \dotsc, X_n]\dblbrack{t}$.
      (For a commutative ring $R$ a power series $\sum_{i \in \N} a_i t^i \in R\dblbrack{t}$ is invertible if and only if $a_0 \in R^\times$.)
      For $1 \leq i \leq n$ let
      \[
                  Q_i
        \coloneqq \frac{1}{1-X_i t} \in k[X_1, \dotsc, X_n] \dblbrack{t} .
      \]
      We then have
      \[
          \prod_{i=1}^n \frac{1}{1-X_i t}
        = Q_1 \dotsm Q_n \,.
      \]
      By using the geometric series it is now easy to see that
      \[
          Q_i
        = 1 + X_i t + X_i^2 t^2 + X_i^3 t^3 + \dotsb
      \]
      With this we find that the coefficient of $t^r$ in $Q_1 \dotsm Q_n$ is
      \[
        \sum_{|\alpha| = r} X_1^{\alpha_1} \dotsm X_n^{\alpha_n} \,,
      \]
      which is precisely $h^{(n)}_r$.
    \item
      We have
      \begin{align*}
         &\, \sum_{i=1}^n \frac{X_i^l}{1-X_i t}
        =    \sum_{i=1}^n X_i^l Q_i \\
        =&\, \sum_{i=1}^n (X_i^l + X_i^{l+1} t + X_i^{l+2} t^2 + X_i^{l+3} t^3 + \dotsb) \\
        =&\, p^{(n)}_l + p^{(n)}_{l+1} t + p^{(n)}_{l+2} t^2 + p^{(n)}_{l+3} t^3 + \dotsb
        \qedhere
      \end{align*}
  \end{enumerate}
\end{proof}


From this equalities we can now easily derive more equalities.


\begin{corollary}
  For all $s \geq 1$ we have
  \[
        h^{(n)}_s
      - e^{(n)}_1 h^{(n)}_{s-1}
      + e^{(n)}_2 h^{(n)}_{s-2}
      + \dotsb
      + (-1)^{s-1} e^{(n)}_{s-1} h^{(n)}_1
      + (-1)^s e^{(n)}_s
    = 0
  \]
  as well as
  \[
        e^{(n)}_s
      - h^{(n)}_1 e^{(n)}_{s-1}
      + h^{(n)}_2 e^{(n)}_{s-2}
      + \dotsb
      + (-1)^{s-1} h^{(n)}_{s-1} e^{(n)}_1
      + (-1)^s h^{(n)}_s
    = 0 \,
  \]
\end{corollary}
\begin{proof}
  From the previous proposition it follows that
  \[
      E(-t)H(t)
    = H(-t)E(t) = 1 \,.
  \]
  The first sum is the $s$-th coefficient of $E(-t)H(t)$ and the second sum is the $s$-th coefficient of $H(-t)E(t)$.
  One equation can also be obtained from the other by multiplying with $(-1)^s$.
\end{proof}


\begin{corollary}
  For all $r \geq 1$ we have
  \[
      r h^{(n)}_r
    =   p^{(n)}_1 h^{(n)}_{r-1}
      + p^{(n)}_2 h^{(n)}_{r-2}
      + \dotsb
      + p^{(n)}_{r-1} h^{(n)}_1
      + p^{(n)}_r.
  \]
\end{corollary}
\begin{proof}
  By $H'(t)$ we denote the formal derivative of $H$ with respect to $t$.
  On the one hand we have
  \[
      H'(t)
    = \sum_{k \geq 1} k h^{(n)}_k t^{k-1} \,.
  \]
  On the other hand we have
  \[
      H'(t)
    = \sum_{i=1}^n \frac{X_i}{(1-X_i t)^2} \prod_{j \neq i} \frac{1}{1 - X_j t}
    = \sum_{i=1}^n \frac{X_i}{1 - X_i t} \prod_{j=1}^n \frac{1}{1 - X_j t}
    = P(t) H(t) \,.
  \]
  By comparing the $(r-1)$-th coefficient on both expressions we find that
  \[
      r h^{(n)}_r
    = \sum_{i=0}^{r-1} p^{(n)}_{1+i} h_{r-1-i}
    = \sum_{i=1}^r p^{(n)}_i h^{(n)}_{r-i}
  \]
  for all $r \geq 1$.
\end{proof}


In the same way we can derive Newtonâs identities as another corollary.


\begin{proof}[Proof of Newtonâs identities]
  We have
  \[
      E'(t)
    = \sum_{k \geq 1} k e^{(n)}_k t^{k-1}
  \]
  as well as
  \[
      E'(t)
    = \sum_{i=1}^n X_i \prod_{j \neq i} (1 + X_j t)
    = \sum_{i=1}^n \frac{X_i}{1 + X_i t} \prod_{j=1}^n (1 + X_j t)
    = P(-t)E(t) \,.
  \]
  Comparing the $(r-1)$-th coefficient of both expressions gives us
  \[
      r e^{(n)}_r
    = \sum_{i=0}^{r-1} (-1)^i p^{(n)}_{1+i} e^{(n)}_{r-1-i}
    = \sum_{i=1}^r (-1)^{i+1} p^{(n)}_i e^{(n)}_{r-i} \,.
  \]
  for all $r \geq 1$ and thus
  \begin{align*}
        0
    &=  \sum_{i=0}^r (-1)^{i+1} p^{(n)}_i e^{(n)}_{r-i} \\
    &=  (-1)^{r+1} p^{(n)}_r + (-1)^r p^{(n)}_{r-1} e^{(n)}_1 + \dotsb + p^{(n)}_1 e^{(n)}_{r-1} - r e^{(n)}_r \,.
  \end{align*}
  Multplying by $(-1)^{r+1}$ results in Newtonâs identities as they were given before.
\end{proof}



Using the first corallary we can not only give recursive formals for expressing $h^{(n)}_1, \dotsc, h^{(n)}_n$ as polynomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$ (which we knew was possible from the Fundamental theorem) but also formulas for expressing $e^{(n)}_1, \dotsc, e^{(n)}_n$ in polynomials of $h^{(n)}_1, \dotsc, h^{(n)}_n$.
Thus we find that $k[X_1, \dotsc, X_n]^{S_n}$ is also generated by $h^{(n)}_1, \dotsc, h^{(n)}_n$.
One natural question which comes with this observation is whether $h^{(n)}_1, \dotsc, h^{(n)}_n$ are also algebraically independent, which the symmetry of the two equations in the first corollary seem to hint at.
Using this symmetrie we get the following Lemma.


\begin{lemma}
  There exists polynomials $F_1, \dotsc, F_n \in k[X_1, \dotsc, X_n]$ such that
  \[
      h^{(n)}_i
    = F_i\left(e^{(n)}_1, \dotsc, e^{(n)}_n\right)
    \text{ and }
      e^{(n)}_i
    = F_i\left(h^{(n)}_1, \dotsc, h^{(n)}_n\right)
  \]
  for all $1 \leq i \leq n$.
\end{lemma}
\begin{proof}
  We construct $F_1, \dotsc, F_n$ recursively.
  Since $e^{(n)}_1 = h^{(n)}_1$ we start with $F_1 = X_1$.
  If $F_1, \dotsc, F_{k-1}$ are already constructed for $k \leq n$ we set
  \[
              F_k
    \coloneqq   X_1 F_{k-1}
              - X_2 F_{k-2}
              + \dotsb
              + (-1)^k X_{k-1} F_1
              + (-1)^{k+1} X_k.
  \]
  From the first of the previous corollaries and the induction hypothesis it follows that
  \begin{align*}
     &\,  F_k\left(e^{(n)}_1, \dotsc, e^{(n)}_n\right) \\
    =&\,    e^{(n)}_1 F_{k-1}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
          - e^{(n)}_2 F_{k-2}\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
          + \dotsb
          + (-1)^{k+1} e^{(n)}_k \\
    =&\,    e^{(n)}_1 h^{(n)}_{k-1}
          - e^{(n)}_2 h^{(n)}_{k-2}
          + \dotsb
          + (-1)^k e^{(n)}_{k-1} h^{(n)}_1
          + (-1)^{k+1} e^{(n)}_k \\
    =&\,  h^{(n)}_k,
  \end{align*}
  and by switching $e^{(n)}_i$ and $h^{(n)}_i$ we also find that
  \[
      F_k\left(h^{(n)}_1, \dotsc, h^{(n)}_n\right)
    = e^{(n)}_k.
    \qedhere
  \]
\end{proof}


With the help of this Lemma we can now easily prove the following Theorem, which points out a deep connection between the elementary symmetric polynomials and completely homogenous symmetirc polynomials and how they are, in a certain way, dual to each other.


\begin{theorem}
There exists an unique homomorphism
\[
          \phi
  \colon  k[X_1, \dotsc, X_n]^{S_n}
  \to     k[X_1, \dotsc, X_n]^{S_n}
\]
of $k$-algebras with $\phi\left(e^{(n)}_i\right) = h^{(n)}_i$ for all $1 \leq i \leq n$.
$\phi$ in an automorphism and involutive
\end{theorem}
\begin{proof}
  The existence and uniqueness of $\phi$ follow directly from the fact that $e^{(n)}_1, \dotsc, e^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent.
  (This is basically the universal property of the polynomial ring.)
  Using the previous Lemma we find that for all $1 \leq i \leq n$
  \begin{align*}
        \phi\left(h^{(n)}_i\right)
    &=  \phi\left(F_i\left(e^{(n)}_1, \dotsc, e^{(n)}_n\right)\right) \\
    &=  F_i\left(\phi\left(e^{(n)}_1\right), \dotsc, \phi\left(e^{(n)}_n\right)\right) \\
    &=  F_i\left(h^{(n)}_1, \dotsc, h^{(n)}_n\right)
    =   e^{(n)}_i
  \end{align*}
  and thus
  \[
      \phi^2\left(e^{(n)}_i\right)
    = \phi\left(h^{(n)}_i\right) = e^{(n)}_i.
  \]
  Since $\phi^2\left(e^{(n)}_i\right) = e^{(n)}_i$ for all $1 \leq i \leq n$ we find that $\phi^2 = \id_{k[X_1, \dotsc, X_n]^{S_n}}$ by using the universal property of the polynomial ring once more.
\end{proof}


This Theorem shows that $h^{(n)}_1, \dotsc, h^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent.
As for the Fundamental Theorem this result also holds for $k = \Z$.


Since $e^{(n)}_1, \dotsc, e^{(n)}_n$ and $h^{(n)}_1, \dotsc, h^{(n)}_n$ are each algebraically independent and generate $k[X_1, \dotsc, X_n]^{S_n}$ for arbitrary fields and also $k = \Z$, it seems natural to ask if this is also true for $p^{(n)}_1, \dotsc, p^{(n)}_n$.
The next theorem shows that this is true under additional assumptions.


\begin{theorem}
  Let $k$ be a field with either $\kchar k = 0$ or $\kchar k > n$.
  Then $p^{(n)}_1, \dotsc, p^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent.
\end{theorem}
\begin{proof}
  We first show that $p^{(n)}_1, \dotsc, p^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra.
  For this it is enough to show that $e^{(n)}_1, \dotsc, e^{(n)}_n$ can be expressed as polynomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$.
  That this is possible follows directly from Newtonâs identities.
  (Here we need that $2, \dotsc, n$ are invertible in $k$.)
  
  To show that $p^{(n)}_1, \dotsc, p^{(n)}_n$ are algebraically independent we need to show that the monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$, i.e.\
  \[
      \left(p^{(n)}\right)^\alpha
    = \left(p^{(n)}_1\right)^{\alpha_1} \dotsm \left(p^{(n)}_n\right)^{\alpha_n}
    \text{ with }
      \alpha
    = (\alpha_1, \dotsc, \alpha_n)
  \]
  are linearly independent.
  For this is sufficies to show that the monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$ of degree $\leq N$ form a $k$-basis of the symmetric polynomials of degree $\leq N$.
  We denote the number of (pairwise different) monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$ by $P_N$ and this subspace by $V_N$.
  Because $p^{(n)}_1, \dotsc, p^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra we already know that $V_N$ is generated as a vector subspace by the monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$ of degree $\leq N$.
  Thus $\dim V_N \leq P_N$.
  Because $e^{(n)}_1, \dotsc, e^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent we also know that the monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$ of degree $\leq N$ form a $k$-basis of $V_N$.
  We denote the number of these monomials by $E_N$ and have $\dim V_N = E_N$.
  Since $P_N \leq E_N$ we also have $P_N \leq \dim V_N$. (That $P_N \leq E_N$ follows from the fact that for each monomial in $p^{(n)}_1, \dotsc, p^{(n)}_n$ we have a corresponding monomial in $e^{(n)}_1, \dotsc, e^{(n)}_n$ of the same degree, i.e.\ we have a surjective map
\begin{align*}
  \left\{
    \begin{array}{c}
      \text{monomials in} \\
      e^{(n)}_1, \dotsc, e^{(n)}_n \\
      \text{of degree $\leq N$}
    \end{array}
  \right\}
  \to
  \left\{
    \begin{array}{c}
      \text{monomials in} \\
      p^{(n)}_1, \dotsc, p^{(n)}_n \\
      \text{of degree $\leq N$}
    \end{array}
  \right\},
  \quad   \left( e^{(n)} \right)^\alpha
  \mapsto \left( p^{(n)} \right)^\alpha,
\end{align*}
where we use that the monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$ are pairwise different.)
Therefore we have $P_N = \dim V_N$, so the monomials in $p^{(n)}_1, \dotsc, p^{(n)}_n$ of degree $\leq N$ are a $k$-basis of $V_N$.
\end{proof}


It is important to notice that the theorem does not hold for $k = \Z$! This is easy to see: In $\Q[X_1, X_2]^{S_2}$ we have
\[
    e^{(2)}_2
  = \frac{1}{2} \left( p^{(2)}_1 \right)^2 - \frac{1}{2} p^{(2)}_2 \,.
\]
If $\Z[X_1, X_2]^{S_2}$ was generated by $p^{(n)}_1, p^{(n)}_2$ a polynomial $F \in \Z[X_1, X_2]$ with $e^{(2)}_2 = F\left( e^{(2)}_1, e^{(2)}_2 \right)$ would exist.
But this would contradict the algebraically independency of $p^{(2)}_1, p^{(2)}_2$ in $\Q[X_1, X_2]^{S_2}$ as $F(X_1, X_2) \neq \frac{1}{2} X_1^2 - \frac{1}{2} X_2$.


Using the same argumentation we find that for a symmetric polynomial $f \in \Q[X_1, \dotsc, X_n]^{S_n}$ with integer coefficients and $F,G \in \Q[X_1, \dotsc, X_n]^{S_n}$ with
\[
    f
  = F\left( e^{(n)}_1, \dotsc, e^{(n)}_n \right)
  = G\left( h^{(n)}_1, \dotsc, h^{(n)}_n \right)
\]
both $F$ and $G$ must have integer coefficients.


\begin{expl}
  Another example of formal power series are Hilbert series.
  Given a graded $k$-algebra $A = \bigoplus_{d \geq 0} A_d$ ($A_d = 0$ for $d < 0$) with $\dim_k A_d < \infty$ for all $d$ the corresponding Hilbert series is defined as
  \[
              P_A(t)
    \coloneqq \sum_{d \geq 0} \left( \dim_k A_d \right) t^d
    \in       k\dblbrack{t}.
  \]
  If $\dim_k A < \infty$ we have $P_A(t) \in k[t] \subseteq k\dblbrack{t}$.
  
  If $A = \bigoplus_{d \geq 0} A_d$ and $B = \bigoplus_{d \geq 0} B_d$ are graded $k$-algebras then $A \otimes_k B$ is a $k$-algebra via
  \[
      (a_1 \otimes b_1) (a_2 \otimes b_2)
    = (a_1 a_2) \otimes (b_1 b_2)
  \]
  and a graded $k$-algebra $A \otimes B = \bigoplus_{d \geq 0} (A \otimes B)_d$ by setting
  \[
      (A \otimes B)_d
    = \bigoplus_{i=0}^d (A_i \otimes B_{d-i}) \,.
  \]
  We than have
  \[
      \dim_k (A \otimes B)_d
    = \sum_{i=0}^d \dim_k (A_i \otimes B_{d-i})
    = \sum_{i=0}^d (\dim_k A_i) (\dim_k B_{d-i})
  \]
  for all $d \geq 0$ and thus
  \[
      P_{A \otimes B}(t)
    = P_A(t) P_B(t) \,.
  \]
\end{expl}



\begin{definition}
  $\lambda = (\lambda_1, \dotsc, \lambda_s) \in \N^s$ is \emph{a partition of $|\lambda| \coloneqq \sum_{i=1}^s \lambda_i$} if
  \[
          \lambda_1
    \geq  \lambda_2
    \geq  \dotsb
    \geq  \lambda_s
    \geq  0 \,.
  \]
  The $\lambda_i$ are the \emph{parts of $\lambda$} and $l(\lambda) \coloneqq s$ is the \emph{length of $\lambda$}.
  
  An \emph{infinite partition} is a sequence $\lambda_1, \lambda_2, \dotsc \in \N$ with $\lambda_i = 0$ for all $i \geq s$ for some $s$ such that $(\lambda_1, \dotsc, \lambda_s)$ is a partition.
  For a partition $(\lambda_1, \dotsc, \lambda_s) \in \N^s$ the \emph{infinite partition associated to $\lambda$} is defined as
  \[
              \hat{\lambda}_i
    \coloneqq \begin{cases}
                \lambda_i & \text{for } 1 \leq i \leq l(\lambda) \,,  \\
                        0 & \text{otherwise} \,.
              \end{cases}
  \]
  
  The \emph{transposed partition of a partition $\lambda$} is defined as
  \[
              \lambda'_i
    \coloneqq |\{j \mid \lambda_j \geq i\}| \,.
  \]
\end{definition}


Partitions are often displayed in terms of \emph{Young diagrams}.
The Young diagram corresponding to a partition $\lambda$ is an array of boxes, left adjusted, such that the $i$-th row consists of $\lambda_i$ boxes.


\begin{expl}
  $\lambda = (4,2,2)$ is a partition of $8$ and $\lambda' = (3,3,1,1)$ is the transposed partion.
  The Young diagram corresponding to $\lambda'$ is the `transposition' of the Young diagram corresponding to $\lambda$.
  (See figure \ref{figure: Young diagram example}.)
  \begin{figure}
    \centering
    \ydiagram{4,2,2}
    \qquad
    \ydiagram{3,3,1,1}
    \caption{The Young diagrams corresponding to $\lambda$ (left) and to $\lambda'$ (right).}
    \label{figure: Young diagram example}
  \end{figure}
\end{expl}


\begin{definition}
  For $n \in \N$ we write
  \[
              \Par(n)
    \coloneqq \{\text{partitions of $n$}\}
  \]
  and we set
  \[
              \Par
    \coloneqq \bigcup_{n \in \N} \Par(n) \,.
  \]
\end{definition}
  

\begin{definition}
  Let $\lambda$ and $\mu$ be partitions.
  We say that $\lambda \geq \mu$ if $|\lambda| = |\mu|$ and $\sum_{i=1}^r \hat{\lambda}_i \geq \sum_{i=1}^r \hat{\mu}_i$ for all $r$.
\end{definition}


\begin{expls}
  The following is a simple example of partitions of $6$.
  \[
      \ydiagram{6}
    > \ydiagram{4,2}
    > \ydiagram{3,3}
    > \ydiagram{3,2,1}
    > \ydiagram{1,1,1,1,1,1}
  \]
  The partitions
  \[
    \ydiagram{2,2}
    \quad \text{and} \quad
    \ydiagram{1,1}
  \]
  are not comparable (because $4 \neq 2$ in $\N$). The partitions
  \[
    \ydiagram{4,2,1,1,1}
    \quad \text{and} \quad
    \ydiagram{3,3,2,1}
  \]
  are also not comparable.
  (Because $4+2+1 = 7 < 8 = 3+3+2$ in $\N$.)
\end{expls}
%TODO: Adding a better explanation of this partial order. Bessere ErklÃ¤rung der partiellen Ordnung hinzufÃ¼gen.

\begin{lemma}
  $\geq$ defines a partial ordering on $\Par$.
\end{lemma}
\begin{proof}
  It is clear that $\geq$ is reflexive.
  
  If $\lambda$ and $\mu$ are partitions with $\lambda \geq \mu$ and $\lambda \leq \mu$ then $\lambda = \mu$:
  Because $\lambda \geq \mu$ we have $\lambda_1 \geq \mu_1$ and because $\lambda \leq \mu$ we have $\lambda_1 \leq \mu_1$.
  Thus we have $\lambda_1 = \mu_1$.
  In the same way we find that $\lambda_1 + \lambda_2 = \mu_1 + \mu_2$, and from $\lambda_1 = \mu_1$ we get that $\lambda_2 = \mu_2$.
  Because $|\lambda| = |\mu|$ we find inductively that $l(\lambda) = l(\mu)$ and $\lambda_i = \mu_i$ for all $1 \leq i \leq l(\lambda)$.
  
  If $\lambda, \mu$ and $\sigma$ are partitions with $\lambda \geq \mu$ and $\mu \geq \sigma$ then $\lambda \geq \sigma$:
  Because $\lambda \geq \mu$ we have $|\lambda| = |\mu|$ and because $\mu \geq \sigma$ we have $|\mu| = |\sigma|$.
  Thus we have $|\lambda| = |\sigma|$.
  For all $r \geq 1$ we have
  \[
          \sum_{i=1}^r \lambda_i
    \geq  \sum_{i=1}^r \mu_i
    \text{ and }
          \sum_{i=1}^r \mu_i
    \geq  \sum_{i=1}^r \sigma_i
  \]
  because $\lambda \geq \mu$ and $\mu \geq \sigma$, and therefore
  \[
          \sum_{i=1}^r \lambda_i
    \geq  \sum_{i=1}^r \sigma_i \,.
    \qedhere
  \]
\end{proof}


\begin{definition}
  Let $\lambda = (\lambda_1, \dotsc, \lambda_r)$ be a partition. We define the \emph{elementary symmetric polynomial}
  \[
              e^{(n)}_\lambda
    \coloneqq e^{(n)}_{\lambda_1} \dotsm e^{(n)}_{\lambda_r} \,,
  \]
  the \emph{complete symmetric polynomial}
  \[
              h^{(n)}_\lambda
    \coloneqq h^{(n)}_{\lambda_1} \dotsm h^{(n)}_{\lambda_r} \,,
  \]
  the \emph{power symmetric polynomial}
  \[
              p^{(n)}_\lambda
    \coloneqq p^{(n)}_{\lambda_1} \dotsm p^{(n)}_{\lambda_r}
  \]
  and the \emph{monomial symmetric polynomial}
  \[
              m_\lambda
    \coloneqq   X_1^{\lambda_1} \dotsm X_r^{\lambda_r}
              + \text{ all distinct permutations of this monomial} \,.
  \]
\end{definition}


One can also define $m_\lambda$ in a formal way:
Instead of adding up all distinct permutations of the monomial $X_1^{\lambda_1} \dotsm X_r^{\lambda_r}$ we can also take all distinct permutations of the tupel $\lambda$ and add up the corresponding monomials.
To formalize this we let $S_r$ act on $\N^r$ by permuting the entries, i.e.\
\[
    \pi.(a_1, \dotsc, a_r)
  = \left( a_{\pi^{-1}(1)}, \dotsc, a_{\pi^{-1}(r)} \right)
\]
for all $\pi \in S_r$ and $(a_1, \dotsc, a_r) \in \N^r$.
The set of all distinct permutations of $\lambda$ is precisely the orbit of $\lambda$ under this action.
As we know from basic group theory we have an isomorphism of $G$-sets
\[
          S_r / U
  \to     S_r \lambda,
  \quad   [\pi]
  \mapsto \pi.\lambda
\]
where $U$ is the stabilizer group of $\lambda$ and $S_r \lambda$ is the orbit of $\lambda$.
(Note that $U$ is not necessarily a normal subgroup in $S_r$ and $S_r/U$ is only the set of left cosets.)
Thus we can write
\[
    m_\lambda
  = \sum_{[\pi] \in S_r/U} X_1^{\lambda_{\pi^{-1}(1)}} \dotsm X_r^{\lambda_{\pi^{-1}(r)}}
  = \sum_{[\pi] \in S_r/U} X_{\pi(1)}^{\lambda_1} \dotsm X_{\pi(r)}^{\lambda_r} \,.
\]
Also notice that
\[
        U
  \cong S_{\nu_0} \times \dotsb \times S_{\nu_m}
\]
where
\[
            \nu_n
  \coloneqq \left|
              \left\{
                1 \leq i \leq r
              \mid
                  \lambda_i
                = n
              \right\}
            \right|
\]
and $m \coloneqq \max_{i=1,\dotsc,r} \lambda_i$.


\begin{expl}[]
  Let $k$ be a field with $\kchar k \neq 2$.
  Let $\lambda = (\lambda_1, \dotsc, \lambda_n) \in \Par$ be a partition.
  The \emph{Schur polynomial corresponding to $\lambda$} is the symmetric polynomial $s_\lambda \in k[X_1, \dotsc, X_n]^{S_n}$ of homogenous degree $|\lambda|$ defined as
  \[
              s_\lambda
    \coloneqq \frac
              {
                \sum_{\sigma \in S_n} \sgn(\sigma)          X_{\sigma(1)}^{\lambda_1}
                                                            X_{\sigma(2)}^{\lambda_2 + 1}
                                                    \dotsm  X_{\sigma(n)}^{\lambda_n + n-1}
               }{
                \prod_{1 \leq i < j \leq n} (X_i - X_j)
               }
  \]
  (In the lecture the same statements were made for the ring of integers $\Z$ and arbitrary fields, but the following argumentation does not work in these cases.)
  
  To show that $s_\lambda$ is well-defined we first notice the numerator
  \[
              N
    \coloneqq \sum_{\sigma \in S_n} \sgn(\sigma)          X_{\sigma(1)}^{\lambda_1}
                                                          X_{\sigma(2)}^{\lambda_2 + 1}
                                                  \dotsm  X_{\sigma(n)}^{\lambda_n + n-1}
  \]
  and the denumerator
  \[
              D
    \coloneqq \prod_{1 \leq i < j \leq n} (X_i - X_j)
  \]
  are alternating polynomials, i.e.\ $\sigma.N = \sgn(\sigma) N$ and $\sigma.D = \sgn(\sigma) D$ for every $\sigma \in S_n$, because
  \begin{gather*}
    N = \det
    \begin{pmatrix}
      X_1^{\lambda_1}       & X_2^{\lambda_1}       & \cdots & X_n^{\lambda_1}       \\
      X_1^{\lambda_2 + 1}   & X_2^{\lambda_2 + 1}   & \cdots & X_n^{\lambda_2 + 1}   \\
      \vdots                & \vdots                & \ddots & \vdots                \\
      X_1^{\lambda_n + n-1} & X_2^{\lambda_n + n-1} & \cdots & X_n^{\lambda_n + n-1}
    \end{pmatrix}
  \shortintertext{and}
    D = \det
    \begin{pmatrix}
      1      & X_1    & X_1^2  & \cdots & X_1^{n-1} \\
      1      & X_2    & X_2^2  & \cdots & X_2^{n-1} \\
      \vdots & \vdots & \vdots & \ddots & \vdots    \\
      1      & X_n    & X_n^2  & \cdots & X_n^{n-1}
    \end{pmatrix}.
  \end{gather*}
  That $D$ divides $N$ follows from the following claim:
  \begin{claim}
    Let $f \in k[X_1, \dotsc, X_n]$ be an alternating polynomial and
    \[
                V
      \coloneqq \prod_{1 \leq i < j \leq n} (X_i - X_j)
      \in       k[X_1, \dotsc, X_n] \,.
    \]
    Then $V$ divides $f$.
  \end{claim}
  \begin{proof}
    Since the polynomials $X_i - X_j$ with $1 \leq i < j \leq n$ are pairwise non-equivalent primes it sufficies to show that $X_i-X_j$ divides $f$ for all $1 \leq i < j \leq n$.
    Because $f$ is alternating it is enough to show that $X_1 - X_2$ divides $f$.
    
    For $R \coloneqq k[X_3, \dotsc, X_n]$, $u = X_1 + X_2$ and $x = X_1 - X_2$ we have
    \[
        k[X_1, \dotsc, X_n]
      = R[X_1, X_2]
      = R[u,v] \,,
    \]
    so we can write $f = \sum_{i \in \N} f_i v^i$ with $f_i \in R[u]$ for every $i \in \N$.
    Because $f$ is alternating we have
    \begin{align*}
           \sum_{i \in \N} f_i v^i
      &=   f(X_1, X_2, \dotsc, X_n)
       =  -f(X_2, X_1, \dotsc, X_n) \\
      &=  -\sum_{i \in \N} (-1)^i f_i v^i
       =   \sum_{i \in \N} (-1)^{i+1} f_i v^i \,.
    \end{align*}
    So $f_i = 0$ if $i$ is even.
    Therefore $v$ divides $f$ .
  \end{proof}
  Since $D$ and $N$ are both alternating it is also clear that $s_\lambda = N/D$ is symmetric.
  To see that $s_\lambda$ is homogeneous of degree $|\lambda|$ notice that $N$ is homogeneous of degree
  \[
      \lambda_1 + (\lambda_2 + 1) + \dotsb + (\lambda_n + n-1)
    = |\lambda| + \binom{n}{2}
  \]
  and that $D$ is homogeneous of degree $\binom{n}{2}$.
  \begin{claim}
    Let $f, g \in k[X_1, \dotsc, X_n]$ be polynomials such that $f$ is homogenous of degree $d_1$ and $g$ homogeneous of degree $d_2$.
    If $g$ divides $f$ then $f/g$ is homogenous of degree $d_1 - d_2$.
  \end{claim}
  \begin{proof}
    We can write $f/g = \sum_{d \in \N} h_d$ where $h_d \in k[X_1, \dotsc, X_n]$ is homogenous of degree $d$.
    Then $f = (f/g)g = \sum_{d \in \N} h_d g$ where $h_d g$ is homogeneous of degree $d + d_2$.
    Because $f$ is homogenous of degree $d_1$ we find that $h_d = 0$ for $d \neq d_1 - d_2$.
    Thus $f/g = h_d$ is homogeneous of degree $d_1 - d_2$.
  \end{proof}
\end{expl}


\begin{lemma}
  The set of monomial symmetric polynomials
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par,
      l(\lambda) = n
    \}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$ (as a $k$-vector space). More precisely
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par(d),
      l(\lambda) = n
    \}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}_d$.
\end{lemma}
\begin{proof}
  Is is clear that
  \[
            \vspan_k \{
                        m_\lambda
                      \mid
                        \lambda \in \Par,
                        l(\lambda) = n,
                        |\lambda| = d
                      \}
  \subseteq k[X_1, \dotsc, X_n]^{S_n}_d \,.
  \]
  On the other side let $f \in k[X_1, \dotsc, X_n]^{S_n}_d$. By induction on the number of monomials of which $f$ consists we show that
  \[
        f
    \in \vspan_k  \{
                    m_\lambda
                  \mid
                    \lambda \in \Par,
                    l(\lambda) = n,
                    |\lambda| = d
                  \} \,.
  \]
  For $f = 0$ this is clear.
  Suppose that $f \neq 0$ and that the statement is true for every polynomial in $k[X_1, \dotsc, X_n]^{S_n}_d$ which consists of fewer monomials than $f$.
  Because $\{X^\alpha \mid \alpha \in \N^n, |\alpha| = d \}$ is a $k$-basis of $k[X_1, \dotsc, X_n]_d$ we can write
  \[
      f
    = \sum_{\substack{\alpha \in \N^n \\ |\alpha| = d}} c_\alpha X^\alpha \,.
  \]
  with unique $c_\alpha \in k$ such that $c_\alpha \neq 0$ for only finitely many $\alpha$.
  Because $f$ is symmetric we find that
  \[
      c_\alpha
    = c_{\pi.\alpha}
    \text{ for all }
    \alpha \in \N^n,
    \pi \in S_n
  \]
  (where the action of $S_n$ on $\N^n$ is defined as above).
  Let $X^\beta$ be a monomial of $f$.
  Because $f \in k[X_1, \dotsc, X_n]^{S_n}_d$ we have $X^\beta \in k[X_1, \dotsc, X_n]_d$ and thus $c_\beta m_\beta \in k[X_1, \dotsc, X_n]^{S_n}_d$.
  Because $c_\beta \neq 0$ and $c_\beta = c_{\pi.\beta}$ for every $\pi \in S_n$ we find that $f - c_{\beta} m_\beta$ consists of fewer monomials than $f$.
  Because $f-c_{\beta} m_\beta$ is symmetric we find by induction hypothesis that
  \[
        f - c_{\beta} m_\beta
    \in \vspan_k  \{
                    m_\lambda
                  \mid
                    \lambda \in \Par,
                    l(\lambda) = n,
                    |\lambda| = d
                  \} \,.
  \]
  The statement for $f$ follows directly.
  
  To show that
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par,
      l(\lambda) = n
    \}
  \]
  is linear independent we notice that for $\lambda, \mu \in \Par$ with $\lambda \neq \mu$ the polynomials $m_\lambda$ and $m_\mu$ have no monomials in common. Because
  \[
    \{
      X^\alpha
    \mid
      \alpha \in \N^n
    \}
  \]
  is linear independent it then follows that
  \[
    \{
      m_\lambda
    \mid
      \lambda \in \Par,
      l(\lambda) = n
    \}
  \]
  is linear independent.
\end{proof}


\begin{definition}
  Let $\lambda, \mu \in \Par$.
  We define the partition $\lambda+\mu$ of length $l(\lambda+\mu) = \max\{ l(\lambda), l(\mu) \}$ as
  \[
      (\lambda+\mu)_i
    = \begin{cases}
        \lambda_i + \mu_i & \text{if } i \leq l(\lambda),l(\mu)       \,, \\
        \lambda_i         & \text{if } i \leq l(\lambda), i > l(\mu)  \,, \\
        \mu_i             & \text{if } i \leq l(\mu), i > l(\lambda)  \,.
      \end{cases}
  \]
  So $\lambda+\mu$ is the partition of minimal length with
  \[
      \hat{\lambda}_i + \hat{\mu}_i
    = \widehat{\lambda + \mu}_i
    \text{ for all }
    i \geq 0 \,.
  \]
\end{definition}


\begin{expl}
  For $\lambda = (4,4,2,2)$ and $\mu = (3,2,1)$ we have $\lambda + \mu = (7,6,3,2)$.
  The addition of two partitions can also easily be visualized with Young diagrams, see figure \ref{figure: addition partition young diagrams}.
  \begin{figure}\centering
    \[
        \ydiagram[*(gray)]{4,4,2,2} + \ydiagram[*(light-gray)]{3,2,1}
      = \ydiagram[*(light-gray)] {4+3,4+2,2+1} *[*(gray)]{7,6,3,2}
    \]
    \caption{Addition of partitions in term of Young diagrams.}
    \label{figure: addition partition young diagrams}
  \end{figure}
\end{expl}


\begin{lemma}
  Let $\lambda, \mu \in \Par$.
  Then
  \[
      m_{\lambda} m_{\mu}
    =   m_{\lambda + \mu}
      + \sum_{\nu < \lambda + \mu} a^\nu_{\lambda,\mu} m_\nu
  \]
  for suitable $a^\nu_{\lambda,\mu} \in k$.
  (This also holds for $k = \Z$.)
\end{lemma}
\begin{proof}
  This is an exercise on the 4th exercise sheet.
\end{proof}


We know from the Fundamental Theorem that $e^{(n)}_1, \dotsc, e^{(n)}_n$ generate the $k$-algebra of symmetric polynomials $k[X_1, \dotsc, X_n]^{S_n}$ and are algebraically independent. This is equivalent to saying that the monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$, i.e.\
\[
    \left( e^{(n)} \right)^\alpha
  = \left( e^{(n)}_1 \right)^{\alpha_1} \dotsm \left( e^{(n)}_n \right)^{\alpha_n}
  \text{ with }
  \alpha = (\alpha_1, \dotsc, \alpha_n)
\]
form a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$.
We can also describe these polynomials in term of partitions.


\begin{proposition}
  The monomials in $e^{(n)}_1, \dotsc, e^{(n)}_n$ are precisely the polynomials $e^{(n)}_\lambda$ with $\lambda \in \Par$ such that $\lambda_1 \leq n$ and $\lambda_{l(\lambda)} \neq 0$.
\end{proposition}
\begin{proof}
  On the one side the monomial $e_1^{\alpha_1} \dotsm e_n^{\alpha_n}$ is the same as $e_\lambda$ for the partition
  \[
      \lambda
    = (
        \underbrace{n, \dotsc, n}_{\alpha_n},
        \underbrace{n-1, \dotsc, n-1}_{\alpha_{n-1}},
        \dotsc,
        \underbrace{1, \dotsc, 1}_{\alpha_1}
      ).
  \]
  On the other side the polynomial $e_\lambda$ equals the monomial $e_1^{\nu_1(\lambda)} \dotsm e_n^{\nu_n(\lambda)}$ for the exponents
  \[
      \nu_j(\lambda)
    = |
        \{
          1 \leq i \leq l(\lambda)
        \mid
          \lambda_i = j
        \}
      |.
  \]
  
  Notice that for $\lambda \neq \mu$ there exists some $1 \leq j \leq n$ with $\nu_j(\lambda) \neq \nu_j(\mu)$ (here we use that $1 \leq \lambda_i \leq n$ for all $1 \leq i \leq l(\lambda)$ and the same for $\mu$) and therefore
  \[
          e_1^{\nu_1(\lambda)} \dotsm e_n^{\nu_n(\lambda)}
    \neq  e_1^{\nu_1(\mu)} \dotsm e_n^{\nu_n(\mu)}
  \]
  because the monomials in $e_1, \dotsc, e_n$ are linearly independent.
\end{proof}


\begin{corollary}
  The set
  \[
              B
    \coloneqq \left\{
                e^{(n)}_\lambda
              \,\middle|\,
                \lambda \in \Par,
                \lambda_1 \leq n,
                \lambda_{l(\lambda)} \neq 0
              \right\}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}$ and the $e^{(n)}_\lambda$ are pairwise different.
  More precisely
  \[
              B_d
    \coloneqq \left\{
                e^{(n)}_\lambda
              \,\middle|\,
                \lambda \in \Par,
                \lambda_1 \leq n,
                \lambda_{l(\lambda)} \neq 0,
                |\lambda| = d
              \right\}
  \]
  forms a $k$-basis of $k[X_1, \dotsc, X_n]^{S_n}_d$ for all $d \geq 0$.
\end{corollary}


\begin{rem}
  Since $h^{(n)}_1, \dotsc, h^{(n)}_n$ generate $k[X_1, \dotsc, X_n]^{S_n}$ as a $k$-algebra and are algebraically independent we can show the same statements for the polynomials $h^{(n)}_\lambda$ with $\lambda \in \Par$ such that $\lambda_1 \leq n$ and $\lambda_{l(\lambda)} \neq 0$.
  
  In the case that $k$ is a field with $\kchar k = 0$ or $\kchar k > n$ we can show the same for the polynomials $p^{(n)}_\lambda$ with $\lambda \in \Par$ such that $\lambda_1 \leq n$ and $\lambda_{l(\lambda)} \neq 0$.
\end{rem}


% TODO: Adding the basis of schur polynomials.





\section{Polynomial Maps}


In this section $k$ is an infinite field.
Until further notice we also fix a finite-dimensional $k$-vector space $V$ with $k$-basis $v_1, \dotsc, v_n$.


\begin{definition}
  By $\mc{P}_k(V)$ we denote set of \emph{polynomial functions} $V \to k$, i.e.\ $f \in \mc{P}_k(V)$ if and only if $f \colon V \to k$ and there is some $p \in k[X_1, \dotsc, X_n]$ such that
  \[
      f\left( \sum_{i=1}^n \lambda_i v_i \right)
    = p(\lambda_1, \dotsc, \lambda_n)
  \]
  for all $\lambda_1, \dotsc, \lambda_n \in k$.
  If the underlying field is clear we also write $\mc{P}(V)$ instead of $\mc{P}_k(V)$.
\end{definition}


This definition does not depend on the chosen basis.
If $(w_1, \dotsc, w_n)$ is another basis of $V$ with $w_i = \sum_{j=1}^n a_{ij} v_j$ for $i = 1, \dotsc, n$ then
\begin{align*}
      f\left( \sum_{i=1}^n \lambda_i w_i \right)
  &=  f\left( \sum_{i,j=1}^n \lambda_i a_{ij} v_j \right)
   =  p\left( \sum_{i=1}^n \lambda_i a_{i1}, \dotsc, \sum_{i=1}^n \lambda_{i} a_{in} \right)  \\
  &=  p'(\lambda_1, \dotsc, \lambda_n)
\end{align*}
for some $p' \in k[X_1, \dotsc, X_n]$.
So if $f \colon V \to k$ is a polynomial in $(v_1, \dotsc, v_n)$ then also in $(w_1, \dotsc, w_n)$.

This has the effect that the restriction of a polynomial function to a vector subspace is again a polynomial function.

\begin{lemma}
  Let $V$ be a finite-dimensional $k$-vector space and $U \subseteq V$ a vector subspace.
  Then for every polynomial function $f \in \mc{P}(V)$ we have $f_{|U} \in \mc{P}(U)$.
\end{lemma}
\begin{proof}
  Let $v_1, \dotsc, v_n$, $v_{n+1}, \dotsc, v_m$ be a $k$-basis of $V$ such that $v_1, \dotsc, v_n$ is a $k$-basis of $U$.
  Because $f \in \mc{P}(V)$ there exist some $p \in k[X_1, \dotsc, X_m]$ with
  \[
      f\left( \sum_{i=1}^m \lambda_i v_i \right)
    = p(\lambda_1, \dotsc, \lambda_m)
  \]
  for all $\lambda_1, \dotsc, \lambda_m \in k$. For
  \[
              \bar{p}
    \coloneqq p(X_1, \dotsc, X_n, 0, \dotsc, 0)
    \in       k[X_1, \dotsc, X_n]
  \]
  we thus have
  \[
      f\left( \sum_{i=1}^n \lambda_i v_i \right)
    = \bar{p}(\lambda_1, \dotsc, \lambda_n)
  \]
  for all $\lambda_1, \dotsc, \lambda_n \in k$.
  Since $v_1, \dotsc, v_n$ is a $k$-basis of $U$ this is equivalent to $f_{|U} \in \mc{P}(U)$.
\end{proof}


\begin{rem}
  If a group $G$ acts linearly on $V$ then it acts linearly on $\mc{P}(V)$ by $(g.f)(v) = f\left(g^{-1}.v\right)$.
\end{rem}


\begin{lemma}
  There is an isomorphism of $k$-algebras
  \[
          \mc{P}(V)
    \cong k[X_1, \dotsc, X_n]
  \]
  where $n = \dim V$.
\end{lemma}
\begin{proof}
  For $1 \leq j \leq n$ define the $j$-th coordinate function (with respect to the chosen basis) as
  \[
            \varphi_j
    \colon  V \to k,
    \quad   \sum_{i=1}^n \lambda_i v_i
    \mapsto \lambda_j \,.
  \]
  By the universal property of the polynomial ring the assignment $X_j \to \varphi_j$ extends to a ring homomorphism
  \[
            \Phi
    \colon  k[X_1, \dotsc, X_n] \to \mc{P}(V),
    \quad   p
    \mapsto \Phi(p)
  \]
  where
  \[
      \Phi(p)\left( \sum_{i=1}^n \lambda_i v_i \right)
    = p(\lambda_1, \dotsc, \lambda_n) \,.
  \]
  Is it clear that $\Phi$ is surjective.
  It is left as an exercise to the reader to check that $\Phi$ is injective.
\end{proof}


\begin{lemma}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      Assume $p \in k[X_1, \dotsc, X_n]$ with $p(\lambda_1, \dotsc, \lambda_n) = 0$ for all $(\lambda_1,\dotsc,\lambda_n) \in k^n$.
      Then $p = 0$.
    \item
      The polynomial functions $\varphi_1, \dotsc, \varphi_n \in \mc{P}(V)$ are algebraically independent over $k$, i.e.\ if $f(\varphi_1, \dotsc, \varphi_n) = 0$ for some polynomial $f$ (over $k$) then $f = 0$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      We show this by induction over $n$.
      
      ($n = 1$)
      Let $p \in k[X_1]$ with $p(\lambda_1) = 0$ for all $\lambda_1 \in k$.
      Since $k$ is infinite $p$ has infinitely many zeroes.
      Therefore $p = 0$.
      
      ($n \geq 2$)
      Assume the claim holds for $n-1$ and $1$.
      Consider $p \in k[X_1, \dotsc, X_n]$ with $p(\lambda_1, \dotsc, \lambda_n) = 0$ for all $(\lambda_1, \dotsc, \lambda_n) \in k^n$.
      We write $p$ as
      \[
          p
        = \sum_{i \in \N} f_i(X_1, \dotsc, X_{n-1}) X_n^i
      \]
      with $f_i \in k[X_1, \dotsc, X_{n-1}]$ for all $i \in \N$ and $f_i = 0$ for all but finitely many $i \in \N$.
      Let $(\lambda_1, \dotsc, \lambda_{n-1}) \in k^{n-1}$ be fixed but arbitrary.
      For all $\lambda_n \in k$ we have
      \[
          0
        = p(\lambda_1, \dotsc, \lambda_n)
        = \sum_{i \in \N} f_i(\lambda_1, \dotsc, \lambda_{n-1}) \lambda_n^i
      \]
      By induction hypothesis we find that $f_i(\lambda_1, \dotsc, \lambda_{n-1}) = 0$ for all $i \in \N$.
      Because $(\lambda_1, \dotsc, \lambda_{n-1})$ is fixed but arbitrary we can use the induction hypothesis to get that $f_i = 0$ for all $i \in \N$.
      So $p = 0$.
    \item
      Assume $f(\varphi_1, \dotsc, \varphi_n) = 0$. Then
      \[
          0
        = f(\varphi_1, \dotsc, \varphi_n)\left(\sum_{i=1}^n \lambda_i v_i\right)
        = f(\lambda_1, \dotsc, \lambda_n)
      \]
      for all $(\lambda_1, \dotsc, \lambda_n) \in k^n$.
      Therefore $f = 0$ by part a).
    \qedhere
  \end{enumerate}
\end{proof}

\begin{warn}
  The assumption that $k$ is infinite is necessary.
  If, for example, $p = X^2 + X \in \F_2[X]$, then $p(0) = p(1) = 0$, so $p(\lambda) = 0$ for all $\lambda \in \F_2$, but $p \neq 0$.
\end{warn}


\begin{corollary}
  The map $\Phi \colon k[X_1, \dotsc, X_n] \to \mc{P}(V), X_j \mapsto \varphi_j$ is injective.
\end{corollary}


Together with the exercise sheet we find that
\[
          \Phi
  \colon  k[X_1, \dotsc, X_n]
  \to     \mc{P}(V),
  \quad   X_j
  \mapsto \varphi_j
\]
is an isomorphism of $k$-algebras.


\begin{rem}
  For $k$ infinite we have an isomorphism of representations
  \[
            \Phi
    \colon  k[X_1, \dotsc, X_n]
    \to     \mc{P}(k^n),
    \quad   X_i
    \mapsto \varphi_i \,,
  \]
  where $S_n$ acts on $k[X_1, \dotsc, X_n]$ as usual by permuting the $X_i$ and on $\mc{P}(k^n)$ via
  \[
      (\sigma.f)(v)
    = f\left( \sigma^{-1}.v \right)
    \text{ for all }
    \sigma \in S_n,
    f \in \mc{P}(V),
    v \in k^n \,.
  \]
  We know that $\Phi$ is an isomorphism of $k$-vector spaces.
  It is $S_n$-equivariant, since for all $\sigma \in G, p = X_1^{\alpha_1} \dotsm X_n^{\alpha_n} \in k[X_1, \dotsc, X_n]$ and $v = (\lambda_1, \dotsc, \lambda_n) \in k^n$
  \begin{align*}
       \Phi(g.p)(v)
    &= \Phi\left( X_{g(1)}^{\alpha_1} \dotsm X_{g(n)}^{\alpha_n} \right)(v)
     = \lambda_{g(1)}^{\alpha_1} \dotsm \lambda_{g(n)}^{\alpha_n} \\
    &= \Phi(p)( \lambda_{g(1)}, \dotsc, \lambda_{g(n)} )
     = \Phi(p)\left( g^{-1}.(\lambda_1, \dotsc, \lambda_n) \right) \\
    &= (g.\Phi(p))(\lambda_1, \dotsc, \lambda_n) \,.
  \end{align*}
\end{rem}


\begin{definition}
  $f \in \mc{P}(V)$ is \emph{homogeneous of degree $d \in \Z$} if $f(\lambda y) = \lambda^d f(y)$ for all $\lambda \in k, y \in V$.
  By definition the zero polynomial $f=0$ is homogeneous of degree $d$ for any $d \in \Z$.
  For $d \in \Z$ we set
  \[
              \mc{P}(V)_d
    \coloneqq \{
                f \in \mc{P}(V)
              \mid
                f \text{ is homogeneous of degree } d
              \} \,.
  \]
\end{definition}


\begin{lemma}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      $\mc{P}(V)_d$ is a $k$-vector space for all $d \in \Z$ (via pointwise addition and scalar multiplication).
    \item
      If $f \in \mc{P}(V)_i$ and $g \in \mc{P}(V)_j$ then $fg \in \mc{P}(V)_{i+j}$, where the multiplication is given by pointwise multiplication.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item For $f_1, f_2 \in \mc{P}(V)_d$ we have
      \begin{align*}
            (f_1+f_2)(\lambda v)
        &=  f_1(\lambda v) + f_2(\lambda v)
         =  \lambda^d f_1(v) + \lambda^d f_2(v) \\
        &=  \lambda^d (f_1(v) + f_2(v))
         =  \lambda^d (f_1 + f_2)(v)
      \end{align*}
      for all $\lambda \in k$, $v \in V$, so $f_1 + f_2 \in \mc{P}(V)_d$. If $f \in \mc{P}(V)$ and $\mu \in k$ then
      \[
          (\mu f)(\lambda v)
        = \mu f(\lambda v)
        = \lambda^d \mu f(v)
        = \lambda^d (\mu f)(v) \,,
      \]
      so $\mu f \in \mc{P}(V)_d$.
    \item
      For all $\lambda \in k$ we have for all $v \in V$
      \[
          fg(\lambda v)
        = f(\lambda v) g(\lambda v)
        = \left( \lambda^i f(v) \right)\left( \lambda^j g(v) \right)
        = \lambda^{i+j} f(v) g(v)
        = \lambda^{i+j} (fg)(v) \,,
      \]
      and therefore $fg \in \mc{P}(V)_{i+j}$.
    \qedhere
  \end{enumerate}
\end{proof}

This shows that $\mc{P}(V)$ is a graded algebra via
\[
      \mc{P}(V)
    = \bigoplus_{d \in \Z} \mc{P}(V)_d \,.
\]
This grading can be seen as inherted from $k[X_1, \dotsc, X_n]$ via the isomorphism $\Phi$. Since
\[
    (\lambda X_1)^{\alpha_1} \dotsm (\lambda X_n)^{\alpha_n}
  = \lambda^{\sum_{i=1}^n \alpha_i} X_1^{\alpha_1} \dotsm X_n^{\alpha_n}
\]
we obtain that a monomial of degree $d$ corresponds to a polynomial function $\Phi(p) \in \mc{P}(V)$ which is homogeneous of degree $d$.


Given a field extension $L/k$ and an $n$-dimensional $k$-vector space $W$ we have an isomorphism of $k$-algebras
\[
        \mc{P}_k(W)
  \cong k[X_1, \dotsc X_n]
\]
and therefore an isomorphism of $L$-algebras
\[
        \mc{P}_k(W)_L
  \cong k[X_1, \dotsc, X_n]_L
  \cong L[X_1, \dotsc, X_n] \,.
\]
Since $\dim_L W_L = \dim_k W = n$ we also have an isomorphism of $L$-algebras
\[
        \mc{P}_L(W_L)
  \cong L[X_1, \dotsc, X_n]
\]
Combining this we have an isomorphism of $L$-algebras $\mc{P}_L(W)_L \cong \mc{P}(W_L)$. Since the isomorphism
\[
        \mc{P}_k(W)
  \cong k[X_1, \dotsc X_n]
\]
depends on choosing a $k$-basis of $W$ and the isomorphism
\[
        \mc{P}_L(W_L)
  \cong L[X_1, \dotsc, X_n]
\]
depends on choosing an $L$-basis of $W_L$ we can not expect this isomorphism $\mc{P}_k(W)_L \cong \mc{P}(W_L)$ to be independent of such choice.
We will come back to this later.


We will know generalize the definition of polynomial functions on maps between arbitrary finite-dimensional $k$-vector spaces.
For this we also stop fixing $V$.


\begin{definition}
  Let $V$ and $W$ be finite dimensional $k$-vector spaces. A map
  \[
            f
    \colon  W
    \to     V
  \]
  is called a \emph{polynomial map} if, given a basis $v_1, \dotsc, v_n$ of $V$, the coordinate functions of $f$ are polynomial, i.e.\ there exist $f_1, \dotsc, f_n \in \mc{P}(W)$ with
  \[
      f(w)
    = \sum_{i=1}^n f_i(w) v_i
  \]
  for all $w \in W$.
  We write
  \[
              \Pol_k(V,W)
    \coloneqq \{
                        f
                \colon  V
                \to     W
              \mid
                f \text{ is a polynomial map}
              \} \,.
  \]
\end{definition}


\begin{rem}
  One can show (as for $\mc{P}_k(W)$) that the definition doesnât depend on the chosen basis of $V$.
\end{rem}


\begin{rem}
  In the case of $V = k$ we have $\Pol_k(W,k) = \mc{P}_k(W)$.
\end{rem}


\begin{expl}
  Let $W$ be a finite dimensional $k$-vector space. Then
  \[
            f
    \colon  W
    \to     W^{\otimes r},
    \quad   w
    \mapsto w \otimes \dotsb \otimes w \,.
  \]
  is a polynomial map.
  To see this choose a basis $w_1, \dotsc, w_n$ of $W$. Then the elements
  \[
      w_{\underline{i}}
    = w_{i_1} \otimes \dotsb \otimes w_{i_r}
    \text{ with }
        \underline{i}
    =   (i_1, \dotsc, i_r)
    \in \{1, \dotsc, n\}^r
  \]
  form a basis of $W^{\otimes r}$.
  For $w \in W$ with $w = \sum_{i=1}^r \lambda_i w_i$ we have
  \[
      f(w)
    = w \otimes \dotsb \otimes w
    =         \left( \sum_{i=1}^r \lambda_i w_i \right)
      \otimes \dotsb
      \otimes \left( \sum_{i=1}^r \lambda_i w_i \right)
    = \sum_{\underline{i}} \lambda_{i_1} \dotsm \lambda_{i_r} w_{\underline{i}} \,.
  \]
  For the polynomials
  \[
      p_{\underline{i}}
    = X_{i_1} \dotsm X_{i_r}
  \]
  and polynomial maps $f_{\underline{i}} \colon W \to k$ with
  \[
      f_{\underline{i}}\left( \sum_{i=1}^r \lambda_i w_i \right)
    = p_{\underline{i}}(\lambda_1, \dotsc, \lambda_r)
  \]
  we thus have
  \[
      f(w)
    = \sum_{\underline{i}} f_{\underline{i}}(w) w_{\underline{i}} \,.
  \]
\end{expl}


\begin{lemma}
  For a finite-dimensional $k$-vector space $V$ the map $\id_V \colon V \to V$ is a polynomial map.
  If $U$ and $W$ are finite-dimensonial vector spaces and $f \colon W \to V$ and $g \colon V \to U$ are polynomial maps then $gf \colon W \to U$ is also a polynomial map.
\end{lemma}
\begin{proof}
  The first statement is clear.
  To show the second let $v_1, \dotsc, v_r$ be a basis of $V$, $w_1, \dotsc, w_s$ be a basis of $W$ and $u_1, \dotsc, u_t$ be a basis of $U$.
  Because $f$ is a polynomial map we can find polynomials $P_1, \dotsc, P_s \in k[X_1, \dotsc, X_r]$ such that
  \[
      f\left( \sum_{i=1}^r \lambda_i v_i \right)
    = \sum_{j=1}^s F_j(\lambda_1, \dotsc, \lambda_r) w_j
  \]
  and because $g$ is a polynomial map we can find polynomials $Q_1, \dotsc, Q_t \in k[X_1, \dotsc, X_s]$ such that
  \[
      g\left( \sum_{j=1}^s \mu_j w_j \right)
    = \sum_{k=1}^t G_k(\mu_1, \dotsc, \mu_s) u_k \,.
  \]
  Combining this we find that
  \begin{align*}
        gf\left( \sum_{i=1}^r \lambda_i v_i \right)
    &=  g\left( \sum_{j=1}^s F_j(\lambda_1, \dotsc, \lambda_r) w_j \right) \\
    &=  \sum_{k=1}^t Q_k(F_1(\lambda_1, \dotsc, \lambda_r), \dotsc, F_s(\lambda_1, \dotsc, \lambda_r)) w_k \\
    &=  \sum_{k=1}^t R_k(\lambda_1, \dotsc, \lambda_r) w_k
  \end{align*}
  for the polynomials
  \[
              R_k
    \coloneqq Q_k(F_1(X_1, \dotsc, X_r), \dotsc, F_s(X_1, \dotsc, X_r))
    \in       k[X_1, \dotsc, X_r] \,.
    \qedhere
  \]
\end{proof}


It is now easy to see that the class of finite-dimensional $k$-vector spaces together with the polynomial maps between them form a category.
We will denote this category by $\cpol{k}$.
So the objects in $\cpol{k}$ are finite-dimensional $k$-vector spaces and $\Hom_{\cpol{k}}(W,V) = \Pol_k(W,V)$ for all finite-dimensional $k$-vector spaces $W$ and $V$.
Also notice that every linear map between finite-dimensional $k$-vector spaces is a polynomial map.
Therefore $\cvect{k}$ is a subcategory of $\cpol{k}$.

From the definition of a polynomial map it also directly follows that for any finite-dimensional $k$-vector spaces $W$ and $V$ the set $\Pol_k(W,V)$ forms a $k$-vector space via pointwise addition and scalar multiplication.
This $k$-vector space can be given the structure of a $\mc{P}(W)$-module.

\begin{proposition}
  Let $W$ and $V$ be finite-dimensional $k$-vector spaces.
  The $k$-vector space $\Pol_k(W,V)$ is a $\mc{P}(W)$-module via
  \[
      (g \cdot f)(w)
    = g(w) f(w)
  \]
  for all $g \in \Pol_k(W,V)$,
  $f \in \mc{P}(W)$,
  $w \in W$.
\end{proposition}
\begin{proof}
  It is clear that $\Maps(W,V)$ becomes a $\Maps(W,k)$-module by defining the multiplication as above.
  Since $\mc{P}(W)$ is a $k$-subalgebra of $\Maps(W,k)$ we find that $\Maps(W,V)$ is a $\mc{P}(W)$-module.
  The proposition claims that $\Pol_k(W,V)$ is a $\mc{P}(W)$-submodule of $\Maps(W,V)$.
  So we only need to check that $\Pol_k(W,V)$ is closed under the multiplication from $\mc{P}(W)$.
  
  Let $v_1, \dotsc, v_n$ be a $k$-basis of $V$.
  For $f \in \Pol_k(W,V)$ there exists $f_1, \dotsc, f_n \in \mc{P}(W)$ such that
  \[
      f(w)
    = \sum_{i=1}^n f_i(w) v_i
    \text{ for all }
    w \in W \,.
  \]
  Since for all $g \in \mc{P}(W)$
  \begin{align*}
        (g \cdot f)(w)
    &=  g(w) \cdot f(w)
     =  g(w) \cdot \sum_{i=1}^n f_i(w) v_i \\
    &=  \sum_{i=1}^n g(w) f_i(w) v_i
     =  \sum_{i=1}^n (g \cdot f_i)(w) v_i
  \end{align*}
  we find that $g \cdot f \in \Pol_k(W,V)$ for all $g \in \mc{P}(W)$.
\end{proof}



\begin{lemma}
  Let $W$ and $V$ be finite-dimensional $k$-vector spaces and $f \colon W \to V$ be a polynomial map. Then
  \[
            f^*
    \colon  \mc{P}(V)
    \to     \mc{P}(W),
    \quad   h
    \mapsto h \circ f
  \]
  is an algebra homomorphism.
\end{lemma}
\begin{proof}
  We already know that $f^*$ is well-defined, i.e.\ that $h \circ f \in \mc{P}(W)$ for all $h \in \mc{P}(V)$.
  So we just need to show that $f^*$ is an algebra-homomorphism.
  For this fix a basis $v_1, \dotsc, v_n$ of $V$.
  
  Let $h_1, h_2\in \mc{P}(V)$, i.e.\ we have polynomials $p_1, p_2\in k[X_1, \dotsc, X_n]$ with
  \[
      h_j\left( \sum_{i=1}^n \lambda_i v_i \right)
    = p_j(\lambda_1, \dotsc, \lambda_n)
    \text{ for }
    j = 1, 2 \,.
  \]
  For all $w \in W$ we have
  \begin{align*}
        f^*(h_1+h_2)(w)
    &=  ((h_1 + h_2) \circ f)(w)
     =  (h_1 + h_2)(f(w)) \\
    &=  h_1(f(w)) + h_2(f(w))
     =  (h_1 \circ f)(w) + (h_2 \circ f)(w) \\
    &=  f^*(h_1)(w) + f^*(h_2)(w)
     =  (f^*(h_1)+f^*(h_2))(w)
  \end{align*}
  and therefore
  \[
      f^*(h_1 + h_2)
    = f^*(h_1) + f^*(h_2) \,.
  \]
  For all $\lambda \in k$ and $w \in W$ we have
  \begin{align*}
        f^*(\lambda h_1)(w)
    &=  ((\lambda h_1) \circ f)(w)
     =  (\lambda h_1)(f(w)) \\
    &=  \lambda h_1(f(w))
     =  \lambda (h_1 \circ f)(w) \\
    &=  \lambda f^*(h_1)(w)
     =  (\lambda f^*(h_1))(w)
  \end{align*}
  and therefore
  \[
      f^*(\lambda h_1)
    = \lambda f^*(h_1) \,.
  \]
  This shows that $f^*$ is $k$-linear. That it is also a ring homomorphism follows from the fact that for all $w \in W$
  \begin{align*}
        f^*(h_1 h_2)(w)
    &=  ((h_1 h_2) \circ f)(w)
     =  (h_1 h_2)(f(w)) \\
    &=  h_1(f(w)) h_2(f(w))
     =  (h_1 \circ f)(w) (h_2 \circ f)(w) \\
    &=  f^*(h_1)(w) f^*(h_2)(w)
     =  (f^*(h_1) f^*(h_2))(w)
  \end{align*}
  and therefore
  \[
      f^*(h_1 h_2)
    = f^*(h_1) f^*(h_2) \,.
    \qedhere
  \]
\end{proof}


\begin{definition}
  Given finite-dimensional $k$-vector spaces $W$ and $V$ and a polynomial map $f \colon W \to V$ the algebra homomorphism $f^* \colon \mc{P}(V) \to \mc{P}(W)$ is the \emph{comorphism associated with $f$}.
\end{definition}
  

We have now associated finite-dimensional $k$-vector spaces with $k$-algebras and the polynomial maps between these vector spaces with algebra homomorphisms between the corresponding algebras.
This gives rise to a contravariant functor from $\cpol{k}$ to $\cAlg{k}$, as the next lemma shows.


\begin{proposition}
  Let $U$, $V$ and $W$ be finite-dimensional $k$-vector spaces.
  We have $\id_V^* = \id_{\mc{P}(V)}$ and for polynomial maps $f \colon W \to V$ and $g \colon V \to U$ we have
  \[
      (g \circ f)^*
    = f^* \circ g^* \,.
  \]
\end{proposition}
\begin{proof}
  The first statement is clear.
  The second holds because for all $h \in \mc{P}(U)$
  \begin{align*}
      ( g \circ f)^*(h)
    &=  h \circ (g \circ f)
     =  (h \circ g) \circ f \\
    &=  f^* (h \circ g)
     =  f^*(g^*(h))
     = (f^* \circ g^*)(h) \,.
    \qedhere
  \end{align*}
\end{proof}


It is interesting to notice that this functor is fully faithful.


\begin{proposition}
  Let $W$ and $V$ be finite-dimensional $k$-vector spaces.
  Then the map
  \[
            \Omega
    \colon  \Pol_k(W,V)
    \to     \Hom_{\cAlg{k}}(\mc{P}(V), \mc{P}(W)),
    \quad   f
    \mapsto f^*
  \]
  is a bijection.
\end{proposition}
\begin{proof}
  Fix a basis $v_1, \dotsc, v_n$ of $V$.
  Remember that $\mc{P}(V) = k[\varphi_1, \dotsc, \varphi_n]$ where $\varphi_j \in \mc{P}(V)$ is the $j$-th coordinate function, which is defined as
  \[
      \varphi_j\left( \sum_{i=1}^n \lambda_i v_i \right)
    = \lambda_j \,,
\]
  and $\varphi_1, \dotsc, \varphi_n$ are algebraically independent.
  In particular the map
  \[
            \Psi
    \colon  \mc{P}(W)^n
    \to     \Hom_{\cAlg{k}}(\mc{P}(V),\mc{P}(W)),
    \quad   (f_1, \dotsc, f_n)
    \mapsto (\varphi_j \mapsto f_j)
  \]
  is bijective (this is basically the universal property of the polynomial ring).
  
  Because $v_1, \dotsc, v_n$ is a basis of $V$ we also find that the map
  \[
            \Phi
    \colon  \mc{P}(W)^n
    \to     \Pol_k(W,V),
    \quad   (f_1, \dotsc, f_n)
    \mapsto \left(
                      w
              \mapsto \sum_{i=1}^n f_i(w)v_i
            \right)
  \]
  is bijective.
  
  We notice that for a polynomial map $f \colon W \to V$ and $f_1, \dotsc, f_n \in \mc{P}(W)$ with
  \[
      f(w)
    = \sum_{i=1}^n f_i(w) v_i
    \text{ for all }
    w \in W
  \]
  we have for all $1 \leq j \leq n$, $w \in W$
  \[
      f^*(\varphi_j)(w)
    = (\varphi_j \circ f)(w)
    = \varphi_j\left( \sum_{i=1}^n f_i(w) v_i \right)
    = f_j(w)
  \]
  and therefore for all $1 \leq j \leq n$
  \[
      f^*(\varphi_j)
    = f_j \,.
  \]
  Therefore we have
  \[
      \Omega(\Phi(f_1, \dotsc, f_n))
    = \Omega(f)
    = f^*
    = \Psi(f_1, \dotsc, f_n) \,.
  \]
  
  We have shown that $\Omega \Phi = \Psi$ (see figure \ref{figure: bijections}).
  Because $\Phi$ and $\Psi$ are bijections it follows that $\Omega = \Psi \Phi^{-1}$ is a bijection.
  \qedhere
  
  \begin{figure}\centering
    \begin{tikzpicture}[node distance = 6em, auto]
      \node (PWn) {$\mc{P}(W)^n$};
      \node (pol) [below left= 2em and 2em of PWn] {$\Pol_k(W,V)$};
      \node (alg) [below right= 2em and 2em of PWn] {$\Hom_{\cAlg{k}}(\mc{P}(V),\mc{P}(W))$};
      \draw[->] (PWn) to node[swap] {$\Phi$} (pol);
      \draw[->] (PWn) to node {$\Psi$} (alg);
      \draw[->] (pol) to node {$\Omega$} (alg);
    \end{tikzpicture}
    \caption{Bijections.}
    \label{figure: bijections}
  \end{figure}
\end{proof}





\section{Covariants}


\begin{definition}
  Let $G$ be a group and $V$ and $W$ be finite-dimensional representations of $G$.
  A map $f \colon W \to V$ is called \emph{covariant} if $f$ is a polynomial map and $G$-equivariant.
  We denote the space of covariant functions from $W$ to $V$ by
  \[
              \Cov_k(W,V)
    \coloneqq \Pol_k(W,V)^G \,.
  \]
  We also write $\Cov(W,V)$ if it is clear over what field we work.
\end{definition}


\begin{expls}
  \begin{enumerate}[label=\emph{\alph*)},leftmargin=*]
    \item
      Let $W$ be a finite-dimensional $k$-vector space, $d \in \N, d > 0$.
      Then the map
      \[
                \beta
        \colon  W \to W^{\otimes d},
        \quad   w
        \mapsto w \otimes \dotsb \otimes w
      \]
      is a polynomial map.
      It is also $G$-equivariant since
      \[
          \beta(g.w)
        = (g.w) \otimes \dotsb \otimes (g.w)
        = g.(w \otimes \dotsb \otimes w)
        = g.\beta(w)
      \]
      for all $g \in G$, $w \in W$.
    \item
      Fix a field $k$.
      Consider the action of $\GL_n(k)$ on $\Mat_n(k)$ via conjugation, i.e.\
      \[
          g.A
        = gAg^{-1}
      \]
      for all $g \in \GL_n(k)$, $A \in \Mat_n(k)$.
      Then the map
      \[
                \beta_i 
        \colon  \Mat_n(k) 
        \to     \Mat_n(k),
        \quad   A
        \mapsto A^i
      \]
      is covariant for all $i \geq 1$. 
  \end{enumerate}
\end{expls}


Notice that since we have $\Hom_k(W,V) \subseteq \Pol_k(W,V)$ we also have
\[
            \Hom_G(W,V)
  =         \Hom_k(W,V)^G
  \subseteq \Pol_k(W,V)^G
  =         \Cov(W,V) \,.
\]
Therefore every morphism of representations is covariant.


For finite-dimensional representations $W$ and $V$ of a group $G$ we know that $\Pol_k(W,V)$ is a $G$-set via
\[
    (g.f)(w)
  = g.f\left( g^{-1}.w \right)
  \text{ for all }
  g \in G,
  w \in W \,.
\]
That $g.f$ is polynomial follows from the fact that $\tau_{g^{-1}} \colon W \to W$ and $\pi_g \colon V \to V$ are $k$-linear and thus polynomial, and therefore also
\[
    g.f
  = \pi_g \circ f \circ \tau_{g^{-1}} \,.
\]
We also know that $\Pol_k(W,V)$ is a $k$-vector space via pointwise addition and multiplication.
Since the composition above is clearly $k$-linear in $f$ we find that $\Pol_k(W,V)$ is a representation of $G$.
As we have already seen before this implies that $\Cov(W,V) = \Pol_k(W,V)^G$ is also a $k$-vector space.


\begin{proposition}
  Let $G$ be a group, $W$ and $V$ finite-dimensional representations of $G$ over $k$ and $\beta \colon W \to V$ covariant.
  Then
  \[
              \beta^*\left( \mc{P}(V)^G \right)
    \subseteq \mc{P}(W)^G \,.
  \]
  Therefore $\beta^*$ induces an algebra-homomorphism from $\mc{P}(V)^G$ to $\mc{P}(W)^G$ by restriction.
\end{proposition}
\begin{proof}
  For all $f \in \mc{P}(V)^G$ we have for all $g \in G$, $w \in W$
  \begin{align*}
        (g.\beta^*(f))(w)
    &=  \beta^*(f)\left( g^{-1}.w \right)
     =  (f \circ \beta)\left( g^{-1}.w \right) \\
    &=  f\left( \beta\left( g^{-1}.w \right) \right)
     =  f\left( g^{-1}.\beta(w) \right)
     =  (g.f)(\beta(w)) \\
    &=  f(\beta(w))
     =  (f \circ \beta)(w)
     =  \beta^*(f)(w) \,.
  \end{align*}
  Therefore
  \[
      g.\beta^*(f)
    = \beta^*(f)
    \text{ for all }
    g \in G \,.
    \qedhere
  \]
\end{proof}


\begin{proposition}
  Let $V$ and $W$ be finite-dimensional representations of a group $G$ over a field $k$.
  Then the $\mc{P}(W)$-module structure of $\Pol_k(W,V)$ induces a $\mc{P}(W)^G$-module structure on $\Cov(W,V)$ by restriction.
\end{proposition}
\begin{proof}
  Since $\mc{P}(W)^G$ as a $k$-subalgebra of $\mc{P}(W)$ we have a $\mc{P}(W)^G$-module structure on $\Pol_k(W,V)$ by restriction.
  The proposition claims that $\Cov(W,V)$ is a $\mc{P}(W)^G$-submodule of $\Pol_k(W,V)$.
  
  We already know that $\Cov(W,V)$ is a $k$-vector subspace of $\Pol_k(W,V)$.
  For $f \in \mc{P}(W)^G$ and $\beta \in \Cov(W,V)$ we have for all $g \in G$, $w \in W$
  \begin{align*}
        (g.(f \cdot \beta))(w)
    &=  g.(f \cdot \beta)\left( g^{-1}.w \right)
     =  g.\left( f\left( g^{-1}.w \right) \cdot \beta\left( g^{-1}.w \right) \right) \\
    &=  f\left( g^{-1}.w \right) \cdot g.\beta\left( g^{-1}.w \right)
     =  (g.f)(w) \cdot (g.\beta)(w) \\
    &=  f(w) \cdot \beta(w)
     =  (f \cdot \beta)(w) \,.
  \end{align*}
  This shows that $\Cov(W,V)$ is also closed under multiplication by $\mc{P}(W)^G$.
\end{proof}




